{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7SXpaKwwGe5x"
   },
   "source": [
    "# TM10007 Assignment template -- ECG data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and cleaning\n",
    "\n",
    "Below are functions to load the dataset of your choice. After that, it is all up to you to create and evaluate a classification method. Beware, there may be missing values in these datasets. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "CiDn2Sk-VWqE",
    "outputId": "64224cd2-6054-4b04-a3f6-af8290400dfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\irisv\\OneDrive\\Documenten\\TM Master 1st year\\TM10007\\TM10007_group_15\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets as ds\n",
    "import seaborn\n",
    "import math\n",
    "import statistics\n",
    "\n",
    "# Classifiers\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn import feature_selection \n",
    "from sklearn import preprocessing\n",
    "from sklearn import neighbors\n",
    "from sklearn import svm\n",
    "\n",
    "cwd = os.getcwd() # This fn will return the Current Working Directory\n",
    "print(\"Current working directory:\", cwd)\n",
    "\n",
    "zip_path = os.path.join(cwd, 'ecg', 'ecg_data.zip')\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(os.path.join(cwd, 'ecg'))\n",
    "\n",
    "data_path = os.path.join(cwd, 'ecg', 'ecg_data.csv')\n",
    "data = pd.read_csv(data_path, index_col=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing data\n",
    "- Removing features if there is lot of data missing (replace all for a value)\n",
    "- Removing samples (in this case patients) if there is a lot of data missing\n",
    "- Imputation for generating data to fill us missing values\n",
    "    - Fill with the mean or median of feature.\n",
    "    - Fill with the value from a random other sample.\n",
    "    - Fill with value with highest frequency (good for categorical features).\n",
    "    - Use regression or machine learning to estimate the missing value\n",
    "    - We need to make a choice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing data removal\n",
    "location_nan = np.where(pd.isnull(data))  # Finds location of NaN\n",
    "for i,j in zip(*np.where(pd.isnull(data))):\n",
    "    data.iloc[i,j] = statistics.median[data.iloc[:,j]] # Replace nan value with median of feature column\n",
    "\n",
    "data = data.dropna(axis='columns', how='all') # deletes a feature if all values of a column are empty\n",
    "data = data.dropna(axis='rows', how='all') # deletes a patient if all values of a row are empty\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data into training and test data\n",
    "- Subset training and test based on ratios\n",
    "- Stratification\n",
    "- Cross-validation?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     label\n",
      "12    True\n",
      "4    False\n",
      "57    True\n",
      "826   True\n",
      "513  False\n",
      "..     ...\n",
      "717  False\n",
      "304  False\n",
      "817  False\n",
      "772  False\n",
      "338  False\n",
      "\n",
      "[620 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "X = data. iloc[:,:-1] #alles behalve label\n",
    "y = data.iloc[:, -1:] # universeel maken\n",
    "X_train, X_test_DO_NOT_FIT, y_train, y_test_DO_NOT_FIT = model_selection.train_test_split(X, y, test_size=0.25, stratify=y)\n",
    "y_train_ab = y_train==1\n",
    "print(y_train_ab)\n",
    "# X_test_DO_NOT_FIT and y_test_DO_NOT_FIT SHOULD NOT BE USED FOR FITTING\n",
    "\n",
    "# Scale the data to be normal\n",
    "scaler = preprocessing.RobustScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled_DO_NOT_FIT = scaler.transform(X_test_DO_NOT_FIT)\n",
    "\n",
    "# Cross-validation\n",
    "# cv_20fold = model_selection.StratifiedKFold(n_splits=10) --> uit college 1.2_generalization.ipyb\n",
    "\n",
    "# Loop over the folds\n",
    "#for validation_index, test_index in cv_20fold.split(X2, y2):\n",
    "    # Split the data properly\n",
    "#    X_validation = X2[validation_index]\n",
    "#    y_validation = y2[validation_index]\n",
    "    \n",
    "#    X_test = X2[test_index]\n",
    "#    y_test = y2[test_index]\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
