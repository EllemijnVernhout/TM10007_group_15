{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["9m3JUs7lp_aW","uFKm6roUqaTn","XCEIm_wbqDSS","dDh5p1aaqG9o","UkerFxQDqkkw","gwramotsxmFR","fs2JpG9Zy3ib","-bWqOArv0Etj","GqGugMuY06P8","hZ3xrwmn1mdX","t5O52Uz12l3A","4lTgYkJgygiz"],"authorship_tag":"ABX9TyOpIoK3YcbfA61JfFlPBwBU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Clone github and import modules"],"metadata":{"id":"9m3JUs7lp_aW"}},{"cell_type":"code","execution_count":47,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sb2MDd5rpIzI","executionInfo":{"status":"ok","timestamp":1682084173181,"user_tz":-120,"elapsed":4308,"user":{"displayName":"Julia Binder","userId":"04445038020140268015"}},"outputId":"982ed3a6-c6b3-43f0-b868-3d85d231c995"},"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'tm10007_ml' already exists and is not an empty directory.\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: sklearn in /usr/local/lib/python3.9/dist-packages (0.0.post4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (1.22.4)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (3.7.1)\n","Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.9/dist-packages (0.10.1)\n","Requirement already satisfied: statsmodels in /usr/local/lib/python3.9/dist-packages (0.13.5)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (0.11.0)\n","Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (5.12.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (23.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (8.4.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.0.7)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (3.0.9)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (4.39.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.4.4)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from imbalanced-learn) (1.2.0)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from imbalanced-learn) (1.10.1)\n","Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.9/dist-packages (from imbalanced-learn) (1.2.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from imbalanced-learn) (3.1.0)\n","Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.9/dist-packages (from statsmodels) (0.5.3)\n","Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.9/dist-packages (from statsmodels) (1.5.3)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib) (3.15.0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.25->statsmodels) (2022.7.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from patsy>=0.5.2->statsmodels) (1.16.0)\n"]}],"source":["# Clone github and import modules\n","!git clone https://github.com/jveenland/tm10007_ml.git\n","\n","! pip install sklearn numpy matplotlib imbalanced-learn statsmodels\n","\n","import zipfile\n","import statsmodels\n","import warnings\n","import statistics\n","import seaborn as sns\n","import pandas as pd\n","import numpy as np \n","import matplotlib.pyplot as plt\n","\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import precision_recall_curve, average_precision_score, f1_score\n","from sklearn.compose import TransformedTargetRegressor\n","from scipy.stats import shapiro, lognorm, randint\n","from scipy.spatial.distance import cdist\n","from sklearn.model_selection import StratifiedShuffleSplit, learning_curve, GridSearchCV, StratifiedKFold, cross_val_score, KFold, train_test_split, RandomizedSearchCV, validation_curve \n","from sklearn import preprocessing\n","from sklearn.preprocessing import RobustScaler, StandardScaler\n","from imblearn.over_sampling import RandomOverSampler\n","from sklearn import datasets as ds, model_selection, metrics, neighbors\n","from sklearn.linear_model import Lasso\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import f1_score\n","from sklearn.linear_model import LinearRegression\n","from statsmodels.stats.multitest import multipletests\n","from sklearn.exceptions import ConvergenceWarning\n","from sklearn.metrics import make_scorer, f1_score\n","from sklearn.metrics import classification_report\n","\n","# Classifiers\n","from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif, SelectFromModel\n","from sklearn.decomposition import PCA\n","from sklearn.svm import SVC, LinearSVC\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n","from sklearn.naive_bayes import GaussianNB"]},{"cell_type":"markdown","source":["##Defining functions"],"metadata":{"id":"uFKm6roUqaTn"}},{"cell_type":"markdown","source":["Missing data\n"],"metadata":{"id":"KbvCkNP4qoLR"}},{"cell_type":"code","source":["def missing_data(X_design):\n","  # Missing data\n","  nan_count = X_design.isna().sum().sum()  # count missing data -> 0 in our dataset\n","\n","  # Delete missing data when > 50% of feature or sample is missing\n","  X_design = X_design.dropna(axis='columns', how='all') # deletes a feature if all values of a column (so feature) are empty\n","  X_design = X_design.dropna(axis='rows', how='all') # deletes a patient if all values of a row (so sample) are empty\n","  \n","  #threshold = len(X_design.columns) // 2\n","  #X_design = X_design.dropna(axis='columns', thresh=threshold) # deletes a feature if 50% values of a column (so feature) are empty\n","  #X_design = X_design.dropna(axis='rows', thresh=threshold) # deletes a patient if 50% values of a row (so sample) are empty\n","\n","  # Missing data to median per feature\n","  for column in X_design.columns:\n","      X_design[column].fillna(X_design[column].median(), inplace=True)\n","  return X_design\n","\n","def missing_data_zero_check(X_design):\n","  \"\"\"This function does the same as missing_data, but also corrects for zeros, assuming zeros are missing data\"\"\"\n","  # Missing data\n","  X_design = X_design.replace(0, np.nan)  # make all zeros to NaN\n","  nan_count = X_design.isna().sum().sum()  # count missing data -> 10500 in our dataset\n","\n","  # Delete missing data when > 50% of feature or sample is missing\n","  X_design = X_design.dropna(axis='columns', how='all') # deletes a feature if all values of a column (so feature) are empty\n","  X_design = X_design.dropna(axis='rows', how='all') # deletes a patient if all values of a row (so sample) are empty\n","  #threshold = len(X_design.columns) // 2\n","  #X_design = X_design.dropna(axis='columns', thresh=threshold) # deletes a feature if 50% values of a column (so feature) are empty\n","  #X_design = X_design.dropna(axis='rows', thresh=threshold) # deletes a patient if 50% values of a row (so sample) are empty\n","\n","  # Missing data to median per feature\n","  for column in X_design.columns:\n","      X_design[column].fillna(X_design[column].median(), inplace=True)\n","  return X_design"],"metadata":{"id":"mK5Rl9nVqbuq","executionInfo":{"status":"ok","timestamp":1682084174785,"user_tz":-120,"elapsed":4,"user":{"displayName":"Julia Binder","userId":"04445038020140268015"}}},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":["Outliers"],"metadata":{"id":"zaAuqjRVrBJG"}},{"cell_type":"code","source":["def removing_outliers(X_design):\n","    # supress performance warning\n","    warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n","\n","    # Outliers: Tukey's fence \n","    k=3\n","    fences=pd.DataFrame()\n","    outliers = pd.DataFrame(False, index=X_design.index, columns=X_design.columns) # create an empty DataFrame for outliers\n","\n","    for col in X_design.columns:\n","        q1, q3 = np.percentile(X_design[col], [25, 75])\n","        iqr = q3 - q1\n","        lower_fence = q1 - k*iqr\n","        upper_fence = q3 + k*iqr\n","        fences[col]=[lower_fence, upper_fence]\n","        for row in X_design.index:\n","            if X_design.loc[row, col] < lower_fence or X_design.loc[row, col] > upper_fence:\n","                outliers.loc[row, col] = True # mark the place as an outlier\n","\n","    row_count = (outliers == True).sum(axis=1)\n","    col_count = (outliers == True).sum(axis=0)\n","    total_count = row_count.sum() + col_count.sum()\n","    print(f'The total number of outliers in dataset x is {total_count}')\n","\n","    # create a copy of x to modify\n","    new_x = X_design.copy()\n","\n","    #replace outliers with maximum or minimun interquartile range of x by column\n","    for col in outliers.columns:\n","        q3 = X_design.loc[outliers[col] == False, col].quantile(0.75) # 3rd quartile of column where outlier is False\n","        q1 = X_design.loc[outliers[col] == False, col].quantile(0.25) # 1st quartile of column where outlier is False\n","        iqr = q3 - q1 # interquartile range of column where outlier is False\n","        lower_fence = q1 - k*iqr\n","        upper_fence = q3 + k*iqr\n","        new_x.loc[outliers[col] & (new_x[col] > upper_fence), col] = upper_fence # replace outliers with upper fence\n","        new_x.loc[outliers[col] & (new_x[col] < lower_fence), col] = lower_fence # replace outliers with lower fence\n","\n","        X_design = new_x.copy()\n","    return X_design"],"metadata":{"id":"H3L11MYVrCBk","executionInfo":{"status":"ok","timestamp":1682084175138,"user_tz":-120,"elapsed":5,"user":{"displayName":"Julia Binder","userId":"04445038020140268015"}}},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":["For learning curves: PCA+univariate and LASSO"],"metadata":{"id":"NNNHgswNtVVC"}},{"cell_type":"code","source":["def PCA_univariate(X_train, y_train):\n","    #Remove features with zero variance\n","    sel_vt = VarianceThreshold(threshold=0.0)\n","    X_vt = sel_vt.fit_transform(X_train, y_train)\n","    \n","    #PCA\n","    n_samples = X_train.shape[0]\n","    n_features = X_train.shape[1]\n","    n_features = min(n_samples, n_features)\n","\n","    pca = PCA(n_components=n_features)            \n","    X_train = pca.fit_transform(X_train)\n","\n","    #Univariate\n","    sel_kb = SelectKBest(f_classif, k='all')\n","    sel_kb.fit(X_train, y_train)\n","    p_values = sel_kb.pvalues_\n","\n","    reject_fdr, pvals_fdr, _, _ = multipletests(pvals=p_values, alpha=0.05, method='fdr_bh')\n","    features_selected=np.array(np.where(reject_fdr)[0])\n","    print(features_selected.shape)\n","    X_train = X_train[:,features_selected]\n","\n","    return X_train\n","\n","def LASSO_feature(X_train_lasso, y_train):\n","    # Define the Lasso model\n","    lasso = Lasso()\n","\n","    # Grid search\n","    alphas = np.logspace(-5, 1, 100)  # Define the grid of alpha values to search over\n","    grid_search = GridSearchCV(lasso, param_grid={'alpha': alphas}, cv=5)\n","    grid_search.fit(X_train_lasso, y_train)\n","    best_alpha = grid_search.best_params_['alpha']\n","\n","    # Create a new Lasso model using the best alpha value\n","    lasso = Lasso(alpha=best_alpha)\n","    lasso.fit(X_train_lasso, y_train)\n","\n","    # Get the coefficients of the Lasso model, find them and define the new X_train with less features\n","    coef = lasso.coef_\n","    selected_features = np.where(coef != 0)[0]\n","    X_train_lasso = X_train_lasso[selected_features]\n","\n","    return X_train_lasso\n","\n","def plot_learning_curve(estimator, title, X, y, axes, ylim=None, cv=None,\n","                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n","  axes.set_title(title)\n","  if ylim is not None:\n","      axes.set_ylim(*ylim)\n","  axes.set_xlabel(\"Training examples\")\n","  axes.set_ylabel(\"Score\")\n","  \n","  scorer = make_scorer(f1_score)\n","  \n","  train_sizes, train_scores, test_scores  = \\\n","    learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n","                       train_sizes=train_sizes, scoring=scorer)\n","  train_scores_mean = np.mean(train_scores, axis=1)\n","  train_scores_std = np.std(train_scores, axis=1)\n","  test_scores_mean = np.mean(test_scores, axis=1)\n","  test_scores_std = np.std(test_scores, axis=1)\n","    \n","  # Plot learning curve\n","  axes.grid()\n","  axes.fill_between(train_sizes, train_scores_mean - train_scores_std,\n","                         train_scores_mean + train_scores_std, alpha=0.1,\n","                         color=\"r\")\n","  axes.fill_between(train_sizes, test_scores_mean - test_scores_std,\n","                         test_scores_mean + test_scores_std, alpha=0.1,\n","                         color=\"g\")\n","  axes.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n","                 label=\"Training score\")\n","  axes.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n","                 label=\"Cross-validation score\")\n","  axes.legend(loc=\"best\")\n","  \n","  return plt"],"metadata":{"id":"Pr3Be8Mita-p","executionInfo":{"status":"ok","timestamp":1682084175139,"user_tz":-120,"elapsed":5,"user":{"displayName":"Julia Binder","userId":"04445038020140268015"}}},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":["## Load data"],"metadata":{"id":"XCEIm_wbqDSS"}},{"cell_type":"code","source":["with zipfile.ZipFile('/content/tm10007_ml/ecg/ecg_data.zip', 'r') as zip_ref:\n","    zip_ref.extractall('/content/tm10007_ml/ecg')\n","\n","data = pd.read_csv('/content/tm10007_ml/ecg/ecg_data.csv', index_col=0)"],"metadata":{"id":"U0Zgo9L8qC87","executionInfo":{"status":"ok","timestamp":1682084196510,"user_tz":-120,"elapsed":16112,"user":{"displayName":"Julia Binder","userId":"04445038020140268015"}}},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":["## Explore data"],"metadata":{"id":"dDh5p1aaqG9o"}},{"cell_type":"code","source":["print(f'The number of samples: {len(data.index)}')\n","print(f'The number of columns: {len(data.columns)}')\n","\n","X = data.loc[:, data.columns != 'label']  #samples and features\n","y = data['label']  # labels\n","\n","# normal / abnormal ECGs\n","total_abnormal_ECG = np.count_nonzero(y) \n","total_normal_ECG = y.size -np.count_nonzero(y) \n","percentage_abnormal = total_abnormal_ECG / (total_abnormal_ECG + total_normal_ECG)*100\n","\n","print(f'{total_abnormal_ECG} people have an abnormal ECG')\n","print(f'{total_normal_ECG} people have a normal ECG')\n","print(f'The percentage of abnormal ECGs in this dataset is {percentage_abnormal} %')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VnHeWuqWqM71","executionInfo":{"status":"ok","timestamp":1682091349989,"user_tz":-120,"elapsed":257,"user":{"displayName":"Julia Binder","userId":"04445038020140268015"}},"outputId":"1c17ff86-4e7a-4931-b58f-d1ed667c3e12"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["The number of samples: 827\n","The number of columns: 9001\n","9 people have an abnormal ECG\n","41 people have a normal ECG\n","The percentage of abnormal ECGs in this dataset is 18.0 %\n"]}]},{"cell_type":"markdown","source":["## Learning curves\n","The learning curves are commented, but if interested, you can uncomment and see how the curves look like. "],"metadata":{"id":"UkerFxQDqkkw"}},{"cell_type":"markdown","source":["Preperation of data"],"metadata":{"id":"2-T2yJVntgGf"}},{"cell_type":"code","source":["# # create X_train\n","# X_train, X_test_DO_NOT_FIT, y_train, y_test_DO_NOT_FIT = model_selection.train_test_split(X, y, test_size=0.1, stratify=y)\n","\n","# # Balance data\n","# ros = RandomOverSampler(sampling_strategy='minority')\n","# X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n","# X_train = X_resampled\n","# y_train = y_resampled\n","\n","# # Scale the data to be normal\n","# scaler = preprocessing.RobustScaler()\n","# scaler.fit(X_train)\n","# X_train = scaler.transform(X_train)\n","# X_train=pd.DataFrame(X_train)\n","\n","# # Create X_train for PCA+univariate and for LASSO\n","# X_train = X_train.copy()\n","# X_train_lasso = X_train.copy()\n","\n","# clsfs_all = [neighbors.KNeighborsClassifier(n_neighbors=1), \n","#          neighbors.KNeighborsClassifier(n_neighbors=5), \n","#          neighbors.KNeighborsClassifier(n_neighbors=20), \n","#          RandomForestClassifier(n_estimators=1, random_state=42),\n","#          RandomForestClassifier(n_estimators=5, random_state=42),\n","#          RandomForestClassifier(n_estimators=200, random_state=42),\n","#          SVC(kernel='rbf', C=10, gamma=0.1),\n","#          SVC(kernel='linear', C=10, gamma=0.1),\n","#          SVC(kernel='poly', C=10, gamma=0.1),\n","#          SVC(kernel='sigmoid', C=10, gamma=0.1),\n","#          GaussianNB(),\n","#          LinearRegression(),\n","#          QuadraticDiscriminantAnalysis()\n","#          ]\n"],"metadata":{"id":"p0jV-RsvqkWR","executionInfo":{"status":"ok","timestamp":1682084205397,"user_tz":-120,"elapsed":1,"user":{"displayName":"Julia Binder","userId":"04445038020140268015"}}},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":["Learning curves for PCA+univariate"],"metadata":{"id":"Ggz5rrlXtj5c"}},{"cell_type":"code","source":["# X_train=PCA_univariate(X_train,y_train)\n","\n","# num=0\n","# fig = plt.figure(figsize=(24,8*len(clsfs_all)))\n","\n","# # Create a cross-validation object\n","# cv = StratifiedShuffleSplit(n_splits=10, test_size=0.25, random_state=0)\n","\n","# # Now use the classifiers on all datasets\n","# for clf in clsfs_all:\n","#     title = str(type(clf))\n","#     ax = fig.add_subplot(7, 3, num + 1)\n","#     plot_learning_curve(clf, title, X_train, y_train, ax, ylim=(0.3, 1.01), cv=cv)\n","#     num += 1"],"metadata":{"id":"NTE4vtFstm9i","executionInfo":{"status":"ok","timestamp":1682084205669,"user_tz":-120,"elapsed":4,"user":{"displayName":"Julia Binder","userId":"04445038020140268015"}}},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":["Regularization for PCA+ univariate and their new learning curves"],"metadata":{"id":"LrM9BRp2t30t"}},{"cell_type":"code","source":["# # Define the Lasso model\n","# lasso2 = Lasso()\n","# # Define the grid of alpha values to search over\n","# alphas2 = np.logspace(-5, 1, 100)\n","# # Define the grid search\n","# grid_search2 = GridSearchCV(lasso2, param_grid={'alpha': alphas2}, cv=5)\n","# # Fit the grid search to your training data\n","# grid_search2.fit(X_train, y_train)\n","# # Get the best alpha value from the grid search\n","# best_alpha2 = grid_search2.best_params_['alpha']\n","# # Create a new Lasso model using the best alpha value\n","# lasso2 = Lasso(alpha=best_alpha2)\n","# # Fit the Lasso model to your training data\n","# lasso2.fit(X_train, y_train)\n","# # Get the coefficients of the Lasso model\n","# coef2 = lasso2.coef_\n","# # Get the indices of the selected features\n","# selected_features2 = np.where(coef2 != 0)[0]\n","# #X_train_regularization = X_train[selected_features2]\n","# #X_train_regularization = X_train[:,selected_features2]\n","# X_train_regularization = X_train[:,selected_features2[:]]\n","\n","# clsfs_regularization = [SVC(kernel='rbf', C=10, gamma=0.1),\n","#          SVC(kernel='poly', C=10, gamma=0.1),\n","#          RandomForestClassifier(n_estimators=5, random_state=42),\n","#          RandomForestClassifier(n_estimators=200, random_state=42)]\n","\n","# num=0\n","# fig = plt.figure(figsize=(24,8*len(clsfs_regularization)))\n","\n","# # Create a cross-validation object\n","# cv = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n","\n","# # Now use the classifiers on all datasets\n","# for clf in clsfs_regularization:\n","#     title = str(type(clf))\n","#     ax = fig.add_subplot(7, 3, num + 1)\n","#     plot_learning_curve(clf, title, X_train_regularization, y_train, ax, ylim=(0.3, 1.01), cv=cv)\n","#     num += 1"],"metadata":{"id":"SzmTwM50t8Xj","executionInfo":{"status":"ok","timestamp":1682084205669,"user_tz":-120,"elapsed":3,"user":{"displayName":"Julia Binder","userId":"04445038020140268015"}}},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":["Learning curves for LASSO"],"metadata":{"id":"DOry0Zc4uHzy"}},{"cell_type":"code","source":["# num=0\n","# fig = plt.figure(figsize=(24,8*len(clsfs_all)))\n","\n","# X_train_lasso=LASSO_feature(X_train_lasso,y_train)\n","\n","# # Create a cross-validation object\n","# cv = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n","\n","# # Now use the classifiers on all datasets\n","# for clf in clsfs_all:\n","#     title = str(type(clf))\n","#     ax = fig.add_subplot(7, 3, num + 1)\n","#     plot_learning_curve(clf, title, X_train_lasso, y_train, ax, ylim=(0.3, 1.01), cv=cv)\n","#     num += 1"],"metadata":{"id":"MWIKRIhouJSM","executionInfo":{"status":"ok","timestamp":1682084205669,"user_tz":-120,"elapsed":3,"user":{"displayName":"Julia Binder","userId":"04445038020140268015"}}},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":["## Pipeline *1*: PCA + univariate -> Gaussian Naive Bayes"],"metadata":{"id":"gwramotsxmFR"}},{"cell_type":"code","source":["# Define outer and inner cross validation\n","outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42) \n","inner_cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42) \n","\n","#Create empty arrays (for results)\n","f1_design_1_before = []\n","f1_test_1_before = []\n","f1_design_1_after = []\n","f1_test_1_after = []\n","results_1 = pd.DataFrame()\n","best_pipe_1 = []\n","best_hp_1_before = pd.DataFrame()\n","best_hp_1 = pd.DataFrame()\n","y_test_1_all = []\n","y_pred_1_all = []\n","\n","for design_index, test_index in outer_cv.split(X, y): \n","    X_design = X.transpose()[design_index]\n","    X_design = X_design.transpose()\n","    y_design = y[design_index]\n","    \n","    X_test = X.transpose()[test_index]\n","    X_test = X_test.transpose()\n","    y_test_1 = y[test_index].tolist()\n","    y_test_1_all.append(y_test_1)\n","    \n","    # remove outliers\n","    X_design = removing_outliers(X_design)\n","    X_test = removing_outliers(X_test)\n","\n","    # Correct missing data\n","    X_design = missing_data(X_design)\n","    X_test = missing_data(X_test)\n","\n","    # balance the classes, so design set consists of 50% normal and 50% abnormal ECG's\n","    ros = RandomOverSampler(sampling_strategy='minority')\n","    X_resampled, y_resampled = ros.fit_resample(X_design, y_design)\n","    X_design = X_resampled\n","    y_design = y_resampled   \n","\n","    print(f'shape after balancing {X_design.shape}')\n","    \n","    # Define pipeline 1\n","    pipeline_1a = Pipeline([\n","        ('scaler', RobustScaler()),\n","        ('var_threshold', VarianceThreshold(threshold=0.0)),\n","        ('pca', PCA(n_components=0.5)),\n","    ])\n","\n","    param_grid_1a = {\n","        'pca__n_components': [0.5],#,0.75, 0.9, 0.95, 0.99],\n","        }\n","\n","    # Perform grid search with inner cross-validation, part 1\n","    rand_search_1a = RandomizedSearchCV(pipeline_1a, param_distributions=param_grid_1a, n_iter=10, cv=inner_cv, scoring='f1', n_jobs=-1) #klopt n__iter\n","    rand_search_1a.fit(X_design, y_design) # klopt dit fit_transform?\n","    X_design = rand_search_1a.best_estimator_.transform(X_design)\n","    X_test = rand_search_1a.best_estimator_.transform(X_test)\n","    \n","    # Access and store the best set of hyperparameters of each outer-CV loop in a DataFrame\n","    best_hp_1_before = best_hp_1_before.append(rand_search_1a.best_params_,ignore_index=True)\n","    print(best_hp_1_before)\n","    print(f'shape of X_design after pca {X_design.shape}')\n","\n","    sel_kb = SelectKBest(f_classif, k='all')\n","    sel_kb.fit(X_design, y_design)\n","    p_values = sel_kb.pvalues_\n","\n","    reject_fdr, pvals_fdr, _, _ = multipletests(pvals=p_values, alpha=0.05, method='fdr_bh')\n","    features_selected=np.array(np.where(reject_fdr)[0])\n","    print(f'size of features selected{features_selected.shape}')\n","    X_design = X_design[:,features_selected]\n","    X_test = X_test[:,features_selected]\n","    \n","    # pipeline 1b\n","    pipeline_1b = Pipeline([\n","        ('clf', GaussianNB())\n","    ])\n","\n","    # Define scores BEFORE hyperparameter tuning\n","    pipeline_1b.fit(X_design, y_design)\n"," \n","    y_pred_design_1 = pipeline_1b.predict(X_design)\n","    f1_design_1_bef = f1_score(y_design, y_pred_design_1)\n","    f1_design_1_before.append(f1_design_1_bef)\n","\n","    y_pred_test_1_before = pipeline_1b.predict(X_test)\n","    f1_test_1_bef = f1_score(y_test_1, y_pred_test_1_before)\n","    f1_test_1_before.append(f1_test_1_bef)\n","\n","    # Define hyperparameters of pipeline 1\n","    param_grid_1b = {\n","    'clf__var_smoothing': np.logspace(0,-9, num=100),\n","    }\n","\n","    print(f'after feature selection: {X_design.shape}')\n","\n","    # Perform grid search with inner cross-validation, part 2\n","    model_1 = RandomizedSearchCV(pipeline_1b, param_distributions=param_grid_1b, n_iter=50, cv=inner_cv, scoring='f1', n_jobs=-1) \n","    model_1.fit(X_design, y_design)\n","    results = pd.DataFrame(model_1.cv_results_)\n","    results_1 = results_1.append(results,ignore_index=True)\n","\n","    # Define scores AFTER hyperparameter tuning \n","    y_pred_design_1_after = model_1.predict(X_design)\n","    f1_design_1_aft = f1_score(y_design, y_pred_design_1_after)\n","    f1_design_1_after.append(f1_design_1_aft)\n","    \n","    y_pred_test_1_after = model_1.predict(X_test)\n","    y_pred_1_all.append(y_pred_test_1_after)\n","    \n","    f1_test_1_aft = f1_score(y_test_1, y_pred_test_1_after)\n","    f1_test_1_after.append(f1_test_1_aft)\n","\n","    # Access and store the best set of hyperparameters of each outer-CV loop in a DataFrame\n","    best_hp_1 = best_hp_1.append(model_1.best_params_,ignore_index=True)\n","    # Stores the optimum model in best_pipe\n","    best_pipe_1.append(model_1.best_estimator_)\n","\n","# Save results of inner CV into .csv file\n","results_1.to_csv('results_1.csv', index=False)\n","\n","print(f'Mean and std of F1 scores of pipeline 1: {statistics.mean(f1_test_1_after)} +/- {statistics.stdev(f1_test_1_after)}')\n","print(f'The optimal hyperparameters per split: {best_hp_1}')\n","print(f'The best pipes per split {best_pipe_1}')\n","print(f'The design F1 scores before tuning {f1_design_1_before}')\n","print(f'The test F1 scores before tuning {f1_test_1_before}')\n","print(f'The design F1 scores after tuning {f1_design_1_after}')\n","print(f'The test F1 scores afer tuning {f1_test_1_after}')\n","\n","data_1 = [f1_design_1_after, f1_test_1_after]\n","sns.boxplot(data=data_1)\n","plt.title('Boxplot F1 scores design and test pipeline 1')\n","plt.xlabel('design and test')\n","plt.ylabel('F1 score score')\n","plt.show()\n","\n","# Loop over rows and compute precision recall curve for each row\n","for i in range(len(y_pred_1_all)):\n","    precision, recall, thresholds = precision_recall_curve(y_test_1_all[i], y_pred_1_all[i])\n","    auc = average_precision_score(y_test_1_all[i], y_pred_1_all[i])\n","\n","    # Plot the ROC curve for each row\n","    plt.plot(recall, precision, lw=2, label='PR curve it. %d (AP = %0.2f)' % (i+1, auc))\n","\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('Precision-recall curve for all rows')\n","plt.legend(loc=\"lower right\", fontsize=8)\n","plt.show()"],"metadata":{"id":"rnjkGpz8xpCd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Pipeline 2: PCA + Univariate -> Quadratic Discriminant analysis"],"metadata":{"id":"fs2JpG9Zy3ib"}},{"cell_type":"code","source":["# PIPELINE 2\n","# Define outer and inner cross validation\n","outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n","inner_cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n","\n","# Creating empty arrays\n","f1_design_2_before = []\n","f1_test_2_before = []\n","f1_design_2_after = []\n","f1_test_2_after = []\n","results_2 = pd.DataFrame()\n","best_pipe_2 = []\n","best_hp_2_before = pd.DataFrame()\n","best_hp_2 = pd.DataFrame()\n","y_test_2_all = []\n","y_pred_2_all = []\n","\n","for design_index, test_index in outer_cv.split(X, y): \n","    X_design = X.transpose()[design_index]\n","    X_design = X_design.transpose()\n","    print(f'Size_X_design {X_design.shape}') # print size of X_design\n","    y_design = y[design_index]\n","    \n","    X_test = X.transpose()[test_index]\n","    X_test = X_test.transpose()\n","    y_test_2 = y[test_index].tolist()\n","    y_test_2_all.append(y_test_2)\n","    print(f'Size X_test {X_test.shape}')     # print size of X_test\n","    \n","    # remove outliers\n","    X_design = removing_outliers(X_design)\n","    X_test = removing_outliers(X_test)\n","\n","    # Correct missing data\n","    X_design = missing_data(X_design)\n","    X_test = missing_data(X_test)\n","\n","    # balance the classes, so design set consists of 50% normal and 50% abnormal ECG's\n","    ros = RandomOverSampler(sampling_strategy='minority')\n","    X_resampled, y_resampled = ros.fit_resample(X_design, y_design)\n","    X_design = X_resampled\n","    y_design = y_resampled   \n","\n","    print(f'shape after balancing {X_design.shape}')\n","    \n","    # Define pipeline 2\n","    pipeline_2a = Pipeline([\n","        ('scaler', RobustScaler()),\n","        ('var_threshold', VarianceThreshold(threshold=0.0)),\n","        ('pca', PCA(n_components=0.5)),\n","    ])\n","\n","    param_grid_2a = {\n","        'pca__n_components': [0.5],#0.75, 0.9, 0.95, 0.99],\n","        }\n","\n","    # Perform grid search with inner cross-validation, part 1\n","    rand_search_2a = RandomizedSearchCV(pipeline_2a, param_distributions=param_grid_2a, n_iter=10, cv=inner_cv, scoring='f1', n_jobs=-1) #klopt n__iter\n","    rand_search_2a.fit(X_design, y_design) # klopt dit fit_transform?\n","    X_design = rand_search_2a.best_estimator_.transform(X_design)\n","    X_test = rand_search_2a.best_estimator_.transform(X_test)\n","    \n","    # Access and store the best set of hyperparameters of each outer-CV loop in a DataFrame\n","    best_hp_2_before = best_hp_2_before.append(rand_search_2a.best_params_,ignore_index=True)\n","    print(best_hp_2_before)\n","    print(f'shape of X_design after pca {X_design.shape}')\n","\n","    sel_kb = SelectKBest(f_classif, k='all')\n","    sel_kb.fit(X_design, y_design)\n","    p_values = sel_kb.pvalues_\n","\n","    reject_fdr, pvals_fdr, _, _ = multipletests(pvals=p_values, alpha=0.05, method='fdr_bh')\n","    features_selected=np.array(np.where(reject_fdr)[0])\n","    print(f'size of features selected{features_selected.shape}')\n","    X_design = X_design[:,features_selected]\n","    X_test = X_test[:,features_selected]\n","\n","    # design the classifier on the selected features with the best hyperparameters to create best designed classifier\n","    model_2 = QuadraticDiscriminantAnalysis()\n","    model_2.fit(X_design, y_design)\n","\n","    # Define scores AFTER hyperparameter tuning \n","    y_pred_design_2_after = model_2.predict(X_design)\n","    f1_design_2_aft = f1_score(y_design, y_pred_design_2_after)\n","    f1_design_2_after.append(f1_design_2_aft)\n","   \n","    y_pred_test_2_after = model_2.predict(X_test)\n","    y_pred_2_all.append(y_pred_test_2_after)\n","   \n","    f1_test_2_aft = f1_score(y_test_2, y_pred_test_2_after)\n","    f1_test_2_after.append(f1_test_2_aft)\n","\n","print(f'Mean and std of F1 scores of pipeline 1: {statistics.mean(f1_test_2_after)} +/- {statistics.stdev(f1_test_2_after)}')\n","print(f'The optimal hyperparameters per split: {best_hp_2}')\n","print(f'The best pipes per split {best_pipe_2}')\n","print(f'The design F1 scores before tuning {f1_design_2_before}')\n","print(f'The test F1 scores before tuning {f1_test_2_before}')\n","print(f'The design F1 scores after tuning {f1_design_2_after}')\n","print(f'The test F1 scores afer tuning {f1_test_2_after}')\n","\n","data_2 = [f1_design_2_after, f1_test_2_after]\n","sns.boxplot(data=data_2)\n","plt.title('Boxplot F1 scores design and test pipeline 2')\n","plt.xlabel('design and test')\n","plt.ylabel('F1 score score')\n","plt.show()\n","\n","# Loop over rows and compute precision recall curve for each row\n","for i in range(len(y_pred_2_all)):\n","    precision, recall, thresholds = precision_recall_curve(y_test_2_all[i], y_pred_2_all[i])\n","    auc = average_precision_score(y_test_2_all[i], y_pred_2_all[i])\n","\n","    # Plot the ROC curve for each row\n","    plt.plot(recall, precision, lw=2, label='PR curve it. %d (AP = %0.2f)' % (i+1, auc))\n","\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('Precision-recall curve for all rows pipeline 2')\n","plt.legend(loc=\"lower right\", fontsize=8)\n","plt.show()"],"metadata":{"id":"p_OXtR6qy-aB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Pipeline 3"],"metadata":{"id":"kf25y4FtzYdl"}},{"cell_type":"code","source":["# PIPELINE 3\n","# Define outer and inner cross validation\n","outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42) \n","inner_cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n","\n","# Creating empty arrays\n","f1_design_3_before = []\n","f1_test_3_before = []\n","f1_design_3_after = []\n","f1_test_3_after = []\n","results_3 = pd.DataFrame()\n","best_pipe_3 = []\n","best_hp_3_before = pd.DataFrame()\n","best_hp_3 = pd.DataFrame()\n","y_test_3_all = []\n","y_pred_3_all = []\n","\n","for design_index, test_index in outer_cv.split(X, y): \n","    X_design = X.transpose()[design_index]\n","    X_design = X_design.transpose()\n","    print(f'Size_X_design {X_design.shape}') # print size of X_design\n","    y_design = y[design_index]\n","    \n","    X_test = X.transpose()[test_index]\n","    X_test = X_test.transpose()\n","    y_test_3 = y[test_index].tolist()\n","    y_test_3_all.append(y_test_3)\n","    print(f'Size X_test {X_test.shape}')     # print size of X_test\n","    \n","    X_design = removing_outliers(X_design)\n","    X_test = removing_outliers(X_test)\n","    X_design = missing_data(X_design)\n","    X_test = missing_data(X_test)\n","\n","    # balance the classes, so design set consists of 50% normal and 50% abnormal ECG's\n","    ros = RandomOverSampler(sampling_strategy='minority')\n","    X_resampled, y_resampled = ros.fit_resample(X_design, y_design)\n","    X_design = X_resampled\n","    y_design = y_resampled   \n","    print(f'shape after balancing {X_design.shape}')\n","    \n","    # Define pipeline 3\n","    pipeline_3a = Pipeline([\n","        ('scaler', RobustScaler()),\n","        ('var_threshold', VarianceThreshold(threshold=0.0)),\n","        ('pca', PCA(n_components=0.5)),\n","    ])\n","\n","    param_grid_3a = {\n","        'pca__n_components': [0.5],#0.75, 0.9, 0.95, 0.99],\n","        }\n","\n","    # Perform grid search with inner cross-validation, part 1\n","    rand_search_3a = RandomizedSearchCV(pipeline_3a, param_distributions=param_grid_3a, n_iter=10, cv=inner_cv, scoring='f1', n_jobs=-1) #klopt n__iter\n","    rand_search_3a.fit(X_design, y_design) # klopt dit fit_transform?\n","    X_design = rand_search_3a.best_estimator_.transform(X_design)\n","    X_test = rand_search_3a.best_estimator_.transform(X_test)\n","    \n","    # Access and store the best set of hyperparameters of each outer-CV loop in a DataFrame\n","    best_hp_3_before = best_hp_3_before.append(rand_search_3a.best_params_,ignore_index=True)\n","    print(f'shape of X_design after pca {X_design.shape}')\n","\n","    # univariate feature selection\n","    sel_kb = SelectKBest(f_classif, k='all')\n","    sel_kb.fit(X_design, y_design)\n","    p_values = sel_kb.pvalues_\n","\n","    reject_fdr, pvals_fdr, _, _ = multipletests(pvals=p_values, alpha=0.05, method='fdr_bh')\n","    features_selected=np.array(np.where(reject_fdr)[0])\n","    print(f'size of features selected{features_selected.shape}')\n","    X_design = X_design[:,features_selected]\n","    X_test = X_test[:,features_selected]\n","        \n","    print(f'shape of X_design after univariate: {X_design.shape}')\n","    \n","    # pipeline 3b\n","    pipeline_3b = Pipeline([    \n","        ('clf', SVC(kernel='linear'))\n","    ])\n","\n","    # Define scores BEFORE hyperparameter tuning\n","    pipeline_3b.fit(X_design, y_design)\n"," \n","    y_pred_design_3 = pipeline_3b.predict(X_design)\n","    f1_design_3_bef = f1_score(y_design, y_pred_design_3)\n","    f1_design_3_before.append(f1_design_3_bef)\n","\n","    y_pred_test_3_before = pipeline_3b.predict(X_test)\n","    f1_test_3_bef = f1_score(y_test_3, y_pred_test_3_before)\n","    f1_test_3_before.append(f1_test_3_bef)\n","\n","    # Define hyperparameters of pipeline 3\n","    param_grid_3b = {\n","    'clf__C': np.logspace(-3, 1, 20),\n","    }\n","\n","    # Perform grid search with inner cross-validation, part 1\n","    model_3 = RandomizedSearchCV(pipeline_3b, param_distributions=param_grid_3b, cv=inner_cv, scoring='f1', n_iter=50, n_jobs=-1) # optimize parameters\n","    model_3.fit(X_design, y_design)\n","\n","    results = pd.DataFrame(model_3.cv_results_)\n","    results_3 = results_3.append(results,ignore_index=True)\n","\n","    # Define scores AFTER hyperparameter tuning \n","    y_pred_design_3_after = model_3.predict(X_design)\n","    f1_design_3_aft = f1_score(y_design, y_pred_design_3_after)\n","    f1_design_3_after.append(f1_design_3_aft)\n","   \n","    y_pred_test_3_after = model_3.predict(X_test)\n","    y_pred_3_all.append(y_pred_test_3_after)\n","   \n","    f1_test_3_aft = f1_score(y_test_3, y_pred_test_3_after)\n","    f1_test_3_after.append(f1_test_3_aft)\n","\n","\n","    # Access and store the best set of hyperparameters of each outer-CV loop in a DataFrame\n","    best_hp_3 = best_hp_3.append(model_3.best_params_,ignore_index=True)\n","    # Stores the optimum model in best_pipe\n","    best_pipe_3.append(model_3.best_estimator_)\n","\n","# Save results of inner CV into .csv file\n","results_3.to_csv('results_3.csv', index=False)\n","\n","print(f'Mean and std of F1 scores of pipeline 3: {statistics.mean(f1_test_3_after)} +/- {statistics.stdev(f1_test_3_after)}')\n","print(f'The optimal hyperparameters per split: {best_hp_3}')\n","print(f'The best pipes per split {best_pipe_3}')\n","print(f'The design F1 scores before tuning {f1_design_3_before}')\n","print(f'The test F1 scores before tuning {f1_test_3_before}')\n","print(f'The design F1 scores after tuning {f1_design_3_after}')\n","print(f'The test F1 scores afer tuning {f1_test_3_after}')\n","\n","data_3 = [f1_design_3_after, f1_test_3_after]\n","sns.boxplot(data=data_3)\n","plt.title('Boxplot F1 scores design and test pipeline 3')\n","plt.xlabel('design and test')\n","plt.ylabel('F1 score score')\n","plt.show()\n","\n","# Loop over rows and compute precision recall curve for each row\n","for i in range(len(y_pred_3_all)):\n","    precision, recall, thresholds = precision_recall_curve(y_test_3_all[i], y_pred_3_all[i])\n","    auc = average_precision_score(y_test_3_all[i], y_pred_3_all[i])\n","\n","    # Plot the ROC curve for each row\n","    plt.plot(recall, precision, lw=2, label='PR curve it. %d (AP = %0.2f)' % (i+1, auc))\n","\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('Precision-recall curve for all rows')\n","plt.legend(loc=\"lower right\", fontsize=8)\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"ihFqgjuSzsJn","executionInfo":{"status":"error","timestamp":1682091689229,"user_tz":-120,"elapsed":297099,"user":{"displayName":"Julia Binder","userId":"04445038020140268015"}},"outputId":"b6a44b3a-65fc-4d9a-b076-329092550e33"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 9 members, which is less than n_splits=10.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Size_X_design (45, 2000)\n","Size X_test (5, 2000)\n","The total number of outliers in dataset x is 3886\n","The total number of outliers in dataset x is 1012\n","<class 'pandas.core.frame.DataFrame'>\n","            0_0         0_1         0_2         0_3         0_4         0_5  \\\n","0     47.685046   48.416904   37.371002   61.604757   58.732738  125.586049   \n","1    152.726718  103.377522  111.168880   25.419423   14.636085   24.609688   \n","2      1.601260    3.882169   22.978997   21.673040    8.919484   18.329286   \n","3      1.388947    3.052483    3.084103    4.627886   10.016196   15.020347   \n","4      3.625561    3.728466    6.205367   17.722897    5.699401    9.024836   \n","6      6.313823    7.525186   14.898173    7.973962    7.741924    9.925077   \n","7      7.862030   21.680769   25.337318    4.579382    5.601682   18.016299   \n","8      3.933808   14.202943   13.572333   13.162149    6.623755   92.045714   \n","9     65.713418   17.930338    9.035395   11.099228    8.727922    8.879810   \n","10     8.909385   26.552703   45.105873   27.323183   10.273487   30.048169   \n","11   894.420487   47.319198   35.619225   32.931959   25.131503   95.379695   \n","12     6.323226   15.953172   26.120406    5.109627    4.012020   16.225039   \n","13     4.980006    1.113433    3.434486    0.392388    4.746581    0.468230   \n","14     9.501722    5.586242   16.450204    9.001370   13.485390   12.654643   \n","15    18.644972   24.808661   29.530931   34.786282   81.380679   91.139503   \n","16     0.812172   11.987505   19.817093   26.210313   55.224366   73.259071   \n","17  1264.963015   29.076832    4.174963   10.406641    7.447515   38.463177   \n","18   247.728859   82.530058   64.737390  101.079961  110.215813  144.387597   \n","19     0.083161    5.929321    6.917387    6.924807   10.855910   17.821533   \n","20   160.601443  143.422714   38.376866   19.670381   12.283158   17.170820   \n","21   462.167939  103.377522  103.594904   95.428088   81.632232   28.433531   \n","22    41.375439   36.520654   35.163455   28.184115   94.563472   21.907995   \n","23     7.554942   18.251545   19.131640    4.189196   13.567905   13.140102   \n","25     2.291710    3.039610    5.481645    9.873051    3.657961    7.788805   \n","26   385.875221   28.181637   19.201062   16.526245   14.160023   18.202051   \n","27   757.622520   14.768699   16.772315    7.756356   13.015541   36.445433   \n","28   142.319356   40.861111   14.311447   12.825285   10.168552   15.789614   \n","30   343.675806   48.950335   56.999643   10.091110    4.373090    6.670339   \n","31   629.027392  103.377522  107.449684   70.781661   72.694683  105.637914   \n","32   866.953418  103.377522  103.594904   53.340233  107.875803   39.668026   \n","33     7.547616    4.441597    8.504461    9.263530    5.448589    4.011405   \n","35     2.362146    6.863112   22.550274   26.931418  105.863752   34.476580   \n","36     9.161991   10.856414   15.152315   11.796925   17.957884   25.792741   \n","38   629.027392   88.885184  110.685906   20.589209   16.015180   30.676678   \n","39    11.071519    7.924814    6.367171    2.884096    9.385375    8.634683   \n","40   629.027392   21.596540  103.594904   43.813298  110.215813   33.964272   \n","41   939.366407  103.377522   71.517571   36.033812   34.987714   93.368641   \n","42     4.459459    9.978440   22.775076   30.872979   36.263121   54.213621   \n","43    13.818771   12.861720    7.176237   23.215493   45.191018  139.527824   \n","44     2.044676   12.641916   19.011975   10.018019   14.470866    5.059857   \n","45  1009.570351  164.825685  103.594904   45.532984   33.355747   19.186123   \n","46   629.027392  135.084186   31.630025   42.269784   23.539116   86.860689   \n","47     5.076651    0.396899   18.727030    4.525304   21.604739   40.037709   \n","48     5.212389   22.435944   30.438124   25.494512   27.480230   19.577305   \n","49     0.835017    6.575501   12.352972   35.881337   20.712190   49.123780   \n","\n","           0_6         0_7         0_8        0_9  ...     2_490     2_491  \\\n","0   120.890451   40.100639   19.106303   6.043738  ...  0.358220  0.394612   \n","1    49.533842  126.298808  114.172275  68.780449  ...  0.214195  0.346208   \n","2    27.710604   23.350084   37.896254   8.331180  ...  1.084666  0.161850   \n","3   139.823127   20.149108   10.393109   9.241921  ...  0.651411  0.714947   \n","4    15.782812   33.336480   55.257804  19.139128  ...  0.114396  0.354976   \n","6    19.325956   68.563423   15.139322  11.386458  ...  0.582105  0.730925   \n","7    66.027558   23.029366    5.820312   3.542084  ...  0.706435  0.297448   \n","8    30.067347   26.844976   10.494297  35.291274  ...  1.159549  0.658967   \n","9     8.097210   44.556589   47.113102  21.933059  ...  0.398205  0.245733   \n","10   35.091876   45.291780   20.100238   8.416225  ...  0.311644  0.227725   \n","11   81.966685   36.308177   17.355844  10.090681  ...  0.096375  0.122703   \n","12  114.460883    8.289232    4.147352   5.958195  ...  0.420808  0.370241   \n","13   11.634919   61.111967   12.213330   9.727997  ...  0.174855  0.509130   \n","14    6.836319   37.801792   87.602074  23.830667  ...  0.357896  0.024080   \n","15  219.065067   99.557117  114.172275  68.780449  ...  1.368502  1.327078   \n","16   44.214046   31.987609    9.732927  26.773551  ...  0.872281  0.371031   \n","17   82.489406   38.985256   16.685890  31.899988  ...  0.626253  0.756545   \n","18   52.225949   43.735204  114.172275  68.780449  ...  0.776095  0.998492   \n","19   51.306309   70.669414   30.232692  22.993600  ...  0.457623  0.635237   \n","20   15.512118  111.673362    4.522202   8.525927  ...  1.914944  1.652935   \n","21   50.453163   62.488043   72.735797  52.063825  ...  0.264310  0.277353   \n","22    3.091364    6.026701   11.166130  10.255603  ...  0.786101  0.339428   \n","23    2.874134    6.775483    5.447489  62.872336  ...  1.082336  1.203700   \n","25   46.180684   17.594202    6.703946   7.967750  ...  0.732570  0.879275   \n","26   24.457990   61.877175  111.923601  23.398021  ...  0.098929  0.228524   \n","27   38.455515   12.016985    5.105857   2.548889  ...  0.176093  0.374499   \n","28   12.107519  126.298808   25.259786   1.638292  ...  1.681141  1.203190   \n","30   55.156363    4.515468    5.253856   2.463021  ...  0.162845  0.405848   \n","31   47.986717   18.116520   12.969337  21.154639  ...  0.102591  0.046813   \n","32   68.127807   35.020284   36.985907  63.162215  ...  0.868401  1.237484   \n","33   10.682050   18.004418    8.953420  68.780449  ...  0.826393  0.213216   \n","35   23.091628    5.187541   12.055103  20.776187  ...  0.766081  0.433455   \n","36   84.025145   15.480995   10.164311  14.735534  ...  0.032671  0.072995   \n","38   20.490327   58.920433  106.431948  26.090899  ...  0.348429  0.085254   \n","39   90.831259    4.567827    4.655512   8.270545  ...  1.127433  0.520051   \n","40   72.461802   65.642596   40.088273  16.303774  ...  0.133208  0.196779   \n","41   44.387696   33.029416   16.736460  29.516598  ...  0.271802  0.350006   \n","42   12.953561    8.607844    9.163925   8.467530  ...  0.904045  0.442346   \n","43   54.492808   38.533570   14.830765   7.000797  ...  0.555260  0.283301   \n","44    3.315126    5.018288   54.628473  27.966706  ...  0.216970  1.005113   \n","45   13.055429   24.261486   40.078577  21.321404  ...  0.259497  0.219041   \n","46   23.142288   18.125929   10.131506  18.044463  ...  0.111183  0.069266   \n","47   99.842998   38.261544   10.927228   4.330965  ...  0.419429  0.717668   \n","48   21.970321   34.466662    6.322254   2.679117  ...  0.809580  0.377657   \n","49   82.917623   58.757996    8.723075  11.815731  ...  0.554465  0.721695   \n","\n","       2_492     2_493     2_494     2_495     2_496     2_497     2_498  \\\n","0   0.107830  0.425291  0.941574  0.526005  0.306785  0.560049  0.350928   \n","1   0.477954  0.076745  0.172020  0.257445  0.456032  0.486665  0.315492   \n","2   0.306773  0.415093  0.317944  0.265990  0.113143  0.709653  1.012296   \n","3   0.125822  0.643305  0.320884  0.046181  0.322935  0.348999  0.153024   \n","4   0.434708  1.055659  0.547044  0.143111  0.484099  0.336949  0.544538   \n","6   0.605998  0.831952  0.895054  0.767637  0.888651  0.876977  1.308258   \n","7   0.336976  0.642969  0.741255  0.614384  0.492811  0.933041  0.302152   \n","8   0.813533  0.926121  0.649347  0.511128  0.409690  0.293529  0.772780   \n","9   0.433560  0.740165  0.782075  1.319605  0.997819  0.311671  0.175690   \n","10  0.155983  0.110273  0.182669  0.175323  0.234637  0.177746  0.146612   \n","11  0.113910  0.021568  0.174755  0.143924  0.241262  0.052990  0.141293   \n","12  0.393852  0.455820  0.467426  0.564135  0.265910  0.554738  0.828600   \n","13  0.135521  0.063941  0.249348  0.356608  0.367951  0.174740  0.380145   \n","14  0.431905  0.663539  0.912721  0.262156  1.035282  0.538613  0.382229   \n","15  1.090115  1.189827  1.386800  1.159281  1.106265  1.203004  1.552537   \n","16  0.416056  0.225756  0.388362  1.129571  0.313923  0.213291  0.659616   \n","17  0.737900  0.896208  0.836594  0.633173  0.842181  0.501235  0.822268   \n","18  0.843273  0.964916  0.925053  0.910381  0.921904  0.843743  0.983892   \n","19  0.333805  0.690953  0.848387  0.253394  0.943507  0.702775  0.588625   \n","20  1.704367  1.741195  1.835064  1.830482  1.617813  1.849128  1.898409   \n","21  0.158751  0.140927  0.247623  0.207862  0.081059  0.076047  0.144896   \n","22  0.895299  0.266459  0.115387  0.271458  0.466967  0.760682  0.591807   \n","23  0.447047  0.344257  0.105696  0.777201  0.113358  1.058979  0.565462   \n","25  1.257180  0.437430  0.881269  0.042347  0.810611  0.256283  0.888381   \n","26  0.214828  0.080228  0.102691  0.165870  0.093455  0.124307  0.076448   \n","27  0.642888  0.699057  0.370144  0.455704  0.179700  0.843407  0.576035   \n","28  0.359781  1.255172  1.692066  1.233744  0.550624  1.284502  1.671745   \n","30  0.230583  0.207286  0.038841  0.102040  0.313409  0.223369  0.406877   \n","31  0.069053  0.133518  0.122981  0.070043  0.051580  0.109471  0.137101   \n","32  1.034963  1.172944  0.996569  1.151716  0.913109  1.038909  0.844756   \n","33  0.366293  0.201832  0.310691  0.528135  0.420128  0.209581  0.246273   \n","35  0.566932  0.356422  0.486543  0.362846  0.608656  0.276555  0.546481   \n","36  0.038886  0.011217  0.044853  0.077136  0.057671  0.107105  0.071548   \n","38  0.221490  0.237556  0.443707  0.048757  0.259962  0.272971  0.385217   \n","39  0.698503  0.373569  0.387495  0.616913  0.153032  0.600710  0.773475   \n","40  0.099257  0.158684  0.119397  0.107522  0.052266  0.080984  0.104612   \n","41  0.273330  0.291877  0.343993  0.558353  0.252498  0.216288  0.647566   \n","42  0.564042  0.228305  0.636147  0.465784  0.727732  0.433457  0.272619   \n","43  0.469768  0.674416  0.483574  0.641788  0.548664  0.063886  0.231984   \n","44  0.405524  0.238644  0.749902  0.236529  0.034603  0.322453  0.623932   \n","45  0.121233  0.154750  0.543235  0.289687  0.045632  0.237090  0.114697   \n","46  0.050891  0.094307  0.094286  0.025399  0.067310  0.205254  0.117588   \n","47  0.249964  0.118048  0.350482  0.602391  0.289237  0.099252  1.339910   \n","48  0.490011  0.583011  0.636926  0.257066  0.543998  0.293776  0.532469   \n","49  0.834070  0.573318  0.481451  0.473756  0.258202  0.379576  0.245187   \n","\n","       2_499  \n","0   0.119156  \n","1   0.041322  \n","2   0.127475  \n","3   0.270474  \n","4   0.928419  \n","6   0.222228  \n","7   0.275812  \n","8   0.740404  \n","9   0.504363  \n","10  0.144051  \n","11  0.150844  \n","12  0.742125  \n","13  0.168297  \n","14  0.311858  \n","15  1.079688  \n","16  0.806355  \n","17  0.524951  \n","18  0.938260  \n","19  0.303037  \n","20  2.259685  \n","21  0.053389  \n","22  0.206362  \n","23  0.594708  \n","25  0.529242  \n","26  0.278819  \n","27  0.599061  \n","28  1.341713  \n","30  0.290698  \n","31  0.183540  \n","32  1.217628  \n","33  0.438183  \n","35  0.714352  \n","36  0.104164  \n","38  0.362952  \n","39  0.241303  \n","40  0.191639  \n","41  1.063467  \n","42  0.372647  \n","43  0.864135  \n","44  1.111741  \n","45  0.154504  \n","46  0.056213  \n","47  0.274666  \n","48  0.292245  \n","49  0.388276  \n","\n","[45 rows x 2000 columns]\n","<class 'pandas.core.frame.DataFrame'>\n","          0_0       0_1        0_2        0_3        0_4        0_5  \\\n","5    7.897582  4.750754   5.923150   3.999941   7.348004   3.336407   \n","24   6.263398  6.768979   2.564803   3.499548   5.535409  16.535461   \n","29  14.386177  9.544512  14.845894  16.957371  55.818256  70.136786   \n","34   8.711248  5.446407   5.967202   6.607189  28.552336  73.130378   \n","37   5.233561  3.832656   3.096513   8.760333   2.037096  10.374480   \n","\n","          0_6        0_7        0_8        0_9  ...     2_490     2_491  \\\n","5    9.475968  29.113818  11.802082   4.880755  ...  0.923439  0.480057   \n","24  64.552979  13.382436  12.472009   9.521238  ...  0.278984  0.834699   \n","29  36.563568  23.705583   3.889423  24.028924  ...  0.199977  0.345672   \n","34  28.400980   5.935044   7.377658  18.182313  ...  1.089200  0.892079   \n","37  22.269579  65.668801  27.277003   3.746374  ...  0.547558  0.060141   \n","\n","       2_492     2_493     2_494     2_495     2_496     2_497     2_498  \\\n","5   1.352786  0.365713  0.711593  0.396934  0.499613  0.208698  1.025403   \n","24  0.764613  0.613606  0.309103  0.216962  0.072947  1.404239  0.520531   \n","29  0.089255  0.197561  0.182011  0.131626  0.239856  0.355817  0.132830   \n","34  0.366810  0.646200  0.770653  0.269622  0.263445  0.560603  0.416983   \n","37  0.443698  1.153577  0.682995  0.238065  0.628543  0.676422  0.533781   \n","\n","       2_499  \n","5   0.752997  \n","24  0.371348  \n","29  0.139394  \n","34  0.352413  \n","37  0.536429  \n","\n","[5 rows x 2000 columns]\n","shape after balancing (72, 2000)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 1 is smaller than n_iter=10. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [nan]\n","  warnings.warn(\n","<ipython-input-59-707daa621038>:65: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  best_hp_3_before = best_hp_3_before.append(rand_search_3a.best_params_,ignore_index=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 20 is smaller than n_iter=50. Running 20 iterations. For exhaustive searches, use GridSearchCV.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["shape of X_design after pca (72, 2)\n","size of features selected(1,)\n","shape of X_design after univariate: (72, 1)\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-59-707daa621038>:107: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  results_3 = results_3.append(results,ignore_index=True)\n","<ipython-input-59-707daa621038>:122: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  best_hp_3 = best_hp_3.append(model_3.best_params_,ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["Size_X_design (45, 2000)\n","Size X_test (5, 2000)\n","The total number of outliers in dataset x is 3694\n","The total number of outliers in dataset x is 874\n","<class 'pandas.core.frame.DataFrame'>\n","           0_0         0_1         0_2        0_3         0_4         0_5  \\\n","0    47.685046   48.416904   37.371002  61.604757   58.732738  165.331154   \n","1   152.726718  106.220497  111.168880  25.419423   14.636085   24.609688   \n","2     1.601260    3.882169   22.978997  21.673040    8.919484   18.329286   \n","4     3.625561    3.728466    6.205367  17.722897    5.699401    9.024836   \n","5     7.897582    4.750754    5.923150   3.999941    7.348004    3.336407   \n","6     6.313823    7.525186   14.898173   7.973962    7.741924    9.925077   \n","7     7.862030   21.680769   25.337318   4.579382    5.601682   18.016299   \n","8     3.933808   14.202943   13.572333  13.162149    6.623755   92.045714   \n","9    65.713418   17.930338    9.035395  11.099228    8.727922    8.879810   \n","11  894.420487   47.319198   35.619225  32.931959   25.131503   95.379695   \n","12    6.323226   15.953172   26.120406   5.109627    4.012020   16.225039   \n","13    4.980006    1.113433    3.434486   0.392388    4.746581    0.468230   \n","14    9.501722    5.586242   16.450204   9.001370   13.485390   12.654643   \n","15   18.644972   24.808661   29.530931  34.786282   81.380679   91.139503   \n","16    0.812172   11.987505   19.817093  26.210313   55.224366   73.259071   \n","17  565.135178   29.076832    4.174963  10.406641    7.447515   38.463177   \n","18  247.728859   82.530058   64.737390  98.292045  120.308390  144.387597   \n","19    0.083161    5.929321    6.917387   6.924807   10.855910   17.821533   \n","20  160.601443  143.422714   38.376866  19.670381   12.283158   17.170820   \n","21  462.167939  106.220497  119.125112  95.428088   81.632232   28.433531   \n","22   41.375439   36.520654   35.163455  28.184115   94.563472   21.907995   \n","24    6.263398    6.768979    2.564803   3.499548    5.535409   16.535461   \n","25    2.291710    3.039610    5.481645   9.873051    3.657961    7.788805   \n","27  757.622520   14.768699   16.772315   7.756356   13.015541   36.445433   \n","28  142.319356   40.861111   14.311447  12.825285   10.168552   15.789614   \n","29  197.409047   54.839062   39.124339  31.364328   55.818256   70.136786   \n","30  343.675806   48.950335   56.999643  10.091110    4.373090    6.670339   \n","31  565.135178  106.220497  107.449684  70.781661   72.694683  105.637914   \n","32  866.953418  106.220497  119.125112  53.340233  107.875803   39.668026   \n","33    7.547616    4.441597    8.504461   9.263530    5.448589    4.011405   \n","34    8.711248    5.446407    5.967202   6.607189   28.552336   73.130378   \n","35    2.362146    6.863112   22.550274  26.931418  105.863752   34.476580   \n","36    9.161991   10.856414   15.152315  11.796925   17.957884   25.792741   \n","37    5.233561    3.832656    3.096513   8.760333    2.037096   10.374480   \n","38  565.135178   88.885184  110.685906  20.589209   16.015180   30.676678   \n","39   11.071519    7.924814    6.367171   2.884096    9.385375    8.634683   \n","40  565.135178   21.596540  119.125112  43.813298  120.308390   33.964272   \n","41  939.366407  106.220497   71.517571  36.033812   34.987714   93.368641   \n","42    4.459459    9.978440   22.775076  30.872979   36.263121   54.213621   \n","43   13.818771   12.861720    7.176237  23.215493   45.191018  139.527824   \n","44    2.044676   12.641916   19.011975  10.018019   14.470866    5.059857   \n","45  565.135178  164.825685  119.125112  45.532984   33.355747   19.186123   \n","47    5.076651    0.396899   18.727030   4.525304   21.604739   40.037709   \n","48    5.212389   22.435944   30.438124  25.494512   27.480230   19.577305   \n","49    0.835017    6.575501   12.352972  35.881337   20.712190   49.123780   \n","\n","           0_6         0_7         0_8        0_9  ...     2_490     2_491  \\\n","0   120.890451   40.100639   19.106303   6.043738  ...  0.358220  0.394612   \n","1    49.533842  126.970791   91.633093  70.972009  ...  0.214195  0.346208   \n","2    27.710604   23.350084   37.896254   8.331180  ...  1.084666  0.161850   \n","4    15.782812   33.336480   55.257804  19.139128  ...  0.114396  0.354976   \n","5     9.475968   29.113818   11.802082   4.880755  ...  0.923439  0.480057   \n","6    19.325956   68.563423   15.139322  11.386458  ...  0.582105  0.730925   \n","7    66.027558   23.029366    5.820312   3.542084  ...  0.706435  0.297448   \n","8    30.067347   26.844976   10.494297  35.291274  ...  1.159549  0.658967   \n","9     8.097210   44.556589   47.113102  21.933059  ...  0.398205  0.245733   \n","11   81.966685   36.308177   17.355844  10.090681  ...  0.096375  0.122703   \n","12  114.460883    8.289232    4.147352   5.958195  ...  0.420808  0.370241   \n","13   11.634919   61.111967   12.213330   9.727997  ...  0.174855  0.509130   \n","14    6.836319   37.801792   87.602074  23.830667  ...  0.357896  0.024080   \n","15  219.065067   99.557117   91.633093  70.972009  ...  1.368502  1.327078   \n","16   44.214046   31.987609    9.732927  26.773551  ...  0.872281  0.371031   \n","17   82.489406   38.985256   16.685890  31.899988  ...  0.626253  0.756545   \n","18   52.225949   43.735204   91.633093  70.972009  ...  0.776095  0.998492   \n","19   51.306309   70.669414   30.232692  22.993600  ...  0.457623  0.635237   \n","20   15.512118  111.673362    4.522202   8.525927  ...  1.914944  1.652935   \n","21   50.453163   62.488043   72.735797  52.063825  ...  0.264310  0.277353   \n","22    3.091364    6.026701   11.166130  10.255603  ...  0.786101  0.339428   \n","24  111.679516   13.382436   12.472009   9.521238  ...  0.278984  0.834699   \n","25   46.180684   17.594202    6.703946   7.967750  ...  0.732570  0.879275   \n","27   38.455515   12.016985    5.105857   2.548889  ...  0.176093  0.374499   \n","28   12.107519  126.970791   25.259786   1.638292  ...  1.681141  1.203190   \n","29   36.563568   23.705583    3.889423  24.028924  ...  0.199977  0.345672   \n","30   55.156363    4.515468    5.253856   2.463021  ...  0.162845  0.405848   \n","31   47.986717   18.116520   12.969337  21.154639  ...  0.102591  0.046813   \n","32   68.127807   35.020284   36.985907  63.162215  ...  0.868401  1.237484   \n","33   10.682050   18.004418    8.953420  70.972009  ...  0.826393  0.213216   \n","34   28.400980    5.935044    7.377658  18.182313  ...  1.089200  0.892079   \n","35   23.091628    5.187541   12.055103  20.776187  ...  0.766081  0.433455   \n","36   84.025145   15.480995   10.164311  14.735534  ...  0.032671  0.072995   \n","37   22.269579   90.313724   27.277003   3.746374  ...  0.547558  0.060141   \n","38   20.490327   58.920433  106.431948  26.090899  ...  0.348429  0.085254   \n","39   90.831259    4.567827    4.655512   8.270545  ...  1.127433  0.520051   \n","40   72.461802   65.642596   40.088273  16.303774  ...  0.133208  0.196779   \n","41   44.387696   33.029416   16.736460  29.516598  ...  0.271802  0.350006   \n","42   12.953561    8.607844    9.163925   8.467530  ...  0.904045  0.442346   \n","43   54.492808   38.533570   14.830765   7.000797  ...  0.555260  0.283301   \n","44    3.315126    5.018288   54.628473  27.966706  ...  0.216970  1.005113   \n","45   13.055429   24.261486   40.078577  21.321404  ...  0.259497  0.219041   \n","47   99.842998   38.261544   10.927228   4.330965  ...  0.419429  0.717668   \n","48   21.970321   34.466662    6.322254   2.679117  ...  0.809580  0.377657   \n","49   82.917623   58.757996    8.723075  11.815731  ...  0.554465  0.721695   \n","\n","       2_492     2_493     2_494     2_495     2_496     2_497     2_498  \\\n","0   0.107830  0.425291  0.941574  0.526005  0.306785  0.560049  0.350928   \n","1   0.477954  0.076745  0.172020  0.257445  0.456032  0.486665  0.315492   \n","2   0.306773  0.415093  0.317944  0.265990  0.113143  0.709653  1.012296   \n","4   0.434708  1.055659  0.547044  0.143111  0.484099  0.336949  0.544538   \n","5   1.352786  0.365713  0.711593  0.606949  0.499613  0.208698  1.025403   \n","6   0.605998  0.831952  0.895054  0.767637  0.888651  0.876977  1.308258   \n","7   0.336976  0.642969  0.741255  0.614384  0.492811  0.933041  0.302152   \n","8   0.813533  0.926121  0.649347  0.511128  0.409690  0.293529  0.772780   \n","9   0.433560  0.740165  0.782075  1.319605  0.997819  0.311671  0.175690   \n","11  0.113910  0.021568  0.174755  0.143924  0.241262  0.052990  0.141293   \n","12  0.393852  0.455820  0.467426  0.564135  0.265910  0.554738  0.828600   \n","13  0.135521  0.063941  0.249348  0.356608  0.367951  0.174740  0.380145   \n","14  0.431905  0.663539  0.912721  0.262156  1.035282  0.538613  0.382229   \n","15  1.090115  1.189827  1.386800  1.159281  1.106265  1.203004  1.552537   \n","16  0.416056  0.225756  0.388362  1.129571  0.313923  0.213291  0.659616   \n","17  0.737900  0.896208  0.836594  0.633173  0.842181  0.501235  0.822268   \n","18  0.843273  0.964916  0.925053  0.910381  0.921904  0.843743  0.983892   \n","19  0.333805  0.690953  0.848387  0.253394  0.943507  0.702775  0.588625   \n","20  1.800877  1.741195  1.835064  1.740321  1.617813  1.849128  1.898409   \n","21  0.158751  0.140927  0.247623  0.207862  0.081059  0.076047  0.144896   \n","22  0.895299  0.266459  0.115387  0.271458  0.466967  0.760682  0.591807   \n","24  0.764613  0.613606  0.309103  0.216962  0.072947  1.404239  0.520531   \n","25  1.257180  0.437430  0.881269  0.042347  0.810611  0.256283  0.888381   \n","27  0.642888  0.699057  0.370144  0.455704  0.179700  0.843407  0.576035   \n","28  0.359781  1.255172  1.692066  1.233744  0.550624  1.284502  1.671745   \n","29  0.089255  0.197561  0.182011  0.131626  0.239856  0.355817  0.132830   \n","30  0.230583  0.207286  0.038841  0.102040  0.313409  0.223369  0.406877   \n","31  0.069053  0.133518  0.122981  0.070043  0.051580  0.109471  0.137101   \n","32  1.034963  1.172944  0.996569  1.151716  0.913109  1.038909  0.844756   \n","33  0.366293  0.201832  0.310691  0.528135  0.420128  0.209581  0.246273   \n","34  0.366810  0.646200  0.770653  0.269622  0.263445  0.560603  0.416983   \n","35  0.566932  0.356422  0.486543  0.362846  0.608656  0.276555  0.546481   \n","36  0.038886  0.011217  0.044853  0.077136  0.057671  0.107105  0.071548   \n","37  0.443698  1.153577  0.682995  0.238065  0.628543  0.676422  0.533781   \n","38  0.221490  0.237556  0.443707  0.048757  0.259962  0.272971  0.385217   \n","39  0.698503  0.373569  0.387495  0.616913  0.153032  0.600710  0.773475   \n","40  0.099257  0.158684  0.119397  0.107522  0.052266  0.080984  0.104612   \n","41  0.273330  0.291877  0.343993  0.558353  0.252498  0.216288  0.647566   \n","42  0.564042  0.228305  0.636147  0.465784  0.727732  0.433457  0.272619   \n","43  0.469768  0.674416  0.483574  0.641788  0.548664  0.063886  0.231984   \n","44  0.405524  0.238644  0.749902  0.236529  0.034603  0.322453  0.623932   \n","45  0.121233  0.154750  0.543235  0.289687  0.045632  0.237090  0.114697   \n","47  0.249964  0.118048  0.350482  0.602391  0.289237  0.099252  1.339910   \n","48  0.490011  0.583011  0.636926  0.257066  0.543998  0.293776  0.532469   \n","49  0.834070  0.573318  0.481451  0.473756  0.258202  0.379576  0.245187   \n","\n","       2_499  \n","0   0.119156  \n","1   0.041322  \n","2   0.127475  \n","4   0.928419  \n","5   1.257160  \n","6   0.222228  \n","7   0.275812  \n","8   0.740404  \n","9   0.504363  \n","11  0.150844  \n","12  0.742125  \n","13  0.168297  \n","14  0.311858  \n","15  1.079688  \n","16  0.806355  \n","17  0.524951  \n","18  0.938260  \n","19  0.303037  \n","20  2.259685  \n","21  0.053389  \n","22  0.206362  \n","24  0.371348  \n","25  0.529242  \n","27  0.599061  \n","28  1.341713  \n","29  0.139394  \n","30  0.290698  \n","31  0.183540  \n","32  1.217628  \n","33  0.438183  \n","34  0.352413  \n","35  0.714352  \n","36  0.104164  \n","37  0.536429  \n","38  0.362952  \n","39  0.241303  \n","40  0.191639  \n","41  1.063467  \n","42  0.372647  \n","43  0.864135  \n","44  1.111741  \n","45  0.154504  \n","47  0.274666  \n","48  0.292245  \n","49  0.388276  \n","\n","[45 rows x 2000 columns]\n","<class 'pandas.core.frame.DataFrame'>\n","           0_0        0_1        0_2        0_3        0_4        0_5  \\\n","3     1.388947   3.052483   3.084103   4.627886  10.016196  15.020347   \n","10    8.909385  26.552703  45.105873  27.323183  10.273487  30.048169   \n","23    7.554942  18.251545  19.131640   4.189196  13.567905  13.140102   \n","26  385.875221  28.181637  19.201062  16.526245  14.160023  18.202051   \n","46  394.563046  64.484407  31.630025  42.269784  23.539116  41.003463   \n","\n","          0_6        0_7        0_8        0_9  ...     2_490     2_491  \\\n","3   54.240098  20.149108  10.393109   9.241921  ...  0.651411  0.714947   \n","10  35.091876  45.291780  20.100238   8.416225  ...  0.311644  0.227725   \n","23   2.874134   6.775483   5.447489  62.872336  ...  1.082336  1.203700   \n","26  24.457990  61.877175  24.398059  23.398021  ...  0.098929  0.228524   \n","46  23.142288  18.125929  10.131506  18.044463  ...  0.111183  0.069266   \n","\n","       2_492     2_493     2_494     2_495     2_496     2_497     2_498  \\\n","3   0.125822  0.643305  0.320884  0.046181  0.322935  0.348999  0.153024   \n","10  0.155983  0.110273  0.182669  0.175323  0.234637  0.177746  0.146612   \n","23  0.447047  0.344257  0.105696  0.549974  0.113358  0.471602  0.270950   \n","26  0.214828  0.080228  0.102691  0.165870  0.093455  0.124307  0.076448   \n","46  0.050891  0.094307  0.094286  0.025399  0.067310  0.205254  0.117588   \n","\n","       2_499  \n","3   0.270474  \n","10  0.144051  \n","23  0.594708  \n","26  0.278819  \n","46  0.056213  \n","\n","[5 rows x 2000 columns]\n","shape after balancing (74, 2000)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 1 is smaller than n_iter=10. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [nan]\n","  warnings.warn(\n","<ipython-input-59-707daa621038>:65: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  best_hp_3_before = best_hp_3_before.append(rand_search_3a.best_params_,ignore_index=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 20 is smaller than n_iter=50. Running 20 iterations. For exhaustive searches, use GridSearchCV.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["shape of X_design after pca (74, 2)\n","size of features selected(1,)\n","shape of X_design after univariate: (74, 1)\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-59-707daa621038>:107: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  results_3 = results_3.append(results,ignore_index=True)\n","<ipython-input-59-707daa621038>:122: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  best_hp_3 = best_hp_3.append(model_3.best_params_,ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["Size_X_design (45, 2000)\n","Size X_test (5, 2000)\n","The total number of outliers in dataset x is 3812\n","The total number of outliers in dataset x is 734\n","<class 'pandas.core.frame.DataFrame'>\n","           0_0         0_1         0_2        0_3         0_4         0_5  \\\n","0    47.685046   48.416904   37.371002  61.604757   58.732738  130.180696   \n","1   152.726718  128.294652  111.168880  25.419423   14.636085   24.609688   \n","2     1.601260    3.882169   22.978997  21.673040    8.919484   18.329286   \n","3     1.388947    3.052483    3.084103   4.627886   10.016196   15.020347   \n","4     3.625561    3.728466    6.205367  17.722897    5.699401    9.024836   \n","5     7.897582    4.750754    5.923150   3.999941    7.348004    3.336407   \n","6     6.313823    7.525186   14.898173   7.973962    7.741924    9.925077   \n","7     7.862030   21.680769   25.337318   4.579382    5.601682   18.016299   \n","8     3.933808   14.202943   13.572333  13.162149    6.623755   92.045714   \n","9    65.713418   17.930338    9.035395  11.099228    8.727922    8.879810   \n","10    8.909385   26.552703   45.105873  27.323183   10.273487   30.048169   \n","11  894.420487   47.319198   35.619225  32.931959   25.131503   95.379695   \n","12    6.323226   15.953172   26.120406   5.109627    4.012020   16.225039   \n","14    9.501722    5.586242   16.450204   9.001370   13.485390   12.654643   \n","15   18.644972   24.808661   29.530931  34.786282   81.380679   91.139503   \n","17  400.631989   29.076832    4.174963  10.406641    7.447515   38.463177   \n","18  247.728859   82.530058   64.737390  90.225452   89.280974  144.387597   \n","19    0.083161    5.929321    6.917387   6.924807   10.855910   17.821533   \n","20  160.601443  143.422714   38.376866  19.670381   12.283158   17.170820   \n","21  462.167939  128.294652  119.496251  95.428088   81.632232   28.433531   \n","22   41.375439   36.520654   35.163455  28.184115   94.563472   21.907995   \n","23    7.554942   18.251545   19.131640   4.189196   13.567905   13.140102   \n","24    6.263398    6.768979    2.564803   3.499548    5.535409   16.535461   \n","25    2.291710    3.039610    5.481645   9.873051    3.657961    7.788805   \n","26  385.875221   28.181637   19.201062  16.526245   14.160023   18.202051   \n","28  142.319356   40.861111   14.311447  12.825285   10.168552   15.789614   \n","29  197.409047   54.839062   39.124339  31.364328   55.818256   70.136786   \n","30  343.675806   48.950335   56.999643  10.091110    4.373090    6.670339   \n","31  400.631989  128.294652  107.449684  70.781661   72.694683  105.637914   \n","33    7.547616    4.441597    8.504461   9.263530    5.448589    4.011405   \n","34    8.711248    5.446407    5.967202   6.607189   28.552336   73.130378   \n","35    2.362146    6.863112   22.550274  26.931418  105.863752   34.476580   \n","36    9.161991   10.856414   15.152315  11.796925   17.957884   25.792741   \n","37    5.233561    3.832656    3.096513   8.760333    2.037096   10.374480   \n","38  400.631989   88.885184  110.685906  20.589209   16.015180   30.676678   \n","39   11.071519    7.924814    6.367171   2.884096    9.385375    8.634683   \n","40  400.631989   21.596540  119.496251  43.813298   89.280974   33.964272   \n","41  939.366407  128.294652   71.517571  36.033812   34.987714   93.368641   \n","43   13.818771   12.861720    7.176237  23.215493   45.191018  139.527824   \n","44    2.044676   12.641916   19.011975  10.018019   14.470866    5.059857   \n","45  400.631989  164.825685  119.496251  45.532984   33.355747   19.186123   \n","46  400.631989  135.084186   31.630025  42.269784   23.539116   86.860689   \n","47    5.076651    0.396899   18.727030   4.525304   21.604739   40.037709   \n","48    5.212389   22.435944   30.438124  25.494512   27.480230   19.577305   \n","49    0.835017    6.575501   12.352972  35.881337   20.712190   49.123780   \n","\n","           0_6         0_7         0_8        0_9  ...     2_490     2_491  \\\n","0   120.890451   40.100639   19.106303   6.043738  ...  0.358220  0.394612   \n","1    49.533842  126.298808   91.633093  68.071148  ...  0.214195  0.346208   \n","2    27.710604   23.350084   37.896254   8.331180  ...  1.084666  0.161850   \n","3   139.823127   20.149108   10.393109   9.241921  ...  0.651411  0.714947   \n","4    15.782812   33.336480   55.257804  19.139128  ...  0.114396  0.354976   \n","5     9.475968   29.113818   11.802082   4.880755  ...  0.923439  0.480057   \n","6    19.325956   68.563423   15.139322  11.386458  ...  0.582105  0.730925   \n","7    66.027558   23.029366    5.820312   3.542084  ...  0.706435  0.297448   \n","8    30.067347   26.844976   10.494297  35.291274  ...  1.159549  0.658967   \n","9     8.097210   44.556589   47.113102  21.933059  ...  0.398205  0.245733   \n","10   35.091876   45.291780   20.100238   8.416225  ...  0.311644  0.227725   \n","11   81.966685   36.308177   17.355844  10.090681  ...  0.096375  0.122703   \n","12  114.460883    8.289232    4.147352   5.958195  ...  0.420808  0.370241   \n","14    6.836319   37.801792   87.602074  23.830667  ...  0.357896  0.024080   \n","15  215.223967   99.557117   91.633093  68.071148  ...  1.368502  1.327078   \n","17   82.489406   38.985256   16.685890  31.899988  ...  0.626253  0.756545   \n","18   52.225949   43.735204   91.633093  68.071148  ...  0.776095  0.998492   \n","19   51.306309   70.669414   30.232692  22.993600  ...  0.457623  0.635237   \n","20   15.512118  111.673362    4.522202   8.525927  ...  1.914944  1.652935   \n","21   50.453163   62.488043   72.735797  52.063825  ...  0.264310  0.277353   \n","22    3.091364    6.026701   11.166130  10.255603  ...  0.786101  0.339428   \n","23    2.874134    6.775483    5.447489  62.872336  ...  1.082336  1.203700   \n","24  111.679516   13.382436   12.472009   9.521238  ...  0.278984  0.834699   \n","25   46.180684   17.594202    6.703946   7.967750  ...  0.732570  0.879275   \n","26   24.457990   61.877175  111.923601  23.398021  ...  0.098929  0.228524   \n","28   12.107519  126.298808   25.259786   1.638292  ...  1.681141  1.203190   \n","29   36.563568   23.705583    3.889423  24.028924  ...  0.199977  0.345672   \n","30   55.156363    4.515468    5.253856   2.463021  ...  0.162845  0.405848   \n","31   47.986717   18.116520   12.969337  21.154639  ...  0.102591  0.046813   \n","33   10.682050   18.004418    8.953420  68.071148  ...  0.826393  0.213216   \n","34   28.400980    5.935044    7.377658  18.182313  ...  1.089200  0.892079   \n","35   23.091628    5.187541   12.055103  20.776187  ...  0.766081  0.433455   \n","36   84.025145   15.480995   10.164311  14.735534  ...  0.032671  0.072995   \n","37   22.269579   90.313724   27.277003   3.746374  ...  0.547558  0.060141   \n","38   20.490327   58.920433  106.431948  26.090899  ...  0.348429  0.085254   \n","39   90.831259    4.567827    4.655512   8.270545  ...  1.127433  0.520051   \n","40   72.461802   65.642596   40.088273  16.303774  ...  0.133208  0.196779   \n","41   44.387696   33.029416   16.736460  29.516598  ...  0.271802  0.350006   \n","43   54.492808   38.533570   14.830765   7.000797  ...  0.555260  0.283301   \n","44    3.315126    5.018288   54.628473  27.966706  ...  0.216970  1.005113   \n","45   13.055429   24.261486   40.078577  21.321404  ...  0.259497  0.219041   \n","46   23.142288   18.125929   10.131506  18.044463  ...  0.111183  0.069266   \n","47   99.842998   38.261544   10.927228   4.330965  ...  0.419429  0.717668   \n","48   21.970321   34.466662    6.322254   2.679117  ...  0.809580  0.377657   \n","49   82.917623   58.757996    8.723075  11.815731  ...  0.554465  0.721695   \n","\n","       2_492     2_493     2_494     2_495     2_496     2_497     2_498  \\\n","0   0.107830  0.425291  0.941574  0.526005  0.306785  0.560049  0.350928   \n","1   0.477954  0.076745  0.172020  0.257445  0.456032  0.486665  0.315492   \n","2   0.306773  0.415093  0.317944  0.265990  0.113143  0.709653  1.012296   \n","3   0.125822  0.643305  0.320884  0.046181  0.322935  0.348999  0.153024   \n","4   0.434708  1.055659  0.547044  0.143111  0.484099  0.336949  0.544538   \n","5   1.352786  0.365713  0.711593  0.606949  0.499613  0.208698  1.025403   \n","6   0.605998  0.831952  0.895054  0.767637  0.888651  0.876977  1.308258   \n","7   0.336976  0.642969  0.741255  0.614384  0.492811  0.933041  0.302152   \n","8   0.813533  0.926121  0.649347  0.511128  0.409690  0.293529  0.772780   \n","9   0.433560  0.740165  0.782075  1.319605  0.997819  0.311671  0.175690   \n","10  0.155983  0.110273  0.182669  0.175323  0.234637  0.177746  0.146612   \n","11  0.113910  0.021568  0.174755  0.143924  0.241262  0.052990  0.141293   \n","12  0.393852  0.455820  0.467426  0.564135  0.265910  0.554738  0.828600   \n","14  0.431905  0.663539  0.912721  0.262156  1.035282  0.538613  0.382229   \n","15  1.090115  1.189827  1.386800  1.159281  1.106265  1.203004  1.552537   \n","17  0.737900  0.896208  0.836594  0.633173  0.842181  0.501235  0.822268   \n","18  0.843273  0.964916  0.925053  0.910381  0.921904  0.843743  0.983892   \n","19  0.333805  0.690953  0.848387  0.253394  0.943507  0.702775  0.588625   \n","20  1.704367  1.741195  1.835064  1.830482  1.617813  1.849128  1.898409   \n","21  0.158751  0.140927  0.247623  0.207862  0.081059  0.076047  0.144896   \n","22  0.895299  0.266459  0.115387  0.271458  0.466967  0.760682  0.591807   \n","23  0.447047  0.344257  0.105696  0.777201  0.113358  1.058979  0.565462   \n","24  0.764613  0.613606  0.309103  0.216962  0.072947  1.404239  0.520531   \n","25  1.257180  0.437430  0.881269  0.042347  0.810611  0.256283  0.888381   \n","26  0.214828  0.080228  0.102691  0.165870  0.093455  0.124307  0.076448   \n","28  0.359781  1.255172  1.692066  1.233744  0.550624  1.284502  1.671745   \n","29  0.089255  0.197561  0.182011  0.131626  0.239856  0.355817  0.132830   \n","30  0.230583  0.207286  0.038841  0.102040  0.313409  0.223369  0.406877   \n","31  0.069053  0.133518  0.122981  0.070043  0.051580  0.109471  0.137101   \n","33  0.366293  0.201832  0.310691  0.528135  0.420128  0.209581  0.246273   \n","34  0.366810  0.646200  0.770653  0.269622  0.263445  0.560603  0.416983   \n","35  0.566932  0.356422  0.486543  0.362846  0.608656  0.276555  0.546481   \n","36  0.038886  0.011217  0.044853  0.077136  0.057671  0.107105  0.071548   \n","37  0.443698  1.153577  0.682995  0.238065  0.628543  0.676422  0.533781   \n","38  0.221490  0.237556  0.443707  0.048757  0.259962  0.272971  0.385217   \n","39  0.698503  0.373569  0.387495  0.616913  0.153032  0.600710  0.773475   \n","40  0.099257  0.158684  0.119397  0.107522  0.052266  0.080984  0.104612   \n","41  0.273330  0.291877  0.343993  0.558353  0.252498  0.216288  0.647566   \n","43  0.469768  0.674416  0.483574  0.641788  0.548664  0.063886  0.231984   \n","44  0.405524  0.238644  0.749902  0.236529  0.034603  0.322453  0.623932   \n","45  0.121233  0.154750  0.543235  0.289687  0.045632  0.237090  0.114697   \n","46  0.050891  0.094307  0.094286  0.025399  0.067310  0.205254  0.117588   \n","47  0.249964  0.118048  0.350482  0.602391  0.289237  0.099252  1.339910   \n","48  0.490011  0.583011  0.636926  0.257066  0.543998  0.293776  0.532469   \n","49  0.834070  0.573318  0.481451  0.473756  0.258202  0.379576  0.245187   \n","\n","       2_499  \n","0   0.119156  \n","1   0.041322  \n","2   0.127475  \n","3   0.270474  \n","4   0.928419  \n","5   1.257160  \n","6   0.222228  \n","7   0.275812  \n","8   0.740404  \n","9   0.504363  \n","10  0.144051  \n","11  0.150844  \n","12  0.742125  \n","14  0.311858  \n","15  1.079688  \n","17  0.524951  \n","18  0.938260  \n","19  0.303037  \n","20  2.259685  \n","21  0.053389  \n","22  0.206362  \n","23  0.594708  \n","24  0.371348  \n","25  0.529242  \n","26  0.278819  \n","28  1.341713  \n","29  0.139394  \n","30  0.290698  \n","31  0.183540  \n","33  0.438183  \n","34  0.352413  \n","35  0.714352  \n","36  0.104164  \n","37  0.536429  \n","38  0.362952  \n","39  0.241303  \n","40  0.191639  \n","41  1.063467  \n","43  0.864135  \n","44  1.111741  \n","45  0.154504  \n","46  0.056213  \n","47  0.274666  \n","48  0.292245  \n","49  0.388276  \n","\n","[45 rows x 2000 columns]\n","<class 'pandas.core.frame.DataFrame'>\n","           0_0        0_1        0_2        0_3         0_4        0_5  \\\n","13    4.980006   1.113433   3.434486   0.392388    4.746581   0.468230   \n","16    0.812172  11.987505  19.817093  26.210313   55.224366  73.259071   \n","27  757.622520  14.768699  16.772315   7.756356   13.015541  36.445433   \n","32  866.953418  27.444648  41.912781  53.340233  107.875803  39.668026   \n","42    4.459459   9.978440  22.775076  30.872979   36.263121  54.213621   \n","\n","          0_6        0_7        0_8        0_9  ...     2_490     2_491  \\\n","13  11.634919  61.111967  12.213330   9.727997  ...  0.174855  0.509130   \n","16  44.214046  31.987609   9.732927  26.773551  ...  0.872281  0.371031   \n","27  38.455515  12.016985   5.105857   2.548889  ...  0.176093  0.374499   \n","32  68.127807  35.020284  16.963889  63.162215  ...  0.868401  0.715272   \n","42  12.953561   8.607844   9.163925   8.467530  ...  0.904045  0.442346   \n","\n","       2_492     2_493     2_494     2_495     2_496     2_497     2_498  \\\n","13  0.135521  0.063941  0.249348  0.356608  0.367951  0.174740  0.380145   \n","16  0.416056  0.225756  0.388362  1.129571  0.313923  0.213291  0.659616   \n","27  0.642888  0.699057  0.370144  0.455704  0.179700  0.843407  0.576035   \n","32  1.034963  1.172944  0.996569  1.151716  0.913109  1.038909  0.844756   \n","42  0.564042  0.228305  0.636147  0.465784  0.727732  0.433457  0.272619   \n","\n","       2_499  \n","13  0.168297  \n","16  0.806355  \n","27  0.599061  \n","32  1.217628  \n","42  0.372647  \n","\n","[5 rows x 2000 columns]\n","shape after balancing (74, 2000)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 1 is smaller than n_iter=10. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [nan]\n","  warnings.warn(\n","<ipython-input-59-707daa621038>:65: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  best_hp_3_before = best_hp_3_before.append(rand_search_3a.best_params_,ignore_index=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 20 is smaller than n_iter=50. Running 20 iterations. For exhaustive searches, use GridSearchCV.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["shape of X_design after pca (74, 3)\n","size of features selected(2,)\n","shape of X_design after univariate: (74, 2)\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-59-707daa621038>:107: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  results_3 = results_3.append(results,ignore_index=True)\n","<ipython-input-59-707daa621038>:122: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  best_hp_3 = best_hp_3.append(model_3.best_params_,ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["Size_X_design (45, 2000)\n","Size X_test (5, 2000)\n","The total number of outliers in dataset x is 3198\n","The total number of outliers in dataset x is 900\n","<class 'pandas.core.frame.DataFrame'>\n","            0_0         0_1         0_2         0_3         0_4         0_5  \\\n","0     47.685046   48.416904   37.371002   61.604757   58.732738  213.067966   \n","1    152.726718  101.823698  111.168880   25.419423   14.636085   24.609688   \n","3      1.388947    3.052483    3.084103    4.627886   10.016196   15.020347   \n","4      3.625561    3.728466    6.205367   17.722897    5.699401    9.024836   \n","5      7.897582    4.750754    5.923150    3.999941    7.348004    3.336407   \n","6      6.313823    7.525186   14.898173    7.973962    7.741924    9.925077   \n","7      7.862030   21.680769   25.337318    4.579382    5.601682   18.016299   \n","8      3.933808   14.202943   13.572333   13.162149    6.623755   92.045714   \n","9     65.713418   17.930338    9.035395   11.099228    8.727922    8.879810   \n","10     8.909385   26.552703   45.105873   27.323183   10.273487   30.048169   \n","11   894.420487   47.319198   35.619225   32.931959   25.131503   95.379695   \n","13     4.980006    1.113433    3.434486    0.392388    4.746581    0.468230   \n","14     9.501722    5.586242   16.450204    9.001370   13.485390   12.654643   \n","15    18.644972   24.808661   29.530931   34.786282   81.380679   91.139503   \n","16     0.812172   11.987505   19.817093   26.210313   55.224366   73.259071   \n","18   247.728859   82.530058   64.737390  101.079961  117.796901  144.387597   \n","19     0.083161    5.929321    6.917387    6.924807   10.855910   17.821533   \n","21   462.167939  101.823698  119.125112   95.428088   81.632232   28.433531   \n","22    41.375439   36.520654   35.163455   28.184115   94.563472   21.907995   \n","23     7.554942   18.251545   19.131640    4.189196   13.567905   13.140102   \n","24     6.263398    6.768979    2.564803    3.499548    5.535409   16.535461   \n","25     2.291710    3.039610    5.481645    9.873051    3.657961    7.788805   \n","26   385.875221   28.181637   19.201062   16.526245   14.160023   18.202051   \n","27   757.622520   14.768699   16.772315    7.756356   13.015541   36.445433   \n","28   142.319356   40.861111   14.311447   12.825285   10.168552   15.789614   \n","29   197.409047   54.839062   39.124339   31.364328   55.818256   70.136786   \n","30   343.675806   48.950335   56.999643   10.091110    4.373090    6.670339   \n","31   595.966852  101.823698  107.449684   70.781661   72.694683  105.637914   \n","32   866.953418  101.823698  119.125112   53.340233  107.875803   39.668026   \n","33     7.547616    4.441597    8.504461    9.263530    5.448589    4.011405   \n","34     8.711248    5.446407    5.967202    6.607189   28.552336   73.130378   \n","35     2.362146    6.863112   22.550274   26.931418  105.863752   34.476580   \n","36     9.161991   10.856414   15.152315   11.796925   17.957884   25.792741   \n","37     5.233561    3.832656    3.096513    8.760333    2.037096   10.374480   \n","38   595.966852   88.885184  110.685906   20.589209   16.015180   30.676678   \n","39    11.071519    7.924814    6.367171    2.884096    9.385375    8.634683   \n","40   595.966852   21.596540  119.125112   43.813298  117.796901   33.964272   \n","41   939.366407  101.823698   71.517571   36.033812   34.987714   93.368641   \n","42     4.459459    9.978440   22.775076   30.872979   36.263121   54.213621   \n","43    13.818771   12.861720    7.176237   23.215493   45.191018  139.527824   \n","44     2.044676   12.641916   19.011975   10.018019   14.470866    5.059857   \n","45  1009.570351  164.825685  119.125112   45.532984   33.355747   19.186123   \n","46   595.966852  135.084186   31.630025   42.269784   23.539116   86.860689   \n","48     5.212389   22.435944   30.438124   25.494512   27.480230   19.577305   \n","49     0.835017    6.575501   12.352972   35.881337   20.712190   49.123780   \n","\n","           0_6         0_7         0_8        0_9  ...     2_490     2_491  \\\n","0   120.890451   40.100639   19.106303   6.043738  ...  0.358220  0.394612   \n","1    49.533842  130.083943   90.956939  68.780449  ...  0.214195  0.346208   \n","3   139.823127   20.149108   10.393109   9.241921  ...  0.651411  0.714947   \n","4    15.782812   33.336480   55.257804  19.139128  ...  0.114396  0.354976   \n","5     9.475968   29.113818   11.802082   4.880755  ...  0.923439  0.480057   \n","6    19.325956   68.563423   15.139322  11.386458  ...  0.582105  0.730925   \n","7    66.027558   23.029366    5.820312   3.542084  ...  0.706435  0.297448   \n","8    30.067347   26.844976   10.494297  35.291274  ...  1.159549  0.658967   \n","9     8.097210   44.556589   47.113102  21.933059  ...  0.398205  0.245733   \n","10   35.091876   45.291780   20.100238   8.416225  ...  0.311644  0.227725   \n","11   81.966685   36.308177   17.355844  10.090681  ...  0.096375  0.122703   \n","13   11.634919   61.111967   12.213330   9.727997  ...  0.174855  0.509130   \n","14    6.836319   37.801792   87.602074  23.830667  ...  0.357896  0.024080   \n","15  173.331888   99.557117   90.956939  68.780449  ...  1.368502  1.327078   \n","16   44.214046   31.987609    9.732927  26.773551  ...  0.872281  0.371031   \n","18   52.225949   43.735204   90.956939  68.780449  ...  0.776095  0.998492   \n","19   51.306309   70.669414   30.232692  22.993600  ...  0.457623  0.635237   \n","21   50.453163   62.488043   72.735797  52.063825  ...  0.264310  0.277353   \n","22    3.091364    6.026701   11.166130  10.255603  ...  0.786101  0.339428   \n","23    2.874134    6.775483    5.447489  62.872336  ...  1.082336  1.203700   \n","24  111.679516   13.382436   12.472009   9.521238  ...  0.278984  0.834699   \n","25   46.180684   17.594202    6.703946   7.967750  ...  0.732570  0.879275   \n","26   24.457990   61.877175  111.923601  23.398021  ...  0.098929  0.228524   \n","27   38.455515   12.016985    5.105857   2.548889  ...  0.176093  0.374499   \n","28   12.107519  130.083943   25.259786   1.638292  ...  1.681141  1.203190   \n","29   36.563568   23.705583    3.889423  24.028924  ...  0.199977  0.345672   \n","30   55.156363    4.515468    5.253856   2.463021  ...  0.162845  0.405848   \n","31   47.986717   18.116520   12.969337  21.154639  ...  0.102591  0.046813   \n","32   68.127807   35.020284   36.985907  63.162215  ...  0.868401  1.237484   \n","33   10.682050   18.004418    8.953420  68.780449  ...  0.826393  0.213216   \n","34   28.400980    5.935044    7.377658  18.182313  ...  1.089200  0.892079   \n","35   23.091628    5.187541   12.055103  20.776187  ...  0.766081  0.433455   \n","36   84.025145   15.480995   10.164311  14.735534  ...  0.032671  0.072995   \n","37   22.269579   90.313724   27.277003   3.746374  ...  0.547558  0.060141   \n","38   20.490327   58.920433  106.431948  26.090899  ...  0.348429  0.085254   \n","39   90.831259    4.567827    4.655512   8.270545  ...  1.127433  0.520051   \n","40   72.461802   65.642596   40.088273  16.303774  ...  0.133208  0.196779   \n","41   44.387696   33.029416   16.736460  29.516598  ...  0.271802  0.350006   \n","42   12.953561    8.607844    9.163925   8.467530  ...  0.904045  0.442346   \n","43   54.492808   38.533570   14.830765   7.000797  ...  0.555260  0.283301   \n","44    3.315126    5.018288   54.628473  27.966706  ...  0.216970  1.005113   \n","45   13.055429   24.261486   40.078577  21.321404  ...  0.259497  0.219041   \n","46   23.142288   18.125929   10.131506  18.044463  ...  0.111183  0.069266   \n","48   21.970321   34.466662    6.322254   2.679117  ...  0.809580  0.377657   \n","49   82.917623   58.757996    8.723075  11.815731  ...  0.554465  0.721695   \n","\n","       2_492     2_493     2_494     2_495     2_496     2_497     2_498  \\\n","0   0.107830  0.425291  0.941574  0.526005  0.306785  0.560049  0.350928   \n","1   0.477954  0.076745  0.172020  0.257445  0.456032  0.486665  0.315492   \n","3   0.125822  0.643305  0.320884  0.046181  0.322935  0.348999  0.153024   \n","4   0.434708  1.055659  0.547044  0.143111  0.484099  0.336949  0.544538   \n","5   1.352786  0.365713  0.711593  0.606949  0.499613  0.208698  1.025403   \n","6   0.605998  0.831952  0.895054  0.767637  0.888651  0.876977  1.308258   \n","7   0.336976  0.642969  0.741255  0.614384  0.492811  0.933041  0.302152   \n","8   0.813533  0.926121  0.649347  0.511128  0.409690  0.293529  0.772780   \n","9   0.433560  0.740165  0.782075  1.319605  0.997819  0.311671  0.175690   \n","10  0.155983  0.110273  0.182669  0.175323  0.234637  0.177746  0.146612   \n","11  0.113910  0.021568  0.174755  0.143924  0.241262  0.052990  0.141293   \n","13  0.135521  0.063941  0.249348  0.356608  0.367951  0.174740  0.380145   \n","14  0.431905  0.663539  0.912721  0.262156  1.035282  0.538613  0.382229   \n","15  1.090115  1.189827  1.386800  1.159281  1.106265  1.203004  1.552537   \n","16  0.416056  0.225756  0.388362  1.129571  0.313923  0.213291  0.659616   \n","18  0.843273  0.964916  0.925053  0.910381  0.921904  0.843743  0.983892   \n","19  0.333805  0.690953  0.848387  0.253394  0.943507  0.702775  0.588625   \n","21  0.158751  0.140927  0.247623  0.207862  0.081059  0.076047  0.144896   \n","22  0.895299  0.266459  0.115387  0.271458  0.466967  0.760682  0.591807   \n","23  0.447047  0.344257  0.105696  0.777201  0.113358  1.058979  0.565462   \n","24  0.764613  0.613606  0.309103  0.216962  0.072947  1.404239  0.520531   \n","25  1.257180  0.437430  0.881269  0.042347  0.810611  0.256283  0.888381   \n","26  0.214828  0.080228  0.102691  0.165870  0.093455  0.124307  0.076448   \n","27  0.642888  0.699057  0.370144  0.455704  0.179700  0.843407  0.576035   \n","28  0.359781  1.255172  1.692066  1.233744  0.550624  1.284502  1.671745   \n","29  0.089255  0.197561  0.182011  0.131626  0.239856  0.355817  0.132830   \n","30  0.230583  0.207286  0.038841  0.102040  0.313409  0.223369  0.406877   \n","31  0.069053  0.133518  0.122981  0.070043  0.051580  0.109471  0.137101   \n","32  1.034963  1.172944  0.996569  1.151716  0.913109  1.038909  0.844756   \n","33  0.366293  0.201832  0.310691  0.528135  0.420128  0.209581  0.246273   \n","34  0.366810  0.646200  0.770653  0.269622  0.263445  0.560603  0.416983   \n","35  0.566932  0.356422  0.486543  0.362846  0.608656  0.276555  0.546481   \n","36  0.038886  0.011217  0.044853  0.077136  0.057671  0.107105  0.071548   \n","37  0.443698  1.153577  0.682995  0.238065  0.628543  0.676422  0.533781   \n","38  0.221490  0.237556  0.443707  0.048757  0.259962  0.272971  0.385217   \n","39  0.698503  0.373569  0.387495  0.616913  0.153032  0.600710  0.773475   \n","40  0.099257  0.158684  0.119397  0.107522  0.052266  0.080984  0.104612   \n","41  0.273330  0.291877  0.343993  0.558353  0.252498  0.216288  0.647566   \n","42  0.564042  0.228305  0.636147  0.465784  0.727732  0.433457  0.272619   \n","43  0.469768  0.674416  0.483574  0.641788  0.548664  0.063886  0.231984   \n","44  0.405524  0.238644  0.749902  0.236529  0.034603  0.322453  0.623932   \n","45  0.121233  0.154750  0.543235  0.289687  0.045632  0.237090  0.114697   \n","46  0.050891  0.094307  0.094286  0.025399  0.067310  0.205254  0.117588   \n","48  0.490011  0.583011  0.636926  0.257066  0.543998  0.293776  0.532469   \n","49  0.834070  0.573318  0.481451  0.473756  0.258202  0.379576  0.245187   \n","\n","       2_499  \n","0   0.119156  \n","1   0.041322  \n","3   0.270474  \n","4   0.928419  \n","5   1.257160  \n","6   0.222228  \n","7   0.275812  \n","8   0.740404  \n","9   0.504363  \n","10  0.144051  \n","11  0.150844  \n","13  0.168297  \n","14  0.311858  \n","15  1.079688  \n","16  0.806355  \n","18  0.938260  \n","19  0.303037  \n","21  0.053389  \n","22  0.206362  \n","23  0.594708  \n","24  0.371348  \n","25  0.529242  \n","26  0.278819  \n","27  0.599061  \n","28  1.341713  \n","29  0.139394  \n","30  0.290698  \n","31  0.183540  \n","32  1.217628  \n","33  0.438183  \n","34  0.352413  \n","35  0.714352  \n","36  0.104164  \n","37  0.536429  \n","38  0.362952  \n","39  0.241303  \n","40  0.191639  \n","41  1.063467  \n","42  0.372647  \n","43  0.864135  \n","44  1.111741  \n","45  0.154504  \n","46  0.056213  \n","48  0.292245  \n","49  0.388276  \n","\n","[45 rows x 2000 columns]\n","<class 'pandas.core.frame.DataFrame'>\n","           0_0        0_1        0_2        0_3        0_4        0_5  \\\n","2     1.601260   3.882169  22.978997  21.673040   8.919484  18.329286   \n","12    6.323226  15.953172  26.120406   5.109627   4.012020  16.225039   \n","17  166.947713  29.076832   4.174963  10.406641   7.447515  38.463177   \n","20  160.601443  67.903791  38.376866  19.670381  12.283158  17.170820   \n","47    5.076651   0.396899  18.727030   4.525304  21.604739  40.037709   \n","\n","           0_6        0_7        0_8        0_9  ...     2_490     2_491  \\\n","2    27.710604  23.350084  37.896254   8.331180  ...  1.084666  0.161850   \n","12  114.460883   8.289232   4.147352   5.958195  ...  0.420808  0.370241   \n","17   82.489406  38.985256  16.685890  16.865303  ...  0.626253  0.756545   \n","20   15.512118  95.015274   4.522202   8.525927  ...  1.914944  1.652935   \n","47   99.842998  38.261544  10.927228   4.330965  ...  0.419429  0.717668   \n","\n","       2_492     2_493     2_494     2_495     2_496     2_497     2_498  \\\n","2   0.306773  0.415093  0.317944  0.479706  0.113143  0.709653  1.012296   \n","12  0.393852  0.455820  0.467426  0.564135  0.265910  0.554738  0.828600   \n","17  0.737900  0.896208  0.836594  0.633173  0.842181  0.501235  0.822268   \n","20  1.800877  1.741195  1.835064  0.721339  1.617813  1.171648  1.898409   \n","47  0.249964  0.118048  0.350482  0.602391  0.289237  0.099252  1.339910   \n","\n","       2_499  \n","2   0.127475  \n","12  0.742125  \n","17  0.524951  \n","20  1.603373  \n","47  0.274666  \n","\n","[5 rows x 2000 columns]\n","shape after balancing (74, 2000)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 1 is smaller than n_iter=10. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [nan]\n","  warnings.warn(\n","<ipython-input-59-707daa621038>:65: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  best_hp_3_before = best_hp_3_before.append(rand_search_3a.best_params_,ignore_index=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 20 is smaller than n_iter=50. Running 20 iterations. For exhaustive searches, use GridSearchCV.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["shape of X_design after pca (74, 3)\n","size of features selected(1,)\n","shape of X_design after univariate: (74, 1)\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-59-707daa621038>:107: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  results_3 = results_3.append(results,ignore_index=True)\n","<ipython-input-59-707daa621038>:122: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  best_hp_3 = best_hp_3.append(model_3.best_params_,ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["Size_X_design (45, 2000)\n","Size X_test (5, 2000)\n","The total number of outliers in dataset x is 3038\n","The total number of outliers in dataset x is 1502\n","<class 'pandas.core.frame.DataFrame'>\n","            0_0         0_1         0_2         0_3         0_4         0_5  \\\n","0     47.685046   48.416904   37.371002   61.604757   58.732738  213.067966   \n","1    152.726718   96.195967  111.168880   25.419423   14.636085   24.609688   \n","2      1.601260    3.882169   22.978997   21.673040    8.919484   18.329286   \n","3      1.388947    3.052483    3.084103    4.627886   10.016196   15.020347   \n","4      3.625561    3.728466    6.205367   17.722897    5.699401    9.024836   \n","5      7.897582    4.750754    5.923150    3.999941    7.348004    3.336407   \n","6      6.313823    7.525186   14.898173    7.973962    7.741924    9.925077   \n","8      3.933808   14.202943   13.572333   13.162149    6.623755   92.045714   \n","9     65.713418   17.930338    9.035395   11.099228    8.727922    8.879810   \n","10     8.909385   26.552703   45.105873   27.323183   10.273487   30.048169   \n","11   894.420487   47.319198   35.619225   32.931959   25.131503   95.379695   \n","12     6.323226   15.953172   26.120406    5.109627    4.012020   16.225039   \n","13     4.980006    1.113433    3.434486    0.392388    4.746581    0.468230   \n","15    18.644972   24.808661   29.530931   34.786282   81.380679   91.139503   \n","16     0.812172   11.987505   19.817093   26.210313   55.224366   73.259071   \n","17  1264.963015   29.076832    4.174963   10.406641    7.447515   38.463177   \n","18   247.728859   82.530058   64.737390  101.079961  119.717511  144.387597   \n","19     0.083161    5.929321    6.917387    6.924807   10.855910   17.821533   \n","20   160.601443  143.422714   38.376866   19.670381   12.283158   17.170820   \n","21   462.167939   96.195967  121.074832   95.428088   81.632232   28.433531   \n","22    41.375439   36.520654   35.163455   28.184115   94.563472   21.907995   \n","23     7.554942   18.251545   19.131640    4.189196   13.567905   13.140102   \n","24     6.263398    6.768979    2.564803    3.499548    5.535409   16.535461   \n","25     2.291710    3.039610    5.481645    9.873051    3.657961    7.788805   \n","26   385.875221   28.181637   19.201062   16.526245   14.160023   18.202051   \n","27   757.622520   14.768699   16.772315    7.756356   13.015541   36.445433   \n","29   197.409047   54.839062   39.124339   31.364328   55.818256   70.136786   \n","30   343.675806   48.950335   56.999643   10.091110    4.373090    6.670339   \n","31   627.465751   96.195967  107.449684   70.781661   72.694683  105.637914   \n","32   866.953418   96.195967  121.074832   53.340233  107.875803   39.668026   \n","33     7.547616    4.441597    8.504461    9.263530    5.448589    4.011405   \n","34     8.711248    5.446407    5.967202    6.607189   28.552336   73.130378   \n","35     2.362146    6.863112   22.550274   26.931418  105.863752   34.476580   \n","36     9.161991   10.856414   15.152315   11.796925   17.957884   25.792741   \n","37     5.233561    3.832656    3.096513    8.760333    2.037096   10.374480   \n","38   627.465751   88.885184  110.685906   20.589209   16.015180   30.676678   \n","39    11.071519    7.924814    6.367171    2.884096    9.385375    8.634683   \n","40   627.465751   21.596540  121.074832   43.813298  119.717511   33.964272   \n","41   939.366407   96.195967   71.517571   36.033812   34.987714   93.368641   \n","42     4.459459    9.978440   22.775076   30.872979   36.263121   54.213621   \n","43    13.818771   12.861720    7.176237   23.215493   45.191018  139.527824   \n","46   627.465751  135.084186   31.630025   42.269784   23.539116   86.860689   \n","47     5.076651    0.396899   18.727030    4.525304   21.604739   40.037709   \n","48     5.212389   22.435944   30.438124   25.494512   27.480230   19.577305   \n","49     0.835017    6.575501   12.352972   35.881337   20.712190   49.123780   \n","\n","           0_6         0_7        0_8        0_9  ...     2_490     2_491  \\\n","0   120.890451   40.100639  19.106303   6.043738  ...  0.358220  0.394612   \n","1    49.533842  143.435635  46.013673  67.162763  ...  0.214195  0.346208   \n","2    27.710604   23.350084  37.896254   8.331180  ...  1.084666  0.161850   \n","3   139.823127   20.149108  10.393109   9.241921  ...  0.651411  0.714947   \n","4    15.782812   33.336480  55.257804  19.139128  ...  0.114396  0.354976   \n","5     9.475968   29.113818  11.802082   4.880755  ...  0.923439  0.480057   \n","6    19.325956   68.563423  15.139322  11.386458  ...  0.582105  0.730925   \n","8    30.067347   26.844976  10.494297  35.291274  ...  1.159549  0.658967   \n","9     8.097210   44.556589  47.113102  21.933059  ...  0.398205  0.245733   \n","10   35.091876   45.291780  20.100238   8.416225  ...  0.311644  0.227725   \n","11   81.966685   36.308177  17.355844  10.090681  ...  0.096375  0.122703   \n","12  114.460883    8.289232   4.147352   5.958195  ...  0.420808  0.370241   \n","13   11.634919   61.111967  12.213330   9.727997  ...  0.174855  0.509130   \n","15  212.044254   99.557117  46.013673  67.162763  ...  1.368502  1.327078   \n","16   44.214046   31.987609   9.732927  26.773551  ...  0.872281  0.371031   \n","17   82.489406   38.985256  16.685890  31.899988  ...  0.626253  0.756545   \n","18   52.225949   43.735204  46.013673  67.162763  ...  0.776095  0.998492   \n","19   51.306309   70.669414  30.232692  22.993600  ...  0.457623  0.635237   \n","20   15.512118  111.673362   4.522202   8.525927  ...  1.914944  1.652935   \n","21   50.453163   62.488043  72.735797  52.063825  ...  0.264310  0.277353   \n","22    3.091364    6.026701  11.166130  10.255603  ...  0.786101  0.339428   \n","23    2.874134    6.775483   5.447489  62.872336  ...  1.082336  1.203700   \n","24  111.679516   13.382436  12.472009   9.521238  ...  0.278984  0.834699   \n","25   46.180684   17.594202   6.703946   7.967750  ...  0.732570  0.879275   \n","26   24.457990   61.877175  46.013673  23.398021  ...  0.098929  0.228524   \n","27   38.455515   12.016985   5.105857   2.548889  ...  0.176093  0.374499   \n","29   36.563568   23.705583   3.889423  24.028924  ...  0.199977  0.345672   \n","30   55.156363    4.515468   5.253856   2.463021  ...  0.162845  0.405848   \n","31   47.986717   18.116520  12.969337  21.154639  ...  0.102591  0.046813   \n","32   68.127807   35.020284  36.985907  63.162215  ...  0.868401  1.237484   \n","33   10.682050   18.004418   8.953420  67.162763  ...  0.826393  0.213216   \n","34   28.400980    5.935044   7.377658  18.182313  ...  1.089200  0.892079   \n","35   23.091628    5.187541  12.055103  20.776187  ...  0.766081  0.433455   \n","36   84.025145   15.480995  10.164311  14.735534  ...  0.032671  0.072995   \n","37   22.269579   90.313724  27.277003   3.746374  ...  0.547558  0.060141   \n","38   20.490327   58.920433  46.013673  26.090899  ...  0.348429  0.085254   \n","39   90.831259    4.567827   4.655512   8.270545  ...  1.127433  0.520051   \n","40   72.461802   65.642596  40.088273  16.303774  ...  0.133208  0.196779   \n","41   44.387696   33.029416  16.736460  29.516598  ...  0.271802  0.350006   \n","42   12.953561    8.607844   9.163925   8.467530  ...  0.904045  0.442346   \n","43   54.492808   38.533570  14.830765   7.000797  ...  0.555260  0.283301   \n","46   23.142288   18.125929  10.131506  18.044463  ...  0.111183  0.069266   \n","47   99.842998   38.261544  10.927228   4.330965  ...  0.419429  0.717668   \n","48   21.970321   34.466662   6.322254   2.679117  ...  0.809580  0.377657   \n","49   82.917623   58.757996   8.723075  11.815731  ...  0.554465  0.721695   \n","\n","       2_492     2_493     2_494     2_495     2_496     2_497     2_498  \\\n","0   0.107830  0.425291  0.941574  0.526005  0.306785  0.560049  0.350928   \n","1   0.477954  0.076745  0.172020  0.257445  0.456032  0.486665  0.315492   \n","2   0.306773  0.415093  0.317944  0.265990  0.113143  0.709653  1.012296   \n","3   0.125822  0.643305  0.320884  0.046181  0.322935  0.348999  0.153024   \n","4   0.434708  1.055659  0.547044  0.143111  0.484099  0.336949  0.544538   \n","5   1.352786  0.365713  0.711593  0.606949  0.499613  0.208698  1.025403   \n","6   0.605998  0.831952  0.895054  0.767637  0.888651  0.876977  1.308258   \n","8   0.813533  0.926121  0.649347  0.511128  0.409690  0.293529  0.772780   \n","9   0.433560  0.740165  0.782075  1.319605  0.997819  0.311671  0.175690   \n","10  0.155983  0.110273  0.182669  0.175323  0.234637  0.177746  0.146612   \n","11  0.113910  0.021568  0.174755  0.143924  0.241262  0.052990  0.141293   \n","12  0.393852  0.455820  0.467426  0.564135  0.265910  0.554738  0.828600   \n","13  0.135521  0.063941  0.249348  0.356608  0.367951  0.174740  0.380145   \n","15  1.090115  1.189827  1.386800  1.159281  1.106265  1.203004  1.552537   \n","16  0.416056  0.225756  0.388362  1.129571  0.313923  0.213291  0.659616   \n","17  0.737900  0.896208  0.836594  0.633173  0.842181  0.501235  0.822268   \n","18  0.843273  0.964916  0.925053  0.910381  0.921904  0.843743  0.983892   \n","19  0.333805  0.690953  0.848387  0.253394  0.943507  0.702775  0.588625   \n","20  1.800877  1.741195  1.835064  1.830482  1.617813  1.849128  1.898409   \n","21  0.158751  0.140927  0.247623  0.207862  0.081059  0.076047  0.144896   \n","22  0.895299  0.266459  0.115387  0.271458  0.466967  0.760682  0.591807   \n","23  0.447047  0.344257  0.105696  0.777201  0.113358  1.058979  0.565462   \n","24  0.764613  0.613606  0.309103  0.216962  0.072947  1.404239  0.520531   \n","25  1.257180  0.437430  0.881269  0.042347  0.810611  0.256283  0.888381   \n","26  0.214828  0.080228  0.102691  0.165870  0.093455  0.124307  0.076448   \n","27  0.642888  0.699057  0.370144  0.455704  0.179700  0.843407  0.576035   \n","29  0.089255  0.197561  0.182011  0.131626  0.239856  0.355817  0.132830   \n","30  0.230583  0.207286  0.038841  0.102040  0.313409  0.223369  0.406877   \n","31  0.069053  0.133518  0.122981  0.070043  0.051580  0.109471  0.137101   \n","32  1.034963  1.172944  0.996569  1.151716  0.913109  1.038909  0.844756   \n","33  0.366293  0.201832  0.310691  0.528135  0.420128  0.209581  0.246273   \n","34  0.366810  0.646200  0.770653  0.269622  0.263445  0.560603  0.416983   \n","35  0.566932  0.356422  0.486543  0.362846  0.608656  0.276555  0.546481   \n","36  0.038886  0.011217  0.044853  0.077136  0.057671  0.107105  0.071548   \n","37  0.443698  1.153577  0.682995  0.238065  0.628543  0.676422  0.533781   \n","38  0.221490  0.237556  0.443707  0.048757  0.259962  0.272971  0.385217   \n","39  0.698503  0.373569  0.387495  0.616913  0.153032  0.600710  0.773475   \n","40  0.099257  0.158684  0.119397  0.107522  0.052266  0.080984  0.104612   \n","41  0.273330  0.291877  0.343993  0.558353  0.252498  0.216288  0.647566   \n","42  0.564042  0.228305  0.636147  0.465784  0.727732  0.433457  0.272619   \n","43  0.469768  0.674416  0.483574  0.641788  0.548664  0.063886  0.231984   \n","46  0.050891  0.094307  0.094286  0.025399  0.067310  0.205254  0.117588   \n","47  0.249964  0.118048  0.350482  0.602391  0.289237  0.099252  1.339910   \n","48  0.490011  0.583011  0.636926  0.257066  0.543998  0.293776  0.532469   \n","49  0.834070  0.573318  0.481451  0.473756  0.258202  0.379576  0.245187   \n","\n","       2_499  \n","0   0.119156  \n","1   0.041322  \n","2   0.127475  \n","3   0.270474  \n","4   0.928419  \n","5   1.257160  \n","6   0.222228  \n","8   0.740404  \n","9   0.504363  \n","10  0.144051  \n","11  0.150844  \n","12  0.742125  \n","13  0.168297  \n","15  1.079688  \n","16  0.806355  \n","17  0.524951  \n","18  0.938260  \n","19  0.303037  \n","20  2.259685  \n","21  0.053389  \n","22  0.206362  \n","23  0.594708  \n","24  0.371348  \n","25  0.529242  \n","26  0.278819  \n","27  0.599061  \n","29  0.139394  \n","30  0.290698  \n","31  0.183540  \n","32  1.217628  \n","33  0.438183  \n","34  0.352413  \n","35  0.714352  \n","36  0.104164  \n","37  0.536429  \n","38  0.362952  \n","39  0.241303  \n","40  0.191639  \n","41  1.063467  \n","42  0.372647  \n","43  0.864135  \n","46  0.056213  \n","47  0.274666  \n","48  0.292245  \n","49  0.388276  \n","\n","[45 rows x 2000 columns]\n","<class 'pandas.core.frame.DataFrame'>\n","           0_0        0_1        0_2        0_3        0_4        0_5  \\\n","7     7.862030  21.680769  25.337318   4.579382   5.601682  18.016299   \n","14    9.501722   5.586242  16.450204   9.001370  13.485390  12.654643   \n","28  142.319356  40.861111  14.311447  12.825285  10.168552  15.789614   \n","44    2.044676  12.641916  19.011975  10.018019  14.470866   5.059857   \n","45  151.601448  73.269426  34.626698  19.191724  27.846533  19.186123   \n","\n","          0_6        0_7        0_8        0_9  ...     2_490     2_491  \\\n","7   31.509925  23.029366   5.820312   3.542084  ...  0.706435  0.297448   \n","14   6.836319  37.801792  87.602074  23.830667  ...  0.357896  0.024080   \n","28  12.107519  55.006461  25.259786   1.638292  ...  1.681141  1.203190   \n","44   3.315126   5.018288  54.628473  27.966706  ...  0.216970  1.005113   \n","45  13.055429  24.261486  40.078577  21.321404  ...  0.259497  0.219041   \n","\n","       2_492     2_493     2_494     2_495     2_496     2_497     2_498  \\\n","7   0.336976  0.642969  0.741255  0.614384  0.492811  0.933041  0.302152   \n","14  0.431905  0.663539  0.912721  0.262156  1.035282  0.538613  0.382229   \n","28  0.359781  1.255172  1.087177  1.233744  0.550624  1.284502  1.004757   \n","44  0.405524  0.238644  0.749902  0.236529  0.034603  0.322453  0.623932   \n","45  0.179962  0.154750  0.543235  0.289687  0.045632  0.237090  0.114697   \n","\n","       2_499  \n","7   0.275812  \n","14  0.311858  \n","28  1.341713  \n","44  1.111741  \n","45  0.154504  \n","\n","[5 rows x 2000 columns]\n","shape after balancing (74, 2000)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 1 is smaller than n_iter=10. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [nan]\n","  warnings.warn(\n","<ipython-input-59-707daa621038>:65: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  best_hp_3_before = best_hp_3_before.append(rand_search_3a.best_params_,ignore_index=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 20 is smaller than n_iter=50. Running 20 iterations. For exhaustive searches, use GridSearchCV.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["shape of X_design after pca (74, 3)\n","size of features selected(1,)\n","shape of X_design after univariate: (74, 1)\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-59-707daa621038>:107: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  results_3 = results_3.append(results,ignore_index=True)\n","<ipython-input-59-707daa621038>:122: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  best_hp_3 = best_hp_3.append(model_3.best_params_,ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["Size_X_design (45, 2000)\n","Size X_test (5, 2000)\n","The total number of outliers in dataset x is 4040\n","The total number of outliers in dataset x is 650\n","<class 'pandas.core.frame.DataFrame'>\n","            0_0         0_1         0_2        0_3         0_4         0_5  \\\n","2      1.601260    3.882169   22.978997  21.673040    8.919484   18.329286   \n","3      1.388947    3.052483    3.084103   4.627886   10.016196   15.020347   \n","4      3.625561    3.728466    6.205367  17.722897    5.699401    9.024836   \n","5      7.897582    4.750754    5.923150   3.999941    7.348004    3.336407   \n","6      6.313823    7.525186   14.898173   7.973962    7.741924    9.925077   \n","7      7.862030   21.680769   25.337318   4.579382    5.601682   18.016299   \n","8      3.933808   14.202943   13.572333  13.162149    6.623755   92.045714   \n","9     65.713418   17.930338    9.035395  11.099228    8.727922    8.879810   \n","10     8.909385   26.552703   45.105873  27.323183   10.273487   30.048169   \n","11   894.420487   47.319198   35.619225  32.931959   25.131503   95.379695   \n","12     6.323226   15.953172   26.120406   5.109627    4.012020   16.225039   \n","13     4.980006    1.113433    3.434486   0.392388    4.746581    0.468230   \n","14     9.501722    5.586242   16.450204   9.001370   13.485390   12.654643   \n","15    18.644972   24.808661   29.530931  34.786282   81.380679   91.139503   \n","16     0.812172   11.987505   19.817093  26.210313   55.224366   73.259071   \n","17  1264.963015   29.076832    4.174963  10.406641    7.447515   38.463177   \n","18   247.728859   82.530058   64.737390  91.666643   89.871853  119.985577   \n","19     0.083161    5.929321    6.917387   6.924807   10.855910   17.821533   \n","20   160.601443  143.422714   38.376866  19.670381   12.283158   17.170820   \n","21   462.167939   95.967824   96.239111  95.428088   81.632232   28.433531   \n","22    41.375439   36.520654   35.163455  28.184115   94.563472   21.907995   \n","23     7.554942   18.251545   19.131640   4.189196   13.567905   13.140102   \n","24     6.263398    6.768979    2.564803   3.499548    5.535409   16.535461   \n","25     2.291710    3.039610    5.481645   9.873051    3.657961    7.788805   \n","26   385.875221   28.181637   19.201062  16.526245   14.160023   18.202051   \n","27   757.622520   14.768699   16.772315   7.756356   13.015541   36.445433   \n","28   142.319356   40.861111   14.311447  12.825285   10.168552   15.789614   \n","29   197.409047   54.839062   39.124339  31.364328   55.818256   70.136786   \n","30   343.675806   48.950335   56.999643  10.091110    4.373090    6.670339   \n","32   866.953418   95.967824   96.239111  53.340233  107.875803   39.668026   \n","33     7.547616    4.441597    8.504461   9.263530    5.448589    4.011405   \n","34     8.711248    5.446407    5.967202   6.607189   28.552336   73.130378   \n","35     2.362146    6.863112   22.550274  26.931418  105.863752   34.476580   \n","36     9.161991   10.856414   15.152315  11.796925   17.957884   25.792741   \n","37     5.233561    3.832656    3.096513   8.760333    2.037096   10.374480   \n","38   739.059796   88.885184  110.685906  20.589209   16.015180   30.676678   \n","40   739.059796   21.596540   96.239111  43.813298   89.871853   33.964272   \n","41   939.366407   95.967824   71.517571  36.033812   34.987714   93.368641   \n","42     4.459459    9.978440   22.775076  30.872979   36.263121   54.213621   \n","44     2.044676   12.641916   19.011975  10.018019   14.470866    5.059857   \n","45  1009.570351   95.967824   96.239111  45.532984   33.355747   19.186123   \n","46   739.059796  135.084186   31.630025  42.269784   23.539116   86.860689   \n","47     5.076651    0.396899   18.727030   4.525304   21.604739   40.037709   \n","48     5.212389   22.435944   30.438124  25.494512   27.480230   19.577305   \n","49     0.835017    6.575501   12.352972  35.881337   20.712190   49.123780   \n","\n","           0_6         0_7         0_8        0_9  ...     2_490     2_491  \\\n","2    27.710604   23.350084   37.896254   8.331180  ...  1.084666  0.161850   \n","3   139.823127   20.149108   10.393109   9.241921  ...  0.651411  0.714947   \n","4    15.782812   33.336480   55.257804  19.139128  ...  0.114396  0.354976   \n","5     9.475968   29.113818   11.802082   4.880755  ...  0.923439  0.480057   \n","6    19.325956   68.563423   15.139322  11.386458  ...  0.582105  0.730925   \n","7    66.027558   23.029366    5.820312   3.542084  ...  0.706435  0.297448   \n","8    30.067347   26.844976   10.494297  35.291274  ...  1.159549  0.658967   \n","9     8.097210   44.556589   47.113102  21.933059  ...  0.398205  0.245733   \n","10   35.091876   45.291780   20.100238   8.416225  ...  0.311644  0.227725   \n","11   81.966685   36.308177   17.355844  10.090681  ...  0.096375  0.122703   \n","12  114.460883    8.289232    4.147352   5.958195  ...  0.420808  0.370241   \n","13   11.634919   61.111967   12.213330   9.727997  ...  0.174855  0.509130   \n","14    6.836319   37.801792   87.602074  23.830667  ...  0.357896  0.024080   \n","15  186.802809   99.557117  107.922455  69.832698  ...  1.368502  1.327078   \n","16   44.214046   31.987609    9.732927  26.773551  ...  0.872281  0.371031   \n","17   82.489406   38.985256   16.685890  31.899988  ...  0.626253  0.756545   \n","18   52.225949   43.735204  107.922455  69.832698  ...  0.776095  0.998492   \n","19   51.306309   70.669414   30.232692  22.993600  ...  0.457623  0.635237   \n","20   15.512118  111.673362    4.522202   8.525927  ...  1.914944  1.652935   \n","21   50.453163   62.488043   72.735797  52.063825  ...  0.264310  0.277353   \n","22    3.091364    6.026701   11.166130  10.255603  ...  0.786101  0.339428   \n","23    2.874134    6.775483    5.447489  62.872336  ...  1.082336  1.203700   \n","24  111.679516   13.382436   12.472009   9.521238  ...  0.278984  0.834699   \n","25   46.180684   17.594202    6.703946   7.967750  ...  0.732570  0.879275   \n","26   24.457990   61.877175  111.923601  23.398021  ...  0.098929  0.228524   \n","27   38.455515   12.016985    5.105857   2.548889  ...  0.176093  0.374499   \n","28   12.107519  143.435635   25.259786   1.638292  ...  1.681141  1.203190   \n","29   36.563568   23.705583    3.889423  24.028924  ...  0.199977  0.345672   \n","30   55.156363    4.515468    5.253856   2.463021  ...  0.162845  0.405848   \n","32   68.127807   35.020284   36.985907  63.162215  ...  0.868401  1.237484   \n","33   10.682050   18.004418    8.953420  69.832698  ...  0.826393  0.213216   \n","34   28.400980    5.935044    7.377658  18.182313  ...  1.089200  0.892079   \n","35   23.091628    5.187541   12.055103  20.776187  ...  0.766081  0.433455   \n","36   84.025145   15.480995   10.164311  14.735534  ...  0.032671  0.072995   \n","37   22.269579   90.313724   27.277003   3.746374  ...  0.547558  0.060141   \n","38   20.490327   58.920433  106.431948  26.090899  ...  0.348429  0.085254   \n","40   72.461802   65.642596   40.088273  16.303774  ...  0.133208  0.196779   \n","41   44.387696   33.029416   16.736460  29.516598  ...  0.271802  0.350006   \n","42   12.953561    8.607844    9.163925   8.467530  ...  0.904045  0.442346   \n","44    3.315126    5.018288   54.628473  27.966706  ...  0.216970  1.005113   \n","45   13.055429   24.261486   40.078577  21.321404  ...  0.259497  0.219041   \n","46   23.142288   18.125929   10.131506  18.044463  ...  0.111183  0.069266   \n","47   99.842998   38.261544   10.927228   4.330965  ...  0.419429  0.717668   \n","48   21.970321   34.466662    6.322254   2.679117  ...  0.809580  0.377657   \n","49   82.917623   58.757996    8.723075  11.815731  ...  0.554465  0.721695   \n","\n","       2_492     2_493     2_494     2_495     2_496     2_497     2_498  \\\n","2   0.306773  0.415093  0.317944  0.265990  0.113143  0.709653  1.012296   \n","3   0.125822  0.643305  0.320884  0.046181  0.322935  0.348999  0.153024   \n","4   0.434708  1.055659  0.547044  0.143111  0.484099  0.336949  0.544538   \n","5   1.352786  0.365713  0.711593  0.606949  0.499613  0.208698  1.025403   \n","6   0.605998  0.831952  0.895054  0.767637  0.888651  0.876977  1.308258   \n","7   0.336976  0.642969  0.741255  0.614384  0.492811  0.933041  0.302152   \n","8   0.813533  0.926121  0.649347  0.511128  0.409690  0.293529  0.772780   \n","9   0.433560  0.740165  0.782075  1.319605  0.997819  0.311671  0.175690   \n","10  0.155983  0.110273  0.182669  0.175323  0.234637  0.177746  0.146612   \n","11  0.113910  0.021568  0.174755  0.143924  0.241262  0.052990  0.141293   \n","12  0.393852  0.455820  0.467426  0.564135  0.265910  0.554738  0.828600   \n","13  0.135521  0.063941  0.249348  0.356608  0.367951  0.174740  0.380145   \n","14  0.431905  0.663539  0.912721  0.262156  1.035282  0.538613  0.382229   \n","15  1.090115  1.189827  1.386800  1.159281  1.106265  1.203004  1.552537   \n","16  0.416056  0.225756  0.388362  1.129571  0.313923  0.213291  0.659616   \n","17  0.737900  0.896208  0.836594  0.633173  0.842181  0.501235  0.822268   \n","18  0.843273  0.964916  0.925053  0.910381  0.921904  0.843743  0.983892   \n","19  0.333805  0.690953  0.848387  0.253394  0.943507  0.702775  0.588625   \n","20  1.800877  1.741195  1.835064  1.830482  1.617813  1.849128  1.898409   \n","21  0.158751  0.140927  0.247623  0.207862  0.081059  0.076047  0.144896   \n","22  0.895299  0.266459  0.115387  0.271458  0.466967  0.760682  0.591807   \n","23  0.447047  0.344257  0.105696  0.777201  0.113358  1.058979  0.565462   \n","24  0.764613  0.613606  0.309103  0.216962  0.072947  1.404239  0.520531   \n","25  1.257180  0.437430  0.881269  0.042347  0.810611  0.256283  0.888381   \n","26  0.214828  0.080228  0.102691  0.165870  0.093455  0.124307  0.076448   \n","27  0.642888  0.699057  0.370144  0.455704  0.179700  0.843407  0.576035   \n","28  0.359781  1.255172  1.692066  1.233744  0.550624  1.284502  1.671745   \n","29  0.089255  0.197561  0.182011  0.131626  0.239856  0.355817  0.132830   \n","30  0.230583  0.207286  0.038841  0.102040  0.313409  0.223369  0.406877   \n","32  1.034963  1.172944  0.996569  1.151716  0.913109  1.038909  0.844756   \n","33  0.366293  0.201832  0.310691  0.528135  0.420128  0.209581  0.246273   \n","34  0.366810  0.646200  0.770653  0.269622  0.263445  0.560603  0.416983   \n","35  0.566932  0.356422  0.486543  0.362846  0.608656  0.276555  0.546481   \n","36  0.038886  0.011217  0.044853  0.077136  0.057671  0.107105  0.071548   \n","37  0.443698  1.153577  0.682995  0.238065  0.628543  0.676422  0.533781   \n","38  0.221490  0.237556  0.443707  0.048757  0.259962  0.272971  0.385217   \n","40  0.099257  0.158684  0.119397  0.107522  0.052266  0.080984  0.104612   \n","41  0.273330  0.291877  0.343993  0.558353  0.252498  0.216288  0.647566   \n","42  0.564042  0.228305  0.636147  0.465784  0.727732  0.433457  0.272619   \n","44  0.405524  0.238644  0.749902  0.236529  0.034603  0.322453  0.623932   \n","45  0.121233  0.154750  0.543235  0.289687  0.045632  0.237090  0.114697   \n","46  0.050891  0.094307  0.094286  0.025399  0.067310  0.205254  0.117588   \n","47  0.249964  0.118048  0.350482  0.602391  0.289237  0.099252  1.339910   \n","48  0.490011  0.583011  0.636926  0.257066  0.543998  0.293776  0.532469   \n","49  0.834070  0.573318  0.481451  0.473756  0.258202  0.379576  0.245187   \n","\n","       2_499  \n","2   0.127475  \n","3   0.270474  \n","4   0.928419  \n","5   1.257160  \n","6   0.222228  \n","7   0.275812  \n","8   0.740404  \n","9   0.504363  \n","10  0.144051  \n","11  0.150844  \n","12  0.742125  \n","13  0.168297  \n","14  0.311858  \n","15  1.079688  \n","16  0.806355  \n","17  0.524951  \n","18  0.938260  \n","19  0.303037  \n","20  2.259685  \n","21  0.053389  \n","22  0.206362  \n","23  0.594708  \n","24  0.371348  \n","25  0.529242  \n","26  0.278819  \n","27  0.599061  \n","28  1.341713  \n","29  0.139394  \n","30  0.290698  \n","32  1.217628  \n","33  0.438183  \n","34  0.352413  \n","35  0.714352  \n","36  0.104164  \n","37  0.536429  \n","38  0.362952  \n","40  0.191639  \n","41  1.063467  \n","42  0.372647  \n","44  1.111741  \n","45  0.154504  \n","46  0.056213  \n","47  0.274666  \n","48  0.292245  \n","49  0.388276  \n","\n","[45 rows x 2000 columns]\n","<class 'pandas.core.frame.DataFrame'>\n","           0_0         0_1         0_2        0_3        0_4         0_5  \\\n","0    47.685046   48.416904   37.371002  61.604757  58.732738  213.067966   \n","1   152.726718  297.353726  111.168880  25.419423  14.636085   24.609688   \n","31  256.385982  441.211213  107.449684  70.781661  72.694683  105.637914   \n","39   11.071519    7.924814    6.367171   2.884096   9.385375    8.634683   \n","43   13.818771   12.861720    7.176237  23.215493  45.191018  139.527824   \n","\n","           0_6         0_7        0_8        0_9  ...     2_490     2_491  \\\n","0   120.890451   40.100639  19.106303   6.043738  ...  0.358220  0.394612   \n","1    49.533842  111.513308  30.925956  25.681679  ...  0.214195  0.346208   \n","31   47.986717   18.116520  12.969337  21.154639  ...  0.102591  0.046813   \n","39   90.831259    4.567827   4.655512   8.270545  ...  1.127433  0.520051   \n","43   54.492808   38.533570  14.830765   7.000797  ...  0.555260  0.283301   \n","\n","       2_492     2_493     2_494     2_495     2_496     2_497     2_498  \\\n","0   0.107830  0.425291  0.941574  0.526005  0.306785  0.560049  0.350928   \n","1   0.477954  0.076745  0.172020  0.257445  0.456032  0.486665  0.315492   \n","31  0.069053  0.133518  0.122981  0.070043  0.051580  0.109471  0.137101   \n","39  0.698503  0.373569  0.387495  0.616913  0.153032  0.600710  0.672614   \n","43  0.469768  0.674416  0.483574  0.641788  0.548664  0.063886  0.231984   \n","\n","       2_499  \n","0   0.119156  \n","1   0.041322  \n","31  0.183540  \n","39  0.241303  \n","43  0.492832  \n","\n","[5 rows x 2000 columns]\n","shape after balancing (74, 2000)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 1 is smaller than n_iter=10. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [nan]\n","  warnings.warn(\n","<ipython-input-59-707daa621038>:65: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  best_hp_3_before = best_hp_3_before.append(rand_search_3a.best_params_,ignore_index=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 20 is smaller than n_iter=50. Running 20 iterations. For exhaustive searches, use GridSearchCV.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["shape of X_design after pca (74, 2)\n","size of features selected(1,)\n","shape of X_design after univariate: (74, 1)\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-59-707daa621038>:107: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  results_3 = results_3.append(results,ignore_index=True)\n","<ipython-input-59-707daa621038>:122: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  best_hp_3 = best_hp_3.append(model_3.best_params_,ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["Size_X_design (45, 2000)\n","Size X_test (5, 2000)\n","The total number of outliers in dataset x is 3454\n","The total number of outliers in dataset x is 1230\n","<class 'pandas.core.frame.DataFrame'>\n","            0_0         0_1         0_2        0_3         0_4         0_5  \\\n","0     47.685046   48.416904   37.371002  61.604757   58.732738  125.586049   \n","1    152.726718   94.379877  111.168880  25.419423   14.636085   24.609688   \n","2      1.601260    3.882169   22.978997  21.673040    8.919484   18.329286   \n","3      1.388947    3.052483    3.084103   4.627886   10.016196   15.020347   \n","5      7.897582    4.750754    5.923150   3.999941    7.348004    3.336407   \n","6      6.313823    7.525186   14.898173   7.973962    7.741924    9.925077   \n","7      7.862030   21.680769   25.337318   4.579382    5.601682   18.016299   \n","8      3.933808   14.202943   13.572333  13.162149    6.623755   92.045714   \n","9     65.713418   17.930338    9.035395  11.099228    8.727922    8.879810   \n","10     8.909385   26.552703   45.105873  27.323183   10.273487   30.048169   \n","12     6.323226   15.953172   26.120406   5.109627    4.012020   16.225039   \n","13     4.980006    1.113433    3.434486   0.392388    4.746581    0.468230   \n","14     9.501722    5.586242   16.450204   9.001370   13.485390   12.654643   \n","15    18.644972   24.808661   29.530931  34.786282   81.380679   91.139503   \n","16     0.812172   11.987505   19.817093  26.210313   55.224366   73.259071   \n","17  1264.963015   29.076832    4.174963  10.406641    7.447515   38.463177   \n","19     0.083161    5.929321    6.917387   6.924807   10.855910   17.821533   \n","20   160.601443  143.422714   38.376866  19.670381   12.283158   17.170820   \n","21   462.167939   94.379877  101.006717  95.428088   81.632232   28.433531   \n","22    41.375439   36.520654   35.163455  28.184115   94.563472   21.907995   \n","23     7.554942   18.251545   19.131640   4.189196   13.567905   13.140102   \n","24     6.263398    6.768979    2.564803   3.499548    5.535409   16.535461   \n","26   385.875221   28.181637   19.201062  16.526245   14.160023   18.202051   \n","27   757.622520   14.768699   16.772315   7.756356   13.015541   36.445433   \n","28   142.319356   40.861111   14.311447  12.825285   10.168552   15.789614   \n","29   197.409047   54.839062   39.124339  31.364328   55.818256   70.136786   \n","30   343.675806   48.950335   56.999643  10.091110    4.373090    6.670339   \n","31   595.676919   94.379877  107.449684  70.781661   72.694683  105.637914   \n","32   866.953418   94.379877  101.006717  53.340233  107.875803   39.668026   \n","33     7.547616    4.441597    8.504461   9.263530    5.448589    4.011405   \n","34     8.711248    5.446407    5.967202   6.607189   28.552336   73.130378   \n","35     2.362146    6.863112   22.550274  26.931418  105.863752   34.476580   \n","36     9.161991   10.856414   15.152315  11.796925   17.957884   25.792741   \n","37     5.233561    3.832656    3.096513   8.760333    2.037096   10.374480   \n","38   595.676919   88.885184  110.685906  20.589209   16.015180   30.676678   \n","39    11.071519    7.924814    6.367171   2.884096    9.385375    8.634683   \n","40   595.676919   21.596540  101.006717  43.813298  115.781996   33.964272   \n","41   939.366407   94.379877   71.517571  36.033812   34.987714   93.368641   \n","42     4.459459    9.978440   22.775076  30.872979   36.263121   54.213621   \n","43    13.818771   12.861720    7.176237  23.215493   45.191018  139.527824   \n","44     2.044676   12.641916   19.011975  10.018019   14.470866    5.059857   \n","45  1009.570351  164.825685  101.006717  45.532984   33.355747   19.186123   \n","46   595.676919  135.084186   31.630025  42.269784   23.539116   86.860689   \n","47     5.076651    0.396899   18.727030   4.525304   21.604739   40.037709   \n","49     0.835017    6.575501   12.352972  35.881337   20.712190   49.123780   \n","\n","           0_6         0_7         0_8        0_9  ...     2_490     2_491  \\\n","0   120.890451   40.100639   19.106303   6.043738  ...  0.358220  0.394612   \n","1    49.533842  164.804404   87.843373  70.032909  ...  0.214195  0.346208   \n","2    27.710604   23.350084   37.896254   8.331180  ...  1.084666  0.161850   \n","3   139.823127   20.149108   10.393109   9.241921  ...  0.651411  0.714947   \n","5     9.475968   29.113818   11.802082   4.880755  ...  0.923439  0.480057   \n","6    19.325956   68.563423   15.139322  11.386458  ...  0.582105  0.730925   \n","7    66.027558   23.029366    5.820312   3.542084  ...  0.706435  0.297448   \n","8    30.067347   26.844976   10.494297  35.291274  ...  1.159549  0.658967   \n","9     8.097210   44.556589   47.113102  21.933059  ...  0.398205  0.245733   \n","10   35.091876   45.291780   20.100238   8.416225  ...  0.311644  0.227725   \n","12  114.460883    8.289232    4.147352   5.958195  ...  0.420808  0.370241   \n","13   11.634919   61.111967   12.213330   9.727997  ...  0.174855  0.509130   \n","14    6.836319   37.801792   87.602074  23.830667  ...  0.357896  0.024080   \n","15  221.516644   99.557117   87.843373  70.032909  ...  1.368502  1.327078   \n","16   44.214046   31.987609    9.732927  26.773551  ...  0.872281  0.371031   \n","17   82.489406   38.985256   16.685890  31.899988  ...  0.626253  0.756545   \n","19   51.306309   70.669414   30.232692  22.993600  ...  0.457623  0.635237   \n","20   15.512118  111.673362    4.522202   8.525927  ...  1.914944  1.652935   \n","21   50.453163   62.488043   72.735797  52.063825  ...  0.264310  0.277353   \n","22    3.091364    6.026701   11.166130  10.255603  ...  0.786101  0.339428   \n","23    2.874134    6.775483    5.447489  62.872336  ...  1.082336  1.203700   \n","24  111.679516   13.382436   12.472009   9.521238  ...  0.278984  0.834699   \n","26   24.457990   61.877175  111.923601  23.398021  ...  0.098929  0.228524   \n","27   38.455515   12.016985    5.105857   2.548889  ...  0.176093  0.374499   \n","28   12.107519  164.804404   25.259786   1.638292  ...  1.681141  1.203190   \n","29   36.563568   23.705583    3.889423  24.028924  ...  0.199977  0.345672   \n","30   55.156363    4.515468    5.253856   2.463021  ...  0.162845  0.405848   \n","31   47.986717   18.116520   12.969337  21.154639  ...  0.102591  0.046813   \n","32   68.127807   35.020284   36.985907  63.162215  ...  0.868401  1.237484   \n","33   10.682050   18.004418    8.953420  70.032909  ...  0.826393  0.213216   \n","34   28.400980    5.935044    7.377658  18.182313  ...  1.089200  0.892079   \n","35   23.091628    5.187541   12.055103  20.776187  ...  0.766081  0.433455   \n","36   84.025145   15.480995   10.164311  14.735534  ...  0.032671  0.072995   \n","37   22.269579   90.313724   27.277003   3.746374  ...  0.547558  0.060141   \n","38   20.490327   58.920433  106.431948  26.090899  ...  0.348429  0.085254   \n","39   90.831259    4.567827    4.655512   8.270545  ...  1.127433  0.520051   \n","40   72.461802   65.642596   40.088273  16.303774  ...  0.133208  0.196779   \n","41   44.387696   33.029416   16.736460  29.516598  ...  0.271802  0.350006   \n","42   12.953561    8.607844    9.163925   8.467530  ...  0.904045  0.442346   \n","43   54.492808   38.533570   14.830765   7.000797  ...  0.555260  0.283301   \n","44    3.315126    5.018288   54.628473  27.966706  ...  0.216970  1.005113   \n","45   13.055429   24.261486   40.078577  21.321404  ...  0.259497  0.219041   \n","46   23.142288   18.125929   10.131506  18.044463  ...  0.111183  0.069266   \n","47   99.842998   38.261544   10.927228   4.330965  ...  0.419429  0.717668   \n","49   82.917623   58.757996    8.723075  11.815731  ...  0.554465  0.721695   \n","\n","       2_492     2_493     2_494     2_495     2_496     2_497     2_498  \\\n","0   0.107830  0.425291  0.941574  0.526005  0.306785  0.560049  0.350928   \n","1   0.477954  0.076745  0.172020  0.257445  0.456032  0.486665  0.315492   \n","2   0.306773  0.415093  0.317944  0.265990  0.113143  0.709653  1.012296   \n","3   0.125822  0.643305  0.320884  0.046181  0.322935  0.348999  0.153024   \n","5   1.352786  0.365713  0.711593  0.606949  0.499613  0.208698  1.025403   \n","6   0.605998  0.831952  0.895054  0.767637  0.888651  0.876977  1.308258   \n","7   0.336976  0.642969  0.741255  0.614384  0.492811  0.933041  0.302152   \n","8   0.813533  0.926121  0.649347  0.511128  0.409690  0.293529  0.772780   \n","9   0.433560  0.740165  0.782075  1.319605  0.997819  0.311671  0.175690   \n","10  0.155983  0.110273  0.182669  0.175323  0.234637  0.177746  0.146612   \n","12  0.393852  0.455820  0.467426  0.564135  0.265910  0.554738  0.828600   \n","13  0.135521  0.063941  0.249348  0.356608  0.367951  0.174740  0.380145   \n","14  0.431905  0.663539  0.912721  0.262156  1.035282  0.538613  0.382229   \n","15  1.090115  1.189827  1.386800  1.159281  1.106265  1.203004  1.552537   \n","16  0.416056  0.225756  0.388362  1.129571  0.313923  0.213291  0.659616   \n","17  0.737900  0.896208  0.836594  0.633173  0.842181  0.501235  0.822268   \n","19  0.333805  0.690953  0.848387  0.253394  0.943507  0.702775  0.588625   \n","20  1.704367  1.741195  1.835064  1.791172  1.617813  1.849128  1.898409   \n","21  0.158751  0.140927  0.247623  0.207862  0.081059  0.076047  0.144896   \n","22  0.895299  0.266459  0.115387  0.271458  0.466967  0.760682  0.591807   \n","23  0.447047  0.344257  0.105696  0.777201  0.113358  1.058979  0.565462   \n","24  0.764613  0.613606  0.309103  0.216962  0.072947  1.404239  0.520531   \n","26  0.214828  0.080228  0.102691  0.165870  0.093455  0.124307  0.076448   \n","27  0.642888  0.699057  0.370144  0.455704  0.179700  0.843407  0.576035   \n","28  0.359781  1.255172  1.692066  1.233744  0.550624  1.284502  1.671745   \n","29  0.089255  0.197561  0.182011  0.131626  0.239856  0.355817  0.132830   \n","30  0.230583  0.207286  0.038841  0.102040  0.313409  0.223369  0.406877   \n","31  0.069053  0.133518  0.122981  0.070043  0.051580  0.109471  0.137101   \n","32  1.034963  1.172944  0.996569  1.151716  0.913109  1.038909  0.844756   \n","33  0.366293  0.201832  0.310691  0.528135  0.420128  0.209581  0.246273   \n","34  0.366810  0.646200  0.770653  0.269622  0.263445  0.560603  0.416983   \n","35  0.566932  0.356422  0.486543  0.362846  0.608656  0.276555  0.546481   \n","36  0.038886  0.011217  0.044853  0.077136  0.057671  0.107105  0.071548   \n","37  0.443698  1.153577  0.682995  0.238065  0.628543  0.676422  0.533781   \n","38  0.221490  0.237556  0.443707  0.048757  0.259962  0.272971  0.385217   \n","39  0.698503  0.373569  0.387495  0.616913  0.153032  0.600710  0.773475   \n","40  0.099257  0.158684  0.119397  0.107522  0.052266  0.080984  0.104612   \n","41  0.273330  0.291877  0.343993  0.558353  0.252498  0.216288  0.647566   \n","42  0.564042  0.228305  0.636147  0.465784  0.727732  0.433457  0.272619   \n","43  0.469768  0.674416  0.483574  0.641788  0.548664  0.063886  0.231984   \n","44  0.405524  0.238644  0.749902  0.236529  0.034603  0.322453  0.623932   \n","45  0.121233  0.154750  0.543235  0.289687  0.045632  0.237090  0.114697   \n","46  0.050891  0.094307  0.094286  0.025399  0.067310  0.205254  0.117588   \n","47  0.249964  0.118048  0.350482  0.602391  0.289237  0.099252  1.339910   \n","49  0.834070  0.573318  0.481451  0.473756  0.258202  0.379576  0.245187   \n","\n","       2_499  \n","0   0.119156  \n","1   0.041322  \n","2   0.127475  \n","3   0.270474  \n","5   1.257160  \n","6   0.222228  \n","7   0.275812  \n","8   0.740404  \n","9   0.504363  \n","10  0.144051  \n","12  0.742125  \n","13  0.168297  \n","14  0.311858  \n","15  1.079688  \n","16  0.806355  \n","17  0.524951  \n","19  0.303037  \n","20  2.259685  \n","21  0.053389  \n","22  0.206362  \n","23  0.594708  \n","24  0.371348  \n","26  0.278819  \n","27  0.599061  \n","28  1.341713  \n","29  0.139394  \n","30  0.290698  \n","31  0.183540  \n","32  1.217628  \n","33  0.438183  \n","34  0.352413  \n","35  0.714352  \n","36  0.104164  \n","37  0.536429  \n","38  0.362952  \n","39  0.241303  \n","40  0.191639  \n","41  1.063467  \n","42  0.372647  \n","43  0.864135  \n","44  1.111741  \n","45  0.154504  \n","46  0.056213  \n","47  0.274666  \n","49  0.388276  \n","\n","[45 rows x 2000 columns]\n","<class 'pandas.core.frame.DataFrame'>\n","           0_0        0_1        0_2        0_3        0_4         0_5  \\\n","4     3.625561   3.728466   6.205367  17.722897   5.699401    9.024836   \n","11  894.420487  47.319198  35.619225  32.931959  25.131503   95.379695   \n","18  247.728859  82.530058  64.737390  62.134191  87.307617  144.387597   \n","25    2.291710   3.039610   5.481645   9.873051   3.657961    7.788805   \n","48    5.212389  22.435944  30.438124  25.494512  27.480230   19.577305   \n","\n","          0_6        0_7         0_8        0_9  ...     2_490     2_491  \\\n","4   15.782812  33.336480   55.257804  19.139128  ...  0.114396  0.354976   \n","11  81.966685  36.308177   17.355844  10.090681  ...  0.096375  0.122703   \n","18  52.225949  43.735204  159.002631  29.474394  ...  0.776095  0.998492   \n","25  46.180684  22.241667    6.703946   7.967750  ...  0.732570  0.879275   \n","48  21.970321  34.466662    6.322254   2.679117  ...  0.809580  0.377657   \n","\n","       2_492     2_493     2_494     2_495     2_496     2_497     2_498  \\\n","4   0.434708  1.055659  0.547044  0.143111  0.484099  0.336949  0.544538   \n","11  0.113910  0.021568  0.174755  0.143924  0.241262  0.052990  0.141293   \n","18  0.843273  0.964916  0.925053  0.335079  0.921904  0.601898  0.983892   \n","25  1.257180  0.437430  0.881269  0.042347  0.810611  0.256283  0.888381   \n","48  0.490011  0.583011  0.636926  0.257066  0.543998  0.293776  0.532469   \n","\n","       2_499  \n","4   0.928419  \n","11  0.150844  \n","18  0.938260  \n","25  0.529242  \n","48  0.292245  \n","\n","[5 rows x 2000 columns]\n","shape after balancing (74, 2000)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 1 is smaller than n_iter=10. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [nan]\n","  warnings.warn(\n","<ipython-input-59-707daa621038>:65: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  best_hp_3_before = best_hp_3_before.append(rand_search_3a.best_params_,ignore_index=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 20 is smaller than n_iter=50. Running 20 iterations. For exhaustive searches, use GridSearchCV.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["shape of X_design after pca (74, 2)\n","size of features selected(1,)\n","shape of X_design after univariate: (74, 1)\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-59-707daa621038>:107: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  results_3 = results_3.append(results,ignore_index=True)\n","<ipython-input-59-707daa621038>:122: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  best_hp_3 = best_hp_3.append(model_3.best_params_,ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["Size_X_design (45, 2000)\n","Size X_test (5, 2000)\n","The total number of outliers in dataset x is 3660\n","The total number of outliers in dataset x is 752\n","<class 'pandas.core.frame.DataFrame'>\n","            0_0         0_1         0_2         0_3         0_4         0_5  \\\n","0     47.685046   48.416904   37.371002   61.604757   58.732738  213.067966   \n","1    152.726718  104.766591  111.168880   25.419423   14.636085   24.609688   \n","2      1.601260    3.882169   22.978997   21.673040    8.919484   18.329286   \n","3      1.388947    3.052483    3.084103    4.627886   10.016196   15.020347   \n","4      3.625561    3.728466    6.205367   17.722897    5.699401    9.024836   \n","5      7.897582    4.750754    5.923150    3.999941    7.348004    3.336407   \n","6      6.313823    7.525186   14.898173    7.973962    7.741924    9.925077   \n","7      7.862030   21.680769   25.337318    4.579382    5.601682   18.016299   \n","8      3.933808   14.202943   13.572333   13.162149    6.623755   92.045714   \n","10     8.909385   26.552703   45.105873   27.323183   10.273487   30.048169   \n","11   894.420487   47.319198   35.619225   32.931959   25.131503   95.379695   \n","12     6.323226   15.953172   26.120406    5.109627    4.012020   16.225039   \n","13     4.980006    1.113433    3.434486    0.392388    4.746581    0.468230   \n","14     9.501722    5.586242   16.450204    9.001370   13.485390   12.654643   \n","15    18.644972   24.808661   29.530931   34.786282   81.380679   91.139503   \n","16     0.812172   11.987505   19.817093   26.210313   55.224366   73.259071   \n","17  1264.963015   29.076832    4.174963   10.406641    7.447515   38.463177   \n","18   247.728859   82.530058   64.737390  101.079961  111.694809  144.387597   \n","20   160.601443  143.422714   38.376866   19.670381   12.283158   17.170820   \n","21   462.167939  104.766591  119.125112   95.428088   81.632232   28.433531   \n","22    41.375439   36.520654   35.163455   28.184115   94.563472   21.907995   \n","23     7.554942   18.251545   19.131640    4.189196   13.567905   13.140102   \n","24     6.263398    6.768979    2.564803    3.499548    5.535409   16.535461   \n","25     2.291710    3.039610    5.481645    9.873051    3.657961    7.788805   \n","26   385.875221   28.181637   19.201062   16.526245   14.160023   18.202051   \n","27   757.622520   14.768699   16.772315    7.756356   13.015541   36.445433   \n","28   142.319356   40.861111   14.311447   12.825285   10.168552   15.789614   \n","29   197.409047   54.839062   39.124339   31.364328   55.818256   70.136786   \n","31   627.175818  104.766591  107.449684   70.781661   72.694683  105.637914   \n","32   866.953418  104.766591  119.125112   53.340233  107.875803   39.668026   \n","34     8.711248    5.446407    5.967202    6.607189   28.552336   73.130378   \n","36     9.161991   10.856414   15.152315   11.796925   17.957884   25.792741   \n","37     5.233561    3.832656    3.096513    8.760333    2.037096   10.374480   \n","38   627.175818   88.885184  110.685906   20.589209   16.015180   30.676678   \n","39    11.071519    7.924814    6.367171    2.884096    9.385375    8.634683   \n","40   627.175818   21.596540  119.125112   43.813298  111.694809   33.964272   \n","41   939.366407  104.766591   71.517571   36.033812   34.987714   93.368641   \n","42     4.459459    9.978440   22.775076   30.872979   36.263121   54.213621   \n","43    13.818771   12.861720    7.176237   23.215493   45.191018  139.527824   \n","44     2.044676   12.641916   19.011975   10.018019   14.470866    5.059857   \n","45  1009.570351  164.825685  119.125112   45.532984   33.355747   19.186123   \n","46   627.175818  135.084186   31.630025   42.269784   23.539116   86.860689   \n","47     5.076651    0.396899   18.727030    4.525304   21.604739   40.037709   \n","48     5.212389   22.435944   30.438124   25.494512   27.480230   19.577305   \n","49     0.835017    6.575501   12.352972   35.881337   20.712190   49.123780   \n","\n","           0_6         0_7         0_8        0_9  ...     2_490     2_491  \\\n","0   120.890451   40.100639   19.106303   6.043738  ...  0.358220  0.394612   \n","1    49.533842  124.487884   80.590932  70.759674  ...  0.214195  0.346208   \n","2    27.710604   23.350084   37.896254   8.331180  ...  1.084666  0.161850   \n","3   139.823127   20.149108   10.393109   9.241921  ...  0.651411  0.714947   \n","4    15.782812   33.336480   55.257804  19.139128  ...  0.114396  0.354976   \n","5     9.475968   29.113818   11.802082   4.880755  ...  0.923439  0.480057   \n","6    19.325956   68.563423   15.139322  11.386458  ...  0.582105  0.730925   \n","7    66.027558   23.029366    5.820312   3.542084  ...  0.706435  0.297448   \n","8    30.067347   26.844976   10.494297  35.291274  ...  1.159549  0.658967   \n","10   35.091876   45.291780   20.100238   8.416225  ...  0.311644  0.227725   \n","11   81.966685   36.308177   17.355844  10.090681  ...  0.096375  0.122703   \n","12  114.460883    8.289232    4.147352   5.958195  ...  0.420808  0.370241   \n","13   11.634919   61.111967   12.213330   9.727997  ...  0.174855  0.509130   \n","14    6.836319   37.801792   87.602074  23.830667  ...  0.357896  0.024080   \n","15  221.524711   99.557117   80.590932  70.759674  ...  1.368502  1.327078   \n","16   44.214046   31.987609    9.732927  26.773551  ...  0.872281  0.371031   \n","17   82.489406   38.985256   16.685890  31.899988  ...  0.626253  0.756545   \n","18   52.225949   43.735204   80.590932  70.759674  ...  0.776095  0.998492   \n","20   15.512118  111.673362    4.522202   8.525927  ...  1.914944  1.652935   \n","21   50.453163   62.488043   72.735797  52.063825  ...  0.264310  0.277353   \n","22    3.091364    6.026701   11.166130  10.255603  ...  0.786101  0.339428   \n","23    2.874134    6.775483    5.447489  62.872336  ...  1.082336  1.203700   \n","24  111.679516   13.382436   12.472009   9.521238  ...  0.278984  0.834699   \n","25   46.180684   17.594202    6.703946   7.967750  ...  0.732570  0.879275   \n","26   24.457990   61.877175  111.923601  23.398021  ...  0.098929  0.228524   \n","27   38.455515   12.016985    5.105857   2.548889  ...  0.176093  0.374499   \n","28   12.107519  124.487884   25.259786   1.638292  ...  1.681141  1.203190   \n","29   36.563568   23.705583    3.889423  24.028924  ...  0.199977  0.345672   \n","31   47.986717   18.116520   12.969337  21.154639  ...  0.102591  0.046813   \n","32   68.127807   35.020284   36.985907  63.162215  ...  0.868401  1.237484   \n","34   28.400980    5.935044    7.377658  18.182313  ...  1.089200  0.892079   \n","36   84.025145   15.480995   10.164311  14.735534  ...  0.032671  0.072995   \n","37   22.269579   90.313724   27.277003   3.746374  ...  0.547558  0.060141   \n","38   20.490327   58.920433  106.431948  26.090899  ...  0.348429  0.085254   \n","39   90.831259    4.567827    4.655512   8.270545  ...  1.127433  0.520051   \n","40   72.461802   65.642596   40.088273  16.303774  ...  0.133208  0.196779   \n","41   44.387696   33.029416   16.736460  29.516598  ...  0.271802  0.350006   \n","42   12.953561    8.607844    9.163925   8.467530  ...  0.904045  0.442346   \n","43   54.492808   38.533570   14.830765   7.000797  ...  0.555260  0.283301   \n","44    3.315126    5.018288   54.628473  27.966706  ...  0.216970  1.005113   \n","45   13.055429   24.261486   40.078577  21.321404  ...  0.259497  0.219041   \n","46   23.142288   18.125929   10.131506  18.044463  ...  0.111183  0.069266   \n","47   99.842998   38.261544   10.927228   4.330965  ...  0.419429  0.717668   \n","48   21.970321   34.466662    6.322254   2.679117  ...  0.809580  0.377657   \n","49   82.917623   58.757996    8.723075  11.815731  ...  0.554465  0.721695   \n","\n","       2_492     2_493     2_494     2_495     2_496     2_497     2_498  \\\n","0   0.107830  0.425291  0.941574  0.526005  0.306785  0.560049  0.350928   \n","1   0.477954  0.076745  0.172020  0.257445  0.456032  0.486665  0.315492   \n","2   0.306773  0.415093  0.317944  0.265990  0.113143  0.709653  1.012296   \n","3   0.125822  0.643305  0.320884  0.046181  0.322935  0.348999  0.153024   \n","4   0.434708  1.055659  0.547044  0.143111  0.484099  0.336949  0.544538   \n","5   1.352786  0.365713  0.711593  0.606949  0.499613  0.208698  1.025403   \n","6   0.605998  0.831952  0.895054  0.767637  0.888651  0.876977  1.308258   \n","7   0.336976  0.642969  0.741255  0.614384  0.492811  0.933041  0.302152   \n","8   0.813533  0.926121  0.649347  0.511128  0.409690  0.293529  0.772780   \n","10  0.155983  0.110273  0.182669  0.175323  0.234637  0.177746  0.146612   \n","11  0.113910  0.021568  0.174755  0.143924  0.241262  0.052990  0.141293   \n","12  0.393852  0.455820  0.467426  0.564135  0.265910  0.554738  0.828600   \n","13  0.135521  0.063941  0.249348  0.356608  0.367951  0.174740  0.380145   \n","14  0.431905  0.663539  0.912721  0.262156  1.035282  0.538613  0.382229   \n","15  1.090115  1.189827  1.386800  1.159281  1.106265  1.203004  1.552537   \n","16  0.416056  0.225756  0.388362  1.129571  0.313923  0.213291  0.659616   \n","17  0.737900  0.896208  0.836594  0.633173  0.842181  0.501235  0.822268   \n","18  0.843273  0.964916  0.925053  0.910381  0.921904  0.843743  0.983892   \n","20  1.800877  1.741195  1.835064  1.830482  1.617813  1.849128  1.898409   \n","21  0.158751  0.140927  0.247623  0.207862  0.081059  0.076047  0.144896   \n","22  0.895299  0.266459  0.115387  0.271458  0.466967  0.760682  0.591807   \n","23  0.447047  0.344257  0.105696  0.777201  0.113358  1.058979  0.565462   \n","24  0.764613  0.613606  0.309103  0.216962  0.072947  1.404239  0.520531   \n","25  1.257180  0.437430  0.881269  0.042347  0.810611  0.256283  0.888381   \n","26  0.214828  0.080228  0.102691  0.165870  0.093455  0.124307  0.076448   \n","27  0.642888  0.699057  0.370144  0.455704  0.179700  0.843407  0.576035   \n","28  0.359781  1.255172  1.692066  1.233744  0.550624  1.284502  1.671745   \n","29  0.089255  0.197561  0.182011  0.131626  0.239856  0.355817  0.132830   \n","31  0.069053  0.133518  0.122981  0.070043  0.051580  0.109471  0.137101   \n","32  1.034963  1.172944  0.996569  1.151716  0.913109  1.038909  0.844756   \n","34  0.366810  0.646200  0.770653  0.269622  0.263445  0.560603  0.416983   \n","36  0.038886  0.011217  0.044853  0.077136  0.057671  0.107105  0.071548   \n","37  0.443698  1.153577  0.682995  0.238065  0.628543  0.676422  0.533781   \n","38  0.221490  0.237556  0.443707  0.048757  0.259962  0.272971  0.385217   \n","39  0.698503  0.373569  0.387495  0.616913  0.153032  0.600710  0.773475   \n","40  0.099257  0.158684  0.119397  0.107522  0.052266  0.080984  0.104612   \n","41  0.273330  0.291877  0.343993  0.558353  0.252498  0.216288  0.647566   \n","42  0.564042  0.228305  0.636147  0.465784  0.727732  0.433457  0.272619   \n","43  0.469768  0.674416  0.483574  0.641788  0.548664  0.063886  0.231984   \n","44  0.405524  0.238644  0.749902  0.236529  0.034603  0.322453  0.623932   \n","45  0.121233  0.154750  0.543235  0.289687  0.045632  0.237090  0.114697   \n","46  0.050891  0.094307  0.094286  0.025399  0.067310  0.205254  0.117588   \n","47  0.249964  0.118048  0.350482  0.602391  0.289237  0.099252  1.339910   \n","48  0.490011  0.583011  0.636926  0.257066  0.543998  0.293776  0.532469   \n","49  0.834070  0.573318  0.481451  0.473756  0.258202  0.379576  0.245187   \n","\n","       2_499  \n","0   0.119156  \n","1   0.041322  \n","2   0.127475  \n","3   0.270474  \n","4   0.928419  \n","5   1.257160  \n","6   0.222228  \n","7   0.275812  \n","8   0.740404  \n","10  0.144051  \n","11  0.150844  \n","12  0.742125  \n","13  0.168297  \n","14  0.311858  \n","15  1.079688  \n","16  0.806355  \n","17  0.524951  \n","18  0.938260  \n","20  2.259685  \n","21  0.053389  \n","22  0.206362  \n","23  0.594708  \n","24  0.371348  \n","25  0.529242  \n","26  0.278819  \n","27  0.599061  \n","28  1.341713  \n","29  0.139394  \n","31  0.183540  \n","32  1.217628  \n","34  0.352413  \n","36  0.104164  \n","37  0.536429  \n","38  0.362952  \n","39  0.241303  \n","40  0.191639  \n","41  1.063467  \n","42  0.372647  \n","43  0.864135  \n","44  1.111741  \n","45  0.154504  \n","46  0.056213  \n","47  0.274666  \n","48  0.292245  \n","49  0.388276  \n","\n","[45 rows x 2000 columns]\n","<class 'pandas.core.frame.DataFrame'>\n","          0_0        0_1        0_2        0_3        0_4        0_5  \\\n","9   65.713418  17.930338   9.035395  11.099228   8.727922   8.879810   \n","19   0.083161   5.929321   6.917387   6.924807  10.855910  17.821533   \n","30  82.979067  48.950335  56.999643  10.091110   4.373090   6.670339   \n","33   7.547616   4.441597   8.504461   9.263530   5.448589   4.011405   \n","35   2.362146   6.863112  22.550274  15.336010  21.500532  34.476580   \n","\n","          0_6        0_7        0_8        0_9  ...     2_490     2_491  \\\n","9    8.097210  44.556589  47.113102  21.933059  ...  0.398205  0.245733   \n","19  51.306309  70.669414  30.232692  22.993600  ...  0.457623  0.635237   \n","30  55.156363   4.515468   5.253856  18.028504  ...  0.162845  0.405848   \n","33  10.682050  18.004418   8.953420  25.789449  ...  0.826393  0.213216   \n","35  23.091628   5.187541  12.055103  20.776187  ...  0.766081  0.433455   \n","\n","       2_492     2_493     2_494     2_495     2_496     2_497     2_498  \\\n","9   0.433560  0.740165  0.782075  1.319605  0.997819  0.311671  0.175690   \n","19  0.333805  0.690953  0.848387  0.253394  0.943507  0.481571  0.588625   \n","30  0.230583  0.207286  0.038841  0.102040  0.313409  0.223369  0.406877   \n","33  0.366293  0.201832  0.310691  0.528135  0.420128  0.209581  0.246273   \n","35  0.566932  0.356422  0.486543  0.362846  0.608656  0.276555  0.546481   \n","\n","       2_499  \n","9   0.504363  \n","19  0.303037  \n","30  0.290698  \n","33  0.438183  \n","35  0.714352  \n","\n","[5 rows x 2000 columns]\n","shape after balancing (74, 2000)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 1 is smaller than n_iter=10. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [nan]\n","  warnings.warn(\n","<ipython-input-59-707daa621038>:65: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  best_hp_3_before = best_hp_3_before.append(rand_search_3a.best_params_,ignore_index=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 20 is smaller than n_iter=50. Running 20 iterations. For exhaustive searches, use GridSearchCV.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["shape of X_design after pca (74, 2)\n","size of features selected(1,)\n","shape of X_design after univariate: (74, 1)\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-59-707daa621038>:107: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  results_3 = results_3.append(results,ignore_index=True)\n","<ipython-input-59-707daa621038>:122: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  best_hp_3 = best_hp_3.append(model_3.best_params_,ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["Size_X_design (45, 2000)\n","Size X_test (5, 2000)\n","The total number of outliers in dataset x is 3216\n","The total number of outliers in dataset x is 916\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-59-707daa621038>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mX_design\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremoving_outliers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_design\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremoving_outliers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mX_design\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmissing_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_design\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmissing_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-49-3b76f3bbb175>\u001b[0m in \u001b[0;36mremoving_outliers\u001b[0;34m(X_design)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutliers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mq3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_design\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutliers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 3rd quartile of column where outlier is False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mq1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_design\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutliers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 1st quartile of column where outlier is False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0miqr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq3\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mq1\u001b[0m \u001b[0;31m# interquartile range of column where outlier is False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mlower_fence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0miqr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mquantile\u001b[0;34m(self, q, interpolation)\u001b[0m\n\u001b[1;32m   2679\u001b[0m         \u001b[0;31m# We dispatch to DataFrame so that core.internals only has to worry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2680\u001b[0m         \u001b[0;31m#  about 2D cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2681\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2683\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mto_frame\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1938\u001b[0m                 \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1939\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1940\u001b[0;31m                 \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1941\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m             \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, data, dtype, copy, name, tupleize_cols, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m                 )\n\u001b[1;32m    571\u001b[0m                 \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubarr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, data, dtype, copy, name, tupleize_cols, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray_tuplesafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_dtype_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/common.py\u001b[0m in \u001b[0;36masarray_tuplesafe\u001b[0;34m(values, dtype)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m             \u001b[0;31m# Can remove warning filter once NumPy 1.24 is min version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVisibleDeprecationWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.9/warnings.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_entered\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot enter %r twice\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_entered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["## Pipeline 4:PCA + Univariate -> KNN"],"metadata":{"id":"-bWqOArv0Etj"}},{"cell_type":"code","source":["# PIPELINE 4\n","# Define outer and inner cross validation\n","outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42) \n","inner_cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n","\n","# Creating empty arrays\n","f1_design_4_before = []\n","f1_test_4_before = []\n","f1_design_4_after = []\n","f1_test_4_after = []\n","results_4 = pd.DataFrame()\n","best_pipe_4 = []\n","best_hp_4_before = pd.DataFrame()\n","best_hp_4 = pd.DataFrame()\n","y_test_4_all = []\n","y_pred_4_all = []\n","\n","for design_index, test_index in outer_cv.split(X, y): \n","    X_design = X.transpose()[design_index]\n","    X_design = X_design.transpose()\n","    print(f'Size_X_design {X_design.shape}') # print size of X_design\n","    y_design = y[design_index]\n","    \n","    X_test = X.transpose()[test_index]\n","    X_test = X_test.transpose()\n","    y_test_4 = y[test_index].tolist()\n","    y_test_4_all.append(y_test_4)\n","    print(f'Size X_test {X_test.shape}')     # print size of X_test\n","\n","    # remove outliers\n","    X_design = removing_outliers(X_design)\n","    X_test = removing_outliers(X_test)\n","\n","    # Correct missing data\n","    X_design = missing_data(X_design)\n","    X_test = missing_data(X_test)\n","\n","    # balance the classes, so design set consists of 50% normal and 50% abnormal ECG's\n","    ros = RandomOverSampler(sampling_strategy='minority')\n","    X_resampled, y_resampled = ros.fit_resample(X_design, y_design)\n","    X_design = X_resampled\n","    y_design = y_resampled   \n","    print(f'shape after balancing {X_design.shape}')\n","\n","    # Define pipeline 4a\n","    pipeline_4a = Pipeline([\n","        ('scaler', RobustScaler()),\n","        ('var_threshold', VarianceThreshold(threshold=0.0)),\n","        ('pca', PCA(n_components=0.5)),\n","    ])\n","\n","    param_grid_4a = {\n","        'pca__n_components': [0.5],#0.75, 0.9, 0.95, 0.99],\n","        }\n","\n","    # Perform grid search with inner cross-validation, part 1\n","    rand_search_4a = RandomizedSearchCV(pipeline_4a, param_distributions=param_grid_4a, n_iter=10, cv=inner_cv, scoring='f1', n_jobs=-1) #klopt n__iter\n","    rand_search_4a.fit(X_design, y_design) # klopt dit fit_transform?\n","    X_design = rand_search_4a.best_estimator_.transform(X_design)\n","    X_test = rand_search_4a.best_estimator_.transform(X_test)\n","    \n","    # Access and store the best set of hyperparameters of each outer-CV loop in a DataFrame\n","    best_hp_4_before = best_hp_4_before.append(rand_search_4a.best_params_,ignore_index=True)\n","    print(f'shape of X_design after pca {X_design.shape}')\n","\n","    # univariate feature selection\n","    sel_kb = SelectKBest(f_classif, k='all')\n","    sel_kb.fit(X_design, y_design)\n","    p_values = sel_kb.pvalues_\n","\n","    reject_fdr, pvals_fdr, _, _ = multipletests(pvals=p_values, alpha=0.05, method='fdr_bh')\n","    features_selected=np.array(np.where(reject_fdr)[0])\n","    print(f'size of features selected{features_selected.shape}')\n","    X_design = X_design[:,features_selected]\n","    X_test = X_test[:,features_selected]\n","        \n","    print(f'shape of X_design after univariate: {X_design.shape}')\n","    \n","    # pipeline 4b\n","    pipeline_4b = Pipeline([    \n","        ('clf', KNeighborsClassifier())\n","    ])\n","\n","    # Define scores BEFORE hyperparameter tuning\n","    pipeline_4b.fit(X_design, y_design)\n"," \n","    y_pred_design_4 = pipeline_4b.predict(X_design)\n","    f1_design_4_bef = f1_score(y_design, y_pred_design_4)\n","    f1_design_4_before.append(f1_design_4_bef)\n","\n","    y_pred_test_4_before = pipeline_4b.predict(X_test)\n","    f1_test_4_bef = f1_score(y_test_4, y_pred_test_4_before)\n","    f1_test_4_before.append(f1_test_4_bef)\n","    \n","    # Define hyperparameters of pipeline 4\n","    param_grid_4b = {\n","    'clf__n_neighbors': list(range(4,26,2)), \n","    'clf__p': [1,2],\n","    'clf__leaf_size': np.arange(1,26,1)\n","    }\n","\n","    # Perform grid search with inner cross-validation, part 1\n","    model_4 = RandomizedSearchCV(pipeline_4b, param_distributions=param_grid_4b, cv=inner_cv, scoring='f1', n_iter= 50, n_jobs=-1) # optimize parameters\n","    model_4.fit(X_design, y_design)\n","\n","    results = pd.DataFrame(model_4.cv_results_)\n","    results_4 = results_4.append(results,ignore_index=True)\n","\n","    # Define scores AFTER hyperparameter tuning \n","    y_pred_design_4_after = model_4.predict(X_design)\n","    f1_design_4_aft = f1_score(y_design, y_pred_design_4_after)\n","    f1_design_4_after.append(f1_design_4_aft)\n","   \n","    y_pred_test_4_after = model_4.predict(X_test)\n","    y_pred_4_all.append(y_pred_test_4_after)\n","   \n","    f1_test_4_aft = f1_score(y_test_4, y_pred_test_4_after)\n","    f1_test_4_after.append(f1_test_4_aft)\n","\n","    # Access and store the best set of hyperparameters of each outer-CV loop in a DataFrame\n","    best_hp_4 = best_hp_4.append(model_4.best_params_,ignore_index=True)\n","    # Stores the optimum model in best_pipe\n","    best_pipe_4.append(model_4.best_estimator_)\n","\n","# Save results of inner CV into .csv file\n","results_4.to_csv('results_4.csv', index=False)\n","\n","print(f'Mean and std of F1 scores of pipeline 4: {statistics.mean(f1_test_4_after)} +/- {statistics.stdev(f1_test_4_after)}')\n","print(f'The optimal hyperparameters per split: {best_hp_4}')\n","print(f'The best pipes per split {best_pipe_4}')\n","print(f'The design F1 scores before tuning {f1_design_4_before}')\n","print(f'The test F1 scores before tuning {f1_test_4_before}')\n","print(f'The design F1 scores after tuning {f1_design_4_after}')\n","print(f'The test F1 scores afer tuning {f1_test_4_after}')\n","\n","data_4 = [f1_design_4_after, f1_test_4_after]\n","sns.boxplot(data=data_4)\n","plt.title('Boxplot F1 scores design and test pipeline 4')\n","plt.xlabel('design and test')\n","plt.ylabel('F1 score score')\n","plt.show()\n","\n","# Confusion matrix\n","# convert numpy arrays to Python lists\n","y_pred_4_all = [arr.tolist() for arr in y_pred_4_all]\n","y_test_4_all = np.array(y_test_4_all)\n","y_pred_4_all = np.array(y_pred_4_all)\n","y_test_4_all_confusion = [item for sublist in y_test_4_all for item in sublist]\n","y_pred_4_all_confusion = [item for sublist in y_pred_4_all for item in sublist]\n","\n","# Loop over rows and compute precision recall curve for each row\n","for i in range(len(y_pred_4_all)):\n","    precision, recall, thresholds = precision_recall_curve(y_test_4_all[i], y_pred_4_all[i])\n","    auc = average_precision_score(y_test_4_all[i], y_pred_4_all[i])\n","\n","    # Plot the ROC curve for each row\n","    plt.plot(recall, precision, lw=2, label='PR curve it. %d (AP = %0.2f)' % (i+1, auc))\n","\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('Precision-recall curve for all rows')\n","plt.legend(loc=\"lower right\", fontsize=8)\n","plt.show()"],"metadata":{"id":"Kk5rPzuu0MsN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Pipeline 5: LASSO -> KNN"],"metadata":{"id":"GqGugMuY06P8"}},{"cell_type":"code","source":["# PIPELINE 5\n","\n","# Define outer and inner cross validation\n","outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42) \n","inner_cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n","\n","# Creating ampty arrays\n","f1_design_5_before = []\n","f1_test_5_before = []\n","f1_design_5_after = []\n","f1_test_5_after = []\n","results_5 = pd.DataFrame()\n","best_pipe_5 = []\n","best_hp_5_before = pd.DataFrame()\n","best_hp_5 = pd.DataFrame()\n","y_test_5_all = []\n","y_pred_5_all = []\n","\n","for design_index, test_index in outer_cv.split(X, y): \n","    X_design = X.transpose()[design_index]\n","    X_design = X_design.transpose()\n","    print(f'Size_X_design {X_design.shape}') # print size of X_design\n","    y_design = y[design_index]\n","    \n","    X_test = X.transpose()[test_index]\n","    X_test = X_test.transpose()\n","    y_test_5 = y[test_index].tolist()\n","    y_test_5_all.append(y_test_5)\n","    print(f'Size X_test {X_test.shape}')     # print size of X_test\n","\n","    # remove outliers\n","    X_design = removing_outliers(X_design)\n","    X_test = removing_outliers(X_test)\n","\n","    # Correct missing data\n","    X_design = missing_data(X_design)\n","    X_test = missing_data(X_test)\n","\n","    # balance the classes, so design set consists of 50% normal and 50% abnormal ECG's\n","    ros = RandomOverSampler(sampling_strategy='minority')\n","    X_resampled, y_resampled = ros.fit_resample(X_design, y_design)\n","    X_design = X_resampled\n","    y_design = y_resampled   \n","\n","    print(f'shape after balancing {X_design.shape}')  \n","\n","    # Scaling the data\n","    scaler = RobustScaler()\n","    X_design = scaler.fit_transform(X_design)\n","    X_test = scaler.transform(X_test)\n","\n","    ## PIPELINE 5: RobustScaler --> LASSO --> KNN\n","    # Define pipeline 5\n","    pipeline_5a = Pipeline([\n","        ('lasso', Lasso()),\n","    ])\n","    # Define hyperparameters of pipeline 5\n","    param_grid_5a = {\n","    'lasso__alpha': np.logspace(-10, 1, 100),\n","    }\n","\n","    # Perform randomized search with inner cross-validation to find best alpha\n","    rand_search_5a = RandomizedSearchCV(pipeline_5a, param_distributions=param_grid_5a, n_iter=50, cv=inner_cv, scoring='f1',n_jobs=-1) # optimize parameters\n","    rand_search_5a.fit(X_design, y_design)\n","    \n","    # Create a new Lasso model using the best alpha value\n","    lasso = Lasso(alpha=rand_search_5a.best_params_['lasso__alpha'])\n","    lasso.fit(X_design, y_design)\n","\n","    # Get the coefficients of the Lasso model, find them and define the new X_design with less features\n","    coef = lasso.coef_\n","    selected_features = np.where(coef != 0)[0]\n","    X_design = X_design[:, selected_features]\n","    X_test = X_test[:, selected_features]\n","    print(f'This is the size of X_design after LASSO: {X_design.shape}')\n","\n","    # Access and store the best set of hyperparameters of each outer-CV loop in a DataFrame\n","    best_hp_5_before = best_hp_5_before.append(rand_search_5a.best_params_,ignore_index=True)\n","    print(f'This is the size of X_design after LASSO: {X_design.shape}')\n","\n","    # Define pipeline 5b\n","    pipeline_5b = Pipeline([\n","        ('clf', KNeighborsClassifier())\n","        ])\n","    \n","    # Define scores BEFORE hyperparameter tuning\n","    pipeline_5b.fit(X_design, y_design)\n"," \n","    y_pred_design_5 = pipeline_5b.predict(X_design)\n","    f1_design_5_bef = f1_score(y_design, y_pred_design_5)\n","    f1_design_5_before.append(f1_design_5_bef)\n","\n","    y_pred_test_5_before = pipeline_5b.predict(X_test)\n","    f1_test_5_bef = f1_score(y_test_5, y_pred_test_5_before)\n","    f1_test_5_before.append(f1_test_5_bef)\n","\n","    # Define hyperparameters of pipeline 5b\n","    param_grid_5b = {'clf__n_neighbors': list(range(4,26,2)),\n","                  'clf__p': [1,2],\n","                  'clf__leaf_size': np.arange(1,26,1)\n","                }\n","\n","    # Perform grid search with inner cross-validation, part 1\n","    model_5 = RandomizedSearchCV(pipeline_5b, param_distributions=param_grid_5b, n_iter=50, cv=inner_cv, scoring='f1', n_jobs=-1) # optimize parameters\n","    model_5.fit(X_design, y_design)\n","\n","    # Storing results cross-validation\n","    results = pd.DataFrame(model_5.cv_results_)\n","    results_5 = results_5.append(results,ignore_index=True)\n","\n","    # Define scores AFTER hyperparameter tuning \n","    y_pred_design_5_after = model_5.predict(X_design)\n","    f1_design_5_aft = f1_score(y_design, y_pred_design_5_after)\n","    f1_design_5_after.append(f1_design_5_aft)\n","   \n","    y_pred_test_5_after = model_5.predict(X_test)\n","    y_pred_5_all.append(y_pred_test_5_after)\n","   \n","    f1_test_5_aft = f1_score(y_test_5, y_pred_test_5_after)\n","    f1_test_5_after.append(f1_test_5_aft)\n","\n","    # Access and store the best set of hyperparameters of each outer-CV loop in a DataFrame\n","    best_hp_5 = best_hp_5.append(model_5.best_params_,ignore_index=True)\n","    # Stores the optimum model in best_pipe\n","    best_pipe_5.append(model_5.best_estimator_)\n","\n","# Save results of inner CV into .csv file\n","results_5.to_csv('results_5.csv', index=False)\n","\n","print(f'Mean and std of F1 scores of pipeline 5: {statistics.mean(f1_test_5_after)} +/- {statistics.stdev(f1_test_5_after)}')\n","print(f'The optimal hyperparameters per split: {best_hp_5}')\n","print(f'The best pipes per split {best_pipe_5}')\n","print(f'The design F1 scores before tuning {f1_design_5_before}')\n","print(f'The test F1 scores before tuning {f1_test_5_before}')\n","print(f'The design F1 scores after tuning {f1_design_5_after}')\n","print(f'The test F1 scores afer tuning {f1_test_5_after}')\n","\n","data_5 = [f1_design_5_after, f1_test_5_after]\n","sns.boxplot(data=data_5)\n","plt.title('Boxplot F1 scores design and test pipeline 5')\n","plt.xlabel('design and test')\n","plt.ylabel('F1 score score')\n","plt.show()\n","\n","# Loop over rows and compute precision recall curve for each row\n","for i in range(len(y_pred_5_all)):\n","    precision, recall, thresholds = precision_recall_curve(y_test_5_all[i], y_pred_5_all[i])\n","    auc = average_precision_score(y_test_5_all[i], y_pred_5_all[i])\n","\n","    # Plot the ROC curve for each row\n","    plt.plot(recall, precision, lw=2, label='PR curve it. %d (AP = %0.2f)' % (i+1, auc))\n","\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('Precision-recall curve for all rows')\n","plt.legend(loc=\"lower right\", fontsize=8)\n","plt.show()"],"metadata":{"id":"57-faWGZ0-Hi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Pipeline 6: PCA + Univariate -> Random forest"],"metadata":{"id":"hZ3xrwmn1mdX"}},{"cell_type":"code","source":["# PIPELINE 6\n","# Define outer and inner cross validation\n","outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42) \n","inner_cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n","\n","# Creating empty error\n","f1_design_6_before = []\n","f1_test_6_before = []\n","f1_design_6_after = []\n","f1_test_6_after = []\n","results_6 = pd.DataFrame()\n","best_pipe_6 = []\n","best_hp_6_before = pd.DataFrame()\n","best_hp_6 = pd.DataFrame()\n","y_test_6_all = []\n","y_pred_6_all = []\n","\n","for design_index, test_index in outer_cv.split(X, y): \n","    X_design = X.transpose()[design_index]\n","    X_design = X_design.transpose()\n","    print(f'Size_X_design {X_design.shape}') # print size of X_design\n","    y_design = y[design_index]\n","    \n","    X_test = X.transpose()[test_index]\n","    X_test = X_test.transpose()\n","    y_test_6 = y[test_index].tolist()\n","    y_test_6_all.append(y_test_6)\n","    print(f'Size X_test {X_test.shape}')     # print size of X_test\n","    \n","    # remove outliers\n","    X_design = removing_outliers(X_design)\n","    X_test = removing_outliers(X_test)\n","\n","    # Correct missing data\n","    X_design = missing_data(X_design)\n","    X_test = missing_data(X_test)\n","\n","    # balance the classes, so design set consists of 50% normal and 50% abnormal ECG's\n","    ros = RandomOverSampler(sampling_strategy='minority')\n","    X_resampled, y_resampled = ros.fit_resample(X_design, y_design)\n","    X_design = X_resampled\n","    y_design = y_resampled   \n","\n","    print(f'shape after balancing {X_design.shape}')\n","    \n","    # Define pipeline 6a\n","    pipeline_6a = Pipeline([\n","        ('scaler', RobustScaler()),\n","        ('var_threshold', VarianceThreshold(threshold=0.0)),\n","        ('pca', PCA(n_components=0.5)),\n","    ])\n","\n","    param_grid_6a = {\n","        'pca__n_components': [0.5],#,0.75, 0.9, 0.95, 0.99],\n","        }\n","\n","    # Perform grid search with inner cross-validation, part 1\n","    rand_search_6a = RandomizedSearchCV(pipeline_6a, param_distributions=param_grid_6a, n_iter=10, cv=inner_cv, scoring='f1', n_jobs=-1) #klopt n__iter\n","    rand_search_6a.fit(X_design, y_design)\n","    X_design = rand_search_6a.best_estimator_.transform(X_design)\n","    X_test = rand_search_6a.best_estimator_.transform(X_test)\n","    \n","    # Access and store the best set of hyperparameters of each outer-CV loop in a DataFrame\n","    best_hp_6_before = best_hp_6_before.append(rand_search_6a.best_params_,ignore_index=True)\n","    print(f'shape of X_design after pca {X_design.shape}')\n","\n","    # univariate feature selection\n","    sel_kb = SelectKBest(f_classif, k='all')\n","    sel_kb.fit(X_design, y_design)\n","    p_values = sel_kb.pvalues_\n","\n","    reject_fdr, pvals_fdr, _, _ = multipletests(pvals=p_values, alpha=0.05, method='fdr_bh')\n","    features_selected=np.array(np.where(reject_fdr)[0])\n","    print(f'size of features selected{features_selected.shape}')\n","    X_design = X_design[:,features_selected]\n","    X_test = X_test[:,features_selected]\n","        \n","    print(f'shape of X_design after univariate: {X_design.shape}')\n","    \n","    # pipeline 6b\n","    pipeline_6b = Pipeline([    \n","        ('clf', RandomForestClassifier())\n","    ])\n","\n","    # Define scores BEFORE hyperparameter tuning\n","    pipeline_6b.fit(X_design, y_design)\n"," \n","    y_pred_design_6 = pipeline_6b.predict(X_design)\n","    f1_design_6_bef = f1_score(y_design, y_pred_design_6)\n","    f1_design_6_before.append(f1_design_6_bef)\n","\n","    y_pred_test_6_before = pipeline_6b.predict(X_test)\n","    f1_test_6_bef = f1_score(y_test_6, y_pred_test_6_before)\n","    f1_test_6_before.append(f1_test_6_bef)\n","    \n","    # Define hyperparameters of pipeline 6\n","    param_grid_6b = {'clf__n_estimators' : range(2,6),\n","                    'clf__criterion' :['gini','entropy','log_loss'],\n","                    'clf__min_samples_split':range(2,10),\n","                    'clf__min_samples_leaf':range(1,10),\n","                    'clf__min_weight_fraction_leaf' : np.linspace(0, 0.5, 25),\n","                    'clf__max_features':['sqrt','log2',None],\n","                    'clf__bootstrap':[True,False],\n","                    'clf__warm_start':[True,False]\n","                    }\n","\n","    # Perform grid search with inner cross-validation, part 1\n","    model_6 = RandomizedSearchCV(pipeline_6b, param_distributions=param_grid_6b, cv=inner_cv, scoring='f1', n_iter=50, n_jobs=-1) # optimize parameters\n","    model_6.fit(X_design, y_design)\n","\n","    results = pd.DataFrame(model_6.cv_results_)\n","    results_6 = results_6.append(results,ignore_index=True)\n","\n","    # Define scores AFTER hyperparameter tuning \n","    y_pred_design_6_after = model_6.predict(X_design)\n","    f1_design_6_aft = f1_score(y_design, y_pred_design_6_after)\n","    f1_design_6_after.append(f1_design_6_aft)\n","   \n","    y_pred_test_6_after = model_6.predict(X_test)\n","    y_pred_6_all.append(y_pred_test_6_after)\n","   \n","    f1_test_6_aft = f1_score(y_test_6, y_pred_test_6_after)\n","    f1_test_6_after.append(f1_test_6_aft)\n","\n","    # Access and store the best set of hyperparameters of each outer-CV loop in a DataFrame\n","    best_hp_6 = best_hp_6.append(model_6.best_params_,ignore_index=True)\n","    # Stores the optimum model in best_pipe\n","    best_pipe_6.append(model_6.best_estimator_)\n","\n","# Save results of inner CV into .csv file\n","results_6.to_csv('results_6.csv', index=False)\n","\n","print(f'Mean and std of F1 scores of pipeline 1: {statistics.mean(f1_test_6_after)} +/- {statistics.stdev(f1_test_6_after)}')\n","print(f'The optimal hyperparameters per split: {best_hp_6}')\n","print(f'The best pipes per split {best_pipe_6}')\n","print(f'The design F1 scores before tuning {f1_design_6_before}')\n","print(f'The test F1 scores before tuning {f1_test_6_before}')\n","print(f'The design F1 scores after tuning {f1_design_6_after}')\n","print(f'The test F1 scores afer tuning {f1_test_6_after}')\n","\n","data_6 = [f1_design_6_after, f1_test_6_after]\n","sns.boxplot(data=data_6)\n","plt.title('Boxplot F1 scores design and test pipeline 6')\n","plt.xlabel('design and test')\n","plt.ylabel('F1 score score')\n","plt.show()\n","\n","# Loop over rows and compute precision recall curve for each row\n","for i in range(len(y_pred_6_all)):\n","    precision, recall, thresholds = precision_recall_curve(y_test_6_all[i], y_pred_6_all[i])\n","    auc = average_precision_score(y_test_6_all[i], y_pred_6_all[i])\n","\n","    # Plot the ROC curve for each row\n","    plt.plot(recall, precision, lw=2, label='PR curve it. %d (AP = %0.2f)' % (i+1, auc))\n","\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('Precision-recall curve for all rows')\n","plt.legend(loc=\"lower right\", fontsize=8)\n","plt.show()"],"metadata":{"id":"gz6fTXu91qy9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Pipeline 7: LASSO -> Random forest"],"metadata":{"id":"t5O52Uz12l3A"}},{"cell_type":"code","source":["# # Define outer and inner cross validation\n","# outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42) \n","# inner_cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n","\n","# # Creating empty arrays\n","# f1_design_7_before = []\n","# f1_test_7_before = []\n","# f1_design_7_after = []\n","# f1_test_7_after = []\n","# results_7 = pd.DataFrame()\n","# best_pipe_7 = []\n","# best_hp_7_before = pd.DataFrame()\n","# best_hp_7 = pd.DataFrame()\n","# y_test_7_all = []\n","# y_pred_7_all = []\n","\n","# for design_index, test_index in outer_cv.split(X, y): \n","#     X_design = X.transpose()[design_index]\n","#     X_design = X_design.transpose()\n","#     print(f'Size_X_design {X_design.shape}') # print size of X_design\n","#     y_design = y[design_index]\n","    \n","#     X_test = X.transpose()[test_index]\n","#     X_test = X_test.transpose()\n","#     y_test_7 = y[test_index].tolist()\n","#     y_test_7_all.append(y_test_7)\n","#     print(f'Size X_test {X_test.shape}')     # print size of X_test\n","\n","#     # remove outliers\n","#     X_design = removing_outliers(X_design)\n","#     X_test = removing_outliers(X_test)\n","\n","#     # Correct missing data\n","#     X_design = missing_data(X_design)\n","#     X_test = missing_data(X_test)\n","\n","#     # balance the classes, so design set consists of 50% normal and 50% abnormal ECG's\n","#     ros = RandomOverSampler(sampling_strategy='minority')\n","#     X_resampled, y_resampled = ros.fit_resample(X_design, y_design)\n","#     X_design = X_resampled\n","#     y_design = y_resampled  \n","#     print(f'shape after balancing {X_design.shape}')  \n","\n","#     # Scaling the data\n","#     scaler = RobustScaler()\n","#     X_design = scaler.fit_transform(X_design)\n","#     X_test = scaler.transform(X_test)\n","\n","#     # Define pipeline 7\n","#     pipeline_7a = Pipeline([\n","#         ('lasso', Lasso()),\n","#     ])\n","#     # Define hyperparameters of pipeline 5\n","#     param_grid_7a = {\n","#     'lasso__alpha': np.logspace(-10, 1, 100),\n","#     }\n","\n","#     # Perform randomized search with inner cross-validation to find best alpha\n","#     rand_search_7a = RandomizedSearchCV(pipeline_7a, param_distributions=param_grid_7a, n_iter =50, cv=inner_cv, scoring='f1',n_jobs=-1) # optimize parameters\n","#     rand_search_7a.fit(X_design, y_design)\n","    \n","#     # Create a new Lasso model using the best alpha value\n","#     lasso = Lasso(alpha=rand_search_7a.best_params_['lasso__alpha'])\n","#     lasso.fit(X_design, y_design)\n","\n","#     # Get the coefficients of the Lasso model, find them and define the new X_design with less features\n","#     coef = lasso.coef_\n","#     selected_features = np.where(coef != 0)[0]\n","#     X_design = X_design[:, selected_features]\n","#     X_test = X_test[:, selected_features]\n","#     print(f'This is the size of X_design after LASSO: {X_design.shape}')\n","\n","#     # Access and store the best set of hyperparameters of each outer-CV loop in a DataFrame\n","#     best_hp_7_before = best_hp_7_before.append(rand_search_7a.best_params_,ignore_index=True)\n","#     print(f'This is the size of X_design after LASSO: {X_design.shape}')\n","\n","#     # Define pipeline 7b\n","#     pipeline_7b = Pipeline([\n","#         ('clf', RandomForestClassifier())])\n","    \n","#     # Define scores BEFORE hyperparameter tuning\n","#     pipeline_7b.fit(X_design, y_design)\n"," \n","#     y_pred_design_7 = pipeline_7b.predict(X_design)\n","#     f1_design_7_bef = f1_score(y_design, y_pred_design_7)\n","#     f1_design_7_before.append(f1_design_7_bef)\n","\n","#     y_pred_test_7_before = pipeline_7b.predict(X_test)\n","#     f1_test_7_bef = f1_score(y_test_7, y_pred_test_7_before)\n","#     f1_test_7_before.append(f1_test_7_bef)\n","\n","#     # Define hyperparameters of pipeline 7b\n","#     param_grid_7b = {'clf__n_estimators' : range(2,6),\n","#                                'clf__criterion' :['gini','entropy','log_loss'],\n","#                                'clf__min_samples_split':range(2,10),\n","#                                'clf__min_samples_leaf':range(1,10),\n","#                                'clf__min_weight_fraction_leaf' : np.linspace(0, 0.5, 25),\n","#                                'clf__max_features':['sqrt','log2',None],\n","#                                'clf__bootstrap':[True,False],\n","#                                'clf__warm_start':[True,False],\n","#                                }\n","\n","#     # Perform grid search with inner cross-validation, part 1\n","#     model_7 = RandomizedSearchCV(pipeline_7b, param_distributions=param_grid_7b, n_iter=50, cv=inner_cv, scoring='f1', n_jobs=-1) # optimize parameters\n","#     model_7.fit(X_design, y_design)\n","\n","#     # Storing results cross-validation\n","#     results = pd.DataFrame(model_7.cv_results_)\n","#     results_7 = results_7.append(results,ignore_index=True)\n","\n","#     # Define scores AFTER hyperparameter tuning \n","#     y_pred_design_7_after = model_7.predict(X_design)\n","#     f1_design_7_aft = f1_score(y_design, y_pred_design_7_after)\n","#     f1_design_7_after.append(f1_design_7_aft)\n","   \n","#     y_pred_test_7_after = model_7.predict(X_test)\n","#     y_pred_7_all.append(y_pred_test_7_after)\n","   \n","#     f1_test_7_aft = f1_score(y_test_7, y_pred_test_7_after)\n","#     f1_test_7_after.append(f1_test_7_aft)\n","\n","#     # Access and store the best set of hyperparameters of each outer-CV loop in a DataFrame\n","#     best_hp_7 = best_hp_7.append(model_7.best_params_,ignore_index=True)\n","#     # Stores the optimum model in best_pipe\n","#     best_pipe_7.append(model_7.best_estimator_)\n","\n","# # Save results of inner CV into .csv file\n","# results_7.to_csv('results_7.csv', index=False)\n","\n","# print(f'Mean and std of F1 scores of pipeline 7: {statistics.mean(f1_test_7_after)} +/- {statistics.stdev(f1_test_7_after)}')\n","# print(f'The optimal hyperparameters per split: {best_hp_7}')\n","# print(f'The best pipes per split {best_pipe_7}')\n","# print(f'The design F1 scores before tuning {f1_design_7_before}')\n","# print(f'The test F1 scores before tuning {f1_test_7_before}')\n","# print(f'The design F1 scores after tuning {f1_design_7_after}')\n","# print(f'The test F1 scores afer tuning {f1_test_7_after}')\n","\n","# data_7 = [f1_design_7_after, f1_test_7_after]\n","# sns.boxplot(data=data_7)\n","# plt.title('Boxplot F1 scores design and test pipeline 7')\n","# plt.xlabel('design and test')\n","# plt.ylabel('F1 score score')\n","# plt.show()\n","\n","# # Loop over rows and compute precision recall curve for each row\n","# for i in range(len(y_pred_7_all)):\n","#     precision, recall, thresholds = precision_recall_curve(y_test_7_all[i], y_pred_7_all[i])\n","#     auc = average_precision_score(y_test_7_all[i], y_pred_7_all[i])\n","\n","#     # Plot the ROC curve for each row\n","#     plt.plot(recall, precision, lw=2, label='PR curve it. %d (AP = %0.2f)' % (i+1, auc))\n","\n","# plt.xlim([0.0, 1.0])\n","# plt.ylim([0.0, 1.05])\n","# plt.xlabel('Recall')\n","# plt.ylabel('Precision')\n","# plt.title('Precision-recall curve for all rows')\n","# plt.legend(loc=\"lower right\", fontsize=8)\n","# plt.show()"],"metadata":{"id":"jEhZPRmg2n3y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Confusion matrices and classification report"],"metadata":{"id":"4lTgYkJgygiz"}},{"cell_type":"code","source":["# Confusion matrices\n","# Pipeline 1\n","y_pred_1_all_con = [arr.tolist() for arr in y_pred_1_all]\n","y_test_1_all_con = np.array(y_test_1_all)\n","y_pred_1_all_con = np.array(y_pred_1_all_con)\n","y_test_1_all_confusion = [item for sublist in y_test_1_all_con for item in sublist]\n","y_pred_1_all_confusion = [item for sublist in y_pred_1_all_con for item in sublist]\n","\n","cm_1 = confusion_matrix(y_test_1_all_confusion, y_pred_1_all_confusion)\n","\n","sns.heatmap(cm_1, annot=True, cmap='Blues')\n","plt.xlabel('Predicted labels')\n","plt.ylabel('True labels')\n","plt.title('Confusion matrix Pipeline 1')\n","plt.show()\n","\n","# Pipeline 2\n","y_pred_2_all_con = [arr.tolist() for arr in y_pred_2_all]\n","y_test_2_all_con = np.array(y_test_2_all)\n","y_pred_2_all_con = np.array(y_pred_2_all_con)\n","y_test_2_all_confusion = [item for sublist in y_test_2_all_con for item in sublist]\n","y_pred_2_all_confusion = [item for sublist in y_pred_2_all_con for item in sublist]\n","\n","cm_2 = confusion_matrix(y_test_2_all_confusion, y_pred_2_all_confusion)\n","\n","sns.heatmap(cm_2, annot=True, cmap='Blues')\n","plt.xlabel('Predicted labels')\n","plt.ylabel('True labels')\n","plt.title('Confusion matrix Pipeline 2')\n","plt.show()\n","\n","# Pipeline 3\n","y_pred_3_all_con = [arr.tolist() for arr in y_pred_3_all]\n","y_test_3_all_con = np.array(y_test_3_all)\n","y_pred_3_all_con = np.array(y_pred_3_all_con)\n","y_test_3_all_confusion = [item for sublist in y_test_3_all_con for item in sublist]\n","y_pred_3_all_confusion = [item for sublist in y_pred_3_all_con for item in sublist]\n","\n","cm_3 = confusion_matrix(y_test_3_all_confusion, y_pred_3_all_confusion)\n","\n","sns.heatmap(cm_3, annot=True, cmap='Blues')\n","plt.xlabel('Predicted labels')\n","plt.ylabel('True labels')\n","plt.title('Confusion matrix Pipeline 3')\n","plt.show()\n","\n","# Pipeline 4\n","y_pred_4_all_con = [arr.tolist() for arr in y_pred_4_all]\n","y_test_4_all_con = np.array(y_test_4_all)\n","y_pred_4_all_con = np.array(y_pred_4_all_con)\n","y_test_4_all_confusion = [item for sublist in y_test_4_all_con for item in sublist]\n","y_pred_4_all_confusion = [item for sublist in y_pred_4_all_con for item in sublist]\n","\n","cm_4 = confusion_matrix(y_test_4_all_confusion, y_pred_4_all_confusion)\n","\n","sns.heatmap(cm_4, annot=True, cmap='Blues')\n","plt.xlabel('Predicted labels')\n","plt.ylabel('True labels')\n","plt.title('Confusion matrix Pipeline 4')\n","plt.show()\n","\n","# Pipeline 5\n","y_pred_5_all_con = [arr.tolist() for arr in y_pred_5_all]\n","y_test_5_all_con = np.array(y_test_5_all)\n","y_pred_5_all_con = np.array(y_pred_5_all_con)\n","y_test_5_all_confusion = [item for sublist in y_test_5_all_con for item in sublist]\n","y_pred_5_all_confusion = [item for sublist in y_pred_5_all_con for item in sublist]\n","\n","cm_5 = confusion_matrix(y_test_5_all_confusion, y_pred_5_all_confusion)\n","\n","sns.heatmap(cm_5, annot=True, cmap='Blues')\n","plt.xlabel('Predicted labels')\n","plt.ylabel('True labels')\n","plt.title('Confusion matrix Pipeline 5')\n","plt.show()\n","\n","# Pipeline 6\n","y_pred_6_all_con = [arr.tolist() for arr in y_pred_6_all]\n","y_test_6_all_con = np.array(y_test_6_all)\n","y_pred_6_all_con = np.array(y_pred_6_all_con)\n","y_test_6_all_confusion = [item for sublist in y_test_6_all_con for item in sublist]\n","y_pred_6_all_confusion = [item for sublist in y_pred_6_all_con for item in sublist]\n","\n","cm_6 = confusion_matrix(y_test_6_all_confusion, y_pred_6_all_confusion)\n","\n","sns.heatmap(cm_6, annot=True, cmap='Blues')\n","plt.xlabel('Predicted labels')\n","plt.ylabel('True labels')\n","plt.title('Confusion matrix Pipeline 6')\n","plt.show()\n","\n","# # Pipeline 7\n","# y_pred_7_all_con = [arr.tolist() for arr in y_pred_7_all]\n","# y_test_7_all_con = np.array(y_test_7_all)\n","# y_pred_7_all_con = np.array(y_pred_7_all_con)\n","# y_test_7_all_confusion = [item for sublist in y_test_7_all_con for item in sublist]\n","# y_pred_7_all_confusion = [item for sublist in y_pred_7_all_con for item in sublist]\n","\n","# cm_7 = confusion_matrix(y_test_7_all_confusion, y_pred_7_all_confusion)\n","\n","# sns.heatmap(cm_7, annot=True, cmap='Blues')\n","# plt.xlabel('Predicted labels')\n","# plt.ylabel('True labels')\n","# plt.title('Confusion matrix Pipeline 7')\n","# plt.show()\n","\n","# Classification reports\n","print(classification_report(y_test_1_all_confusion, y_pred_1_all_confusion))\n","print(classification_report(y_test_2_all_confusion, y_pred_2_all_confusion))\n","print(classification_report(y_test_3_all_confusion, y_pred_3_all_confusion))\n","print(classification_report(y_test_4_all_confusion, y_pred_4_all_confusion))\n","print(classification_report(y_test_5_all_confusion, y_pred_5_all_confusion))\n","print(classification_report(y_test_6_all_confusion, y_pred_6_all_confusion))\n","# print(classification_report(y_test_7_all_confusion, y_pred_7_all_confusion))"],"metadata":{"id":"ZhsH4Zllyrk_"},"execution_count":null,"outputs":[]}]}