{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TM10007 Assignment template -- ECG data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in c:\\users\\irisv\\miniconda3\\lib\\site-packages (0.0.post1)\n",
      "Requirement already satisfied: numpy in c:\\users\\irisv\\miniconda3\\lib\\site-packages (1.23.5)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\irisv\\miniconda3\\lib\\site-packages (3.6.2)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\irisv\\miniconda3\\lib\\site-packages (0.10.1)\n",
      "Requirement already satisfied: statsmodels in c:\\users\\irisv\\miniconda3\\lib\\site-packages (0.13.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\irisv\\miniconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\irisv\\miniconda3\\lib\\site-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\irisv\\miniconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\irisv\\miniconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\irisv\\miniconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\irisv\\miniconda3\\lib\\site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\irisv\\miniconda3\\lib\\site-packages (from matplotlib) (22.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\irisv\\miniconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\irisv\\miniconda3\\lib\\site-packages (from imbalanced-learn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\irisv\\miniconda3\\lib\\site-packages (from imbalanced-learn) (1.9.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\irisv\\miniconda3\\lib\\site-packages (from imbalanced-learn) (1.1.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\irisv\\miniconda3\\lib\\site-packages (from imbalanced-learn) (1.2.1)\n",
      "Requirement already satisfied: patsy>=0.5.2 in c:\\users\\irisv\\miniconda3\\lib\\site-packages (from statsmodels) (0.5.3)\n",
      "Requirement already satisfied: pandas>=0.25 in c:\\users\\irisv\\miniconda3\\lib\\site-packages (from statsmodels) (1.5.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\irisv\\miniconda3\\lib\\site-packages (from pandas>=0.25->statsmodels) (2022.7)\n",
      "Requirement already satisfied: six in c:\\users\\irisv\\miniconda3\\lib\\site-packages (from patsy>=0.5.2->statsmodels) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "! pip install sklearn numpy matplotlib imbalanced-learn statsmodels\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels\n",
    "import seaborn\n",
    "import warnings\n",
    "import statistics\n",
    "\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from scipy.stats import shapiro, lognorm, randint\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, learning_curve, GridSearchCV, StratifiedKFold, cross_val_score, KFold, train_test_split, RandomizedSearchCV, validation_curve #, multipletests\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn import datasets as ds, model_selection, metrics, neighbors\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif, SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "# Required and still not used:\n",
    "#import torch\n",
    "#import seaborn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd() # This fn will return the Current Working Directory\n",
    "\n",
    "zip_path = os.path.join(cwd, 'ecg', 'ecg_data.zip')\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(os.path.join(cwd, 'ecg'))\n",
    "\n",
    "data_path = os.path.join(cwd, 'ecg', 'ecg_data.csv')\n",
    "data = pd.read_csv(data_path, index_col=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 300)\n",
      "(50,)\n",
      "9 people have an abnormal ECG\n",
      "41 people have a normal ECG\n",
      "The percentage of abnormal ECGs in this dataset is 18.0 %\n"
     ]
    }
   ],
   "source": [
    "# split labels from data\n",
    "X = data.loc[:, data.columns != 'label']  #alles behalve label\n",
    "y = data['label']  # labels\n",
    "\n",
    "X = X.iloc[:50, :300]\n",
    "y = y.iloc[:50]\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# normal / abnormal ECGs\n",
    "total_abnormal_ECG = np.count_nonzero(y) \n",
    "total_normal_ECG = y.size -np.count_nonzero(y) \n",
    "percentage_abnormal = total_abnormal_ECG / (total_abnormal_ECG + total_normal_ECG)*100\n",
    "\n",
    "print(f'{total_abnormal_ECG} people have an abnormal ECG')\n",
    "print(f'{total_normal_ECG} people have a normal ECG')\n",
    "print(f'The percentage of abnormal ECGs in this dataset is {percentage_abnormal} %')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing data\n",
    "- Removing features if there is lot of data missing (replace all for a value)\n",
    "- Removing samples (in this case patients) if there is a lot of data missing\n",
    "- Imputation for generating data to fill us missing values -> median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing data\n",
    "X = X.replace(0, np.nan)  # make all zeros to NaN\n",
    "nan_count = X.isna().sum().sum()  # count missing data -> 10500 in our dataset\n",
    "\n",
    "# Delete missing data when > --% of feature of sample is missing\n",
    "X = X.dropna(axis='columns', how='all') # deletes a feature if all values of a column (so feature) are empty\n",
    "X = X.dropna(axis='rows', how='all') # deletes a patient if all values of a row (so sample) are empty\n",
    "\n",
    "# Missing data to median per feature\n",
    "for column in X.columns:\n",
    "    X[column].fillna(X[column].median(), inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for normal distribution - before outliers and missing data correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median p-value 7.19496347301174e-05 Mean p-value 0.0024352858290631737\n"
     ]
    }
   ],
   "source": [
    "# Normally distributed\n",
    "stat = []\n",
    "p = []\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'float64' or X[col].dtype == 'int64':\n",
    "        s, pv = shapiro(X[col])\n",
    "        stat.append(s)\n",
    "        p.append(pv)\n",
    "    else:\n",
    "        stat.append(None)\n",
    "        p.append(None)\n",
    "print('Median p-value', statistics.median(p), 'Mean p-value', statistics.mean(p))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "- Detect outliers using Z-score since data is not nornally distributed\n",
    "- Replace outliers by the median of that feature\n",
    "- Print -> check wether the outliers are changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of outliers in dataset x is 344\n"
     ]
    }
   ],
   "source": [
    "# supress performance warning\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# Outliers: Tukey's fence \n",
    "k=3\n",
    "fences=pd.DataFrame()\n",
    "outliers = pd.DataFrame(False, index=X.index, columns=X.columns) # create an empty DataFrame for outliers\n",
    "\n",
    "for col in X.columns:\n",
    "    q1, q3 = np.percentile(X[col], [25, 75])\n",
    "    iqr = q3 - q1\n",
    "    lower_fence = q1 - k*iqr\n",
    "    upper_fence = q3 + k*iqr\n",
    "    fences[col]=[lower_fence, upper_fence]\n",
    "    for row in X.index:\n",
    "        if X.loc[row, col] < lower_fence or X.loc[row, col] > upper_fence:\n",
    "            outliers.loc[row, col] = True # mark the place as an outlier\n",
    "\n",
    "row_count = (outliers == True).sum(axis=1)\n",
    "col_count = (outliers == True).sum(axis=0)\n",
    "total_count = row_count.sum() + col_count.sum()\n",
    "print(f'The total number of outliers in dataset x is {total_count}')\n",
    "\n",
    "# create a copy of x to modify\n",
    "new_x = X.copy()\n",
    "\n",
    "#replace outliers with maximum or minimun interquartile range of x by column\n",
    "for col in outliers.columns:\n",
    "    q3 = X.loc[outliers[col] == False, col].quantile(0.75) # 3rd quartile of column where outlier is False\n",
    "    q1 = X.loc[outliers[col] == False, col].quantile(0.25) # 1st quartile of column where outlier is False\n",
    "    iqr = q3 - q1 # interquartile range of column where outlier is False\n",
    "    lower_fence = q1 - k*iqr\n",
    "    upper_fence = q3 + k*iqr\n",
    "    new_x.loc[outliers[col] & (new_x[col] > upper_fence), col] = upper_fence # replace outliers with upper fence\n",
    "    new_x.loc[outliers[col] & (new_x[col] < lower_fence), col] = lower_fence # replace outliers with lower fence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing again for normal distribution - after outliers and missing data correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median p-value after preprocessing 0.00010027730604633689 Mean p-value after preprocessing 0.0024604351888330486\n"
     ]
    }
   ],
   "source": [
    "# Normally distributed\n",
    "stat = []\n",
    "p = []\n",
    "for col in new_x.columns:\n",
    "    if new_x[col].dtype == 'float64' or new_x[col].dtype == 'int64':\n",
    "        s, pv = shapiro(new_x[col])\n",
    "        stat.append(s)\n",
    "        p.append(pv)\n",
    "    else:\n",
    "        stat.append(None)\n",
    "        p.append(None)\n",
    "print('Median p-value after preprocessing', statistics.median(p), 'Mean p-value after preprocessing', statistics.mean(p))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIPELINE 1\n",
    "- RobustScaler --> PCA + univariate --> Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\irisv\\AppData\\Local\\Temp\\ipykernel_17308\\995433304.py:49: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_hp_1 = best_hp_1.append(rand_search.best_params_,ignore_index=True)\n",
      "C:\\Users\\irisv\\AppData\\Local\\Temp\\ipykernel_17308\\995433304.py:79: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_hp_1 = best_hp_1.append(rand_search.best_params_,ignore_index=True)\n",
      "C:\\Users\\irisv\\AppData\\Local\\Temp\\ipykernel_17308\\995433304.py:49: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_hp_1 = best_hp_1.append(rand_search.best_params_,ignore_index=True)\n",
      "C:\\Users\\irisv\\AppData\\Local\\Temp\\ipykernel_17308\\995433304.py:79: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_hp_1 = best_hp_1.append(rand_search.best_params_,ignore_index=True)\n",
      "C:\\Users\\irisv\\AppData\\Local\\Temp\\ipykernel_17308\\995433304.py:49: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_hp_1 = best_hp_1.append(rand_search.best_params_,ignore_index=True)\n",
      "C:\\Users\\irisv\\AppData\\Local\\Temp\\ipykernel_17308\\995433304.py:79: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_hp_1 = best_hp_1.append(rand_search.best_params_,ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The  optimal hyperparameters per split:\n",
      "The optimal hyperparameters per split:   univar_feat_sel__k  pca__n_components  clf__var_smoothing\n",
      "0                all               0.99        2.310130e-01\n",
      "1                all               0.99        2.310130e-01\n",
      "2                all               0.99        1.232847e-06\n",
      "3                all               0.99        1.232847e-06\n",
      "4                all               0.99        1.519911e-06\n",
      "5                all               0.99        1.519911e-06\n",
      "6                all               0.99        4.328761e-07\n",
      "7                all               0.99        4.328761e-07\n",
      "The training AUCs of the nested cv [0.9677777777777778, 0.9906347554630593, 0.9719042663891779, 0.918834547346514]\n",
      "The test AUCs of the nested cv [0.5, 0.6666666666666666, 0.75, 0.7000000000000001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\irisv\\AppData\\Local\\Temp\\ipykernel_17308\\995433304.py:49: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_hp_1 = best_hp_1.append(rand_search.best_params_,ignore_index=True)\n",
      "C:\\Users\\irisv\\AppData\\Local\\Temp\\ipykernel_17308\\995433304.py:79: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_hp_1 = best_hp_1.append(rand_search.best_params_,ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# PIPELINE 1\n",
    "# Define outer and inner cross validation\n",
    "outer_cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42) # to do: in discussie zetten waarom loopen nog beter zou zijn voor generaliseren\n",
    "inner_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42) \n",
    "\n",
    "best_auc_train_1 = []\n",
    "best_auc_test_1 = []\n",
    "best_hp_1 = pd.DataFrame()\n",
    "\n",
    "for train_index, test_index in outer_cv.split(X, y): \n",
    "    X_train = X.transpose()[train_index]\n",
    "    X_train = X_train.transpose()\n",
    "    # print(X_train.shape) # print size of X_train\n",
    "    y_train = y[train_index]\n",
    "    \n",
    "    X_test = X.transpose()[test_index]\n",
    "    X_test = X_test.transpose()\n",
    "    y_test_1 = y[test_index]\n",
    "    # print(X_test.shape)     # print size of X_test\n",
    "\n",
    "    # balance the classes, so training set consists of 50% normal and 50% abnormal ECG's\n",
    "    ros = RandomOverSampler(sampling_strategy='minority')\n",
    "    X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "    X_train = X_resampled\n",
    "    y_train = y_resampled   \n",
    "\n",
    "    ## PIPELINE 1: RobustScaler --> PCA + univariate --> Gaussian Naive Bayes\n",
    "    # Define pipeline 1\n",
    "    pipeline_1 = Pipeline([\n",
    "        ('scaler', RobustScaler()),\n",
    "        ('var_threshold', VarianceThreshold(threshold=0.0)),\n",
    "        ('pca', PCA()),\n",
    "        ('univar_feat_sel', SelectKBest(f_classif)),\n",
    "        ('clf', GaussianNB())\n",
    "    ])\n",
    "    # Define hyperparameters of pipeline 3\n",
    "    param_grid = {\n",
    "    'pca__n_components': [0.5,0.75, 0.9, 0.95, 0.99],\n",
    "    'univar_feat_sel__k': ['all'],\n",
    "    'clf__var_smoothing': np.logspace(0,-9, num=100),\n",
    "    }\n",
    "\n",
    "    # Perform grid search with inner cross-validation, part 1\n",
    "    rand_search = RandomizedSearchCV(pipeline_1, param_distributions=param_grid, n_iter=50, cv=inner_cv, scoring='roc_auc', n_jobs=-1) #klopt n__iter\n",
    "    rand_search.fit(X_train, y_train)\n",
    "\n",
    "    # Storing the best hyperparameters of each outer-CV loop in a DataFrame\n",
    "    # best_hp_1 = pd.DataFrame(columns=param_grid.keys())\n",
    "    best_hp_1 = best_hp_1.append(rand_search.best_params_,ignore_index=True)\n",
    "    \n",
    "    # Print the best hyperparameters and score\n",
    "    # print('Best hyperparameters after randomized search:', rand_search.best_params_)\n",
    "    # print('Best score after randomized search:', rand_search.best_score_)\n",
    "\n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components = rand_search.best_params_['pca__n_components'])\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    # print(f'Size of X_Train after PCA {X_train.shape}')\n",
    "\n",
    "    # Get the selected feature indices from the univariate feature selection\n",
    "    sel_kb = SelectKBest(f_classif, k='all')\n",
    "    sel_kb.fit(X_train, y_train)\n",
    "    p_values = sel_kb.pvalues_\n",
    "\n",
    "    reject_fdr, pvals_fdr, _, _ = multipletests(pvals=p_values, alpha=0.05, method='fdr_bh')\n",
    "    features_selected=np.array(np.where(reject_fdr)[0])\n",
    "    # print(f'Size of selected features {features_selected.shape}')\n",
    "\n",
    "    # Apply univariate feature selection on X_train\n",
    "    # print(f'This is the size of X_train before univariate: {X_train.shape}')\n",
    "    X_train = X_train[:, features_selected]\n",
    "    # print(f'This is the size of X_train after univariate: {X_train.shape}')\n",
    "\n",
    "    # Train the classifier on the selected features with the best hyperparameters to create best trained classifier\n",
    "    clf_1 = GaussianNB(var_smoothing=rand_search.best_params_['clf__var_smoothing'])\n",
    "    clf_1.fit(X_train, y_train)\n",
    "\n",
    "    # Storing the best hyperparameters of each outer-CV loop in a DataFrame\n",
    "    best_hp_1 = best_hp_1.append(rand_search.best_params_,ignore_index=True)\n",
    "\n",
    "    y_pred_1 = clf_1.predict(X_train)\n",
    "\n",
    "    if hasattr(clf_1, 'predict_proba'):\n",
    "    # The first column gives the probability for class = 0, so we take\n",
    "    # the second which gives the probability class = 1:\n",
    "        y_score = clf_1.predict_proba(X_train)[:, 1]\n",
    "    else:\n",
    "        y_score = y_pred_1\n",
    "\n",
    "    auc_train_1 = metrics.roc_auc_score(y_train, y_score)\n",
    "    # Storing the AUC train values of each outer CV loop\n",
    "    best_auc_train_1.append(auc_train_1)\n",
    "    \n",
    "    # print(f'The auc of the training data is {auc_train_1}')\n",
    "    # cv results\n",
    "    #pd.DataFrame(grid_nb.cv_results_)\n",
    "\n",
    "    # Evaluate the classifier on the test data\n",
    "    X_test = pca.transform(X_test)  # Apply the PCA transformation to the test data\n",
    "    X_test = X_test[:, features_selected]  # Apply the feature selection to the PCA-transformed test data\n",
    "    y_pred_1 = clf_1.predict(X_test)\n",
    "    auc_test_1 = metrics.roc_auc_score(y_test_1, y_pred_1)\n",
    "    \n",
    "    best_auc_test_1.append(auc_test_1)\n",
    "     # print(f'The auc of the test data is {auc_test_1}')\n",
    "\n",
    "print(f'The  optimal hyperparameters per split:')\n",
    "best_hp_1\n",
    "print(f'The optimal hyperparameters per split: {best_hp_1}')\n",
    "print(f'The training AUCs of the nested cv {best_auc_train_1}')\n",
    "print(f'The test AUCs of the nested cv {best_auc_test_1}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIPELINE 2\n",
    "- PCA-Uni --> Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:305: UserWarning: The total space of parameters 5 is smaller than n_iter=50. Running 5 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\irisv\\AppData\\Local\\Temp\\ipykernel_17308\\2124358663.py:50: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_hp_2 = best_hp_2.append(rand_search.best_params_,ignore_index=True)\n",
      "c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:305: UserWarning: The total space of parameters 5 is smaller than n_iter=50. Running 5 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\irisv\\AppData\\Local\\Temp\\ipykernel_17308\\2124358663.py:50: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_hp_2 = best_hp_2.append(rand_search.best_params_,ignore_index=True)\n",
      "c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:305: UserWarning: The total space of parameters 5 is smaller than n_iter=50. Running 5 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\irisv\\AppData\\Local\\Temp\\ipykernel_17308\\2124358663.py:50: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_hp_2 = best_hp_2.append(rand_search.best_params_,ignore_index=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 feature(s) (shape=(62, 0)) while a minimum of 1 is required by QuadraticDiscriminantAnalysis.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 77\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[39m# print(f'This is the size of X_train after univariate: {X_train.shape}')\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \n\u001b[0;32m     75\u001b[0m \u001b[39m# Train the classifier on the selected features with the best hyperparameters to create best trained classifier\u001b[39;00m\n\u001b[0;32m     76\u001b[0m clf_2 \u001b[39m=\u001b[39m QuadraticDiscriminantAnalysis()\n\u001b[1;32m---> 77\u001b[0m clf_2\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     79\u001b[0m y_pred_2 \u001b[39m=\u001b[39m clf_2\u001b[39m.\u001b[39mpredict(X_train)\n\u001b[0;32m     81\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(clf_2, \u001b[39m'\u001b[39m\u001b[39mpredict_proba\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     82\u001b[0m \u001b[39m# The first column gives the probability for class = 0, so we take\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[39m# the second which gives the probability class = 1:\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:890\u001b[0m, in \u001b[0;36mQuadraticDiscriminantAnalysis.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit the model according to the given training data and parameters.\u001b[39;00m\n\u001b[0;32m    867\u001b[0m \n\u001b[0;32m    868\u001b[0m \u001b[39m    .. versionchanged:: 0.19\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[39m    Fitted estimator.\u001b[39;00m\n\u001b[0;32m    888\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m--> 890\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, y)\n\u001b[0;32m    891\u001b[0m check_classification_targets(y)\n\u001b[0;32m    892\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_, y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(y, return_inverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    563\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    564\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 565\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    566\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    568\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1106\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1101\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1102\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1103\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[1;32m-> 1106\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1107\u001b[0m     X,\n\u001b[0;32m   1108\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[0;32m   1109\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[0;32m   1110\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   1111\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[0;32m   1112\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m   1113\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m   1114\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[0;32m   1115\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[0;32m   1116\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[0;32m   1117\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[0;32m   1118\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m   1119\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1120\u001b[0m )\n\u001b[0;32m   1122\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[0;32m   1124\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:940\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    938\u001b[0m     n_features \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m    939\u001b[0m     \u001b[39mif\u001b[39;00m n_features \u001b[39m<\u001b[39m ensure_min_features:\n\u001b[1;32m--> 940\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    941\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m feature(s) (shape=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) while\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    942\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m a minimum of \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m is required\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    943\u001b[0m             \u001b[39m%\u001b[39m (n_features, array\u001b[39m.\u001b[39mshape, ensure_min_features, context)\n\u001b[0;32m    944\u001b[0m         )\n\u001b[0;32m    946\u001b[0m \u001b[39mif\u001b[39;00m copy:\n\u001b[0;32m    947\u001b[0m     \u001b[39mif\u001b[39;00m xp\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnumpy.array_api\u001b[39m\u001b[39m\"\u001b[39m}:\n\u001b[0;32m    948\u001b[0m         \u001b[39m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 feature(s) (shape=(62, 0)) while a minimum of 1 is required by QuadraticDiscriminantAnalysis."
     ]
    }
   ],
   "source": [
    "# PIPELINE 2\n",
    "# Define outer and inner cross validation\n",
    "outer_cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42) # to do: in discussie zetten waarom loopen nog beter zou zijn voor generaliseren\n",
    "inner_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "best_auc_train_2 = []\n",
    "best_auc_test_2= []\n",
    "best_hp_2 = pd.DataFrame()\n",
    "\n",
    "for train_index, test_index in outer_cv.split(X, y): \n",
    "    X_train = X.transpose()[train_index]\n",
    "    X_train = X_train.transpose()\n",
    "    # print(X_train.shape)\n",
    "    # print size of X_train\n",
    "    y_train = y[train_index]\n",
    "    \n",
    "    X_test = X.transpose()[test_index]\n",
    "    X_test = X_test.transpose()\n",
    "    y_test_2 = y[test_index]\n",
    "    # print size of X_test\n",
    "    # print(X_test.shape)\n",
    "\n",
    "    # balance the classes, so training set consists of 50% normal and 50% abnormal ECG's\n",
    "    ros = RandomOverSampler(sampling_strategy='minority')\n",
    "    X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "    X_train = X_resampled\n",
    "    # print(f'Size of X_train after resampling {X_train.shape}')\n",
    "    y_train = y_resampled   \n",
    "\n",
    "    ## PIPELINE 2: RobustScaler --> PCA + univariate --> Quadratic Discriminant\n",
    "    # Define pipeline 2\n",
    "    pipeline_2 = Pipeline([\n",
    "        ('scaler', RobustScaler()),\n",
    "        ('var_threshold', VarianceThreshold(threshold=0.0)),\n",
    "        ('pca', PCA()),\n",
    "        ('univar_feat_sel', SelectKBest(f_classif)),\n",
    "        ('clf', QuadraticDiscriminantAnalysis())\n",
    "    ])\n",
    "    # Define hyperparameters of pipeline 3\n",
    "    param_grid = {\n",
    "    'pca__n_components': [0.5,0.75,0.9,0.95,0.99],\n",
    "    'univar_feat_sel__k': ['all']\n",
    "    }\n",
    "\n",
    "    # Perform grid search with inner cross-validation, part 1\n",
    "    rand_search = RandomizedSearchCV(pipeline_2, param_distributions=param_grid,n_iter = 50, cv=inner_cv, scoring='roc_auc', n_jobs=-1) # optimize parameters\n",
    "    rand_search.fit(X_train, y_train)\n",
    "\n",
    "    # Storing the best hyperparameters of each outer-CV loop in a DataFrame\n",
    "    best_hp_2 = best_hp_2.append(rand_search.best_params_,ignore_index=True)\n",
    "\n",
    "    # Print the best hyperparameters and score\n",
    "    # print('Best hyperparameters after first grid search:', rand_search.best_params_)\n",
    "    # print('Best score after first grid search:', rand_search.best_score_)\n",
    "\n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components = rand_search.best_params_['pca__n_components'])\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    # print(f'Size of X_train after PCA {X_train.shape}')\n",
    "\n",
    "    # Get the selected feature indices from the univariate feature selection\n",
    "    sel_kb = SelectKBest(f_classif, k='all')\n",
    "    sel_kb.fit(X_train, y_train)\n",
    "    p_values = sel_kb.pvalues_\n",
    "\n",
    "    reject_fdr, pvals_fdr, _, _ = multipletests(pvals=p_values, alpha=0.05, method='fdr_bh')\n",
    "    features_selected=np.array(np.where(reject_fdr)[0])\n",
    "    # print(f'Size of selected features {features_selected.shape}')\n",
    "    \n",
    "    # Apply univariate feature selection on X_train\n",
    "    # print(f'This is the size of X_train before univariate: {X_train.shape}')\n",
    "    X_train = X_train[:, features_selected]\n",
    "    # print(f'This is the size of X_train after univariate: {X_train.shape}')\n",
    "\n",
    "    # Train the classifier on the selected features with the best hyperparameters to create best trained classifier\n",
    "    clf_2 = QuadraticDiscriminantAnalysis()\n",
    "    clf_2.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_2 = clf_2.predict(X_train)\n",
    "\n",
    "    if hasattr(clf_2, 'predict_proba'):\n",
    "    # The first column gives the probability for class = 0, so we take\n",
    "    # the second which gives the probability class = 1:\n",
    "        y_score = clf_2.predict_proba(X_train)[:, 1]\n",
    "    else:\n",
    "        y_score = y_pred_2\n",
    "\n",
    "    auc_train_2 = metrics.roc_auc_score(y_train, y_score)\n",
    "    # Storing the AUC train values of each outer CV loop\n",
    "    best_auc_train_2.append(auc_train_2)\n",
    "\n",
    "    # cv results\n",
    "    #pd.DataFrame(grid_nb.cv_results_)\n",
    "\n",
    "    # Evaluate the classifier on the test data\n",
    "    X_test = pca.transform(X_test)  # Apply the PCA transformation to the test data\n",
    "    X_test = X_test[:, features_selected]  # Apply the feature selection to the PCA-transformed test data\n",
    "    y_pred_2 = clf_2.predict(X_test)\n",
    "    auc_test_2 = metrics.roc_auc_score(y_test_2, y_pred_2)\n",
    "    \n",
    "    # Storing the AUC test values of each outer CV loop\n",
    "    best_auc_test_2.append(auc_test_2)\n",
    "\n",
    "print(f'The  optimal hyperparameters per split:')\n",
    "best_hp_2\n",
    "print(f'The optimal hyperparameters per split: {best_hp_2}')\n",
    "print(f'The training AUcs of the nested cv {best_auc_train_2}')\n",
    "print(f'The test AUCs of the nested cv {best_auc_test_2}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIPELINE 3\n",
    "- RobustScaler --> PCA + univariate --> SVM_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\irisv\\AppData\\Local\\Temp\\ipykernel_17308\\2009789894.py:50: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_hp_3 = best_hp_3.append(rand_search.best_params_,ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\irisv\\AppData\\Local\\Temp\\ipykernel_17308\\2009789894.py:50: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_hp_3 = best_hp_3.append(rand_search.best_params_,ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\irisv\\AppData\\Local\\Temp\\ipykernel_17308\\2009789894.py:50: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_hp_3 = best_hp_3.append(rand_search.best_params_,ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\irisv\\AppData\\Local\\Temp\\ipykernel_17308\\2009789894.py:50: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_hp_3 = best_hp_3.append(rand_search.best_params_,ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The  optimal hyperparameters per split   univar_feat_sel__k  pca__n_components clf__kernel    clf__C\n",
      "0                all               0.95      linear  0.006952\n",
      "1                all               0.95      linear  0.078476\n",
      "2                all               0.90      linear  2.335721\n",
      "3                all               0.95      linear  6.158482\n",
      "The training AUcs of the nested cv [0.8666666666666667, 0.6774193548387097, 0.6451612903225806, 0.7258064516129032]\n",
      "The test AUCs of the nested cv [0.5, 0.6666666666666666, 0.75, 0.3]\n"
     ]
    }
   ],
   "source": [
    "# PIPELINE 3\n",
    "# Define outer and inner cross validation\n",
    "outer_cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42) # to do: in discussie zetten waarom loopen nog beter zou zijn voor generaliseren\n",
    "inner_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "best_hp_3 = pd.DataFrame()\n",
    "best_auc_train_3 = []\n",
    "best_auc_test_3= []\n",
    "\n",
    "for train_index, test_index in outer_cv.split(X, y): \n",
    "    X_train = X.transpose()[train_index]\n",
    "    X_train = X_train.transpose()\n",
    "    print(X_train.shape) # print size of X_train\n",
    "    y_train = y[train_index]\n",
    "    \n",
    "    X_test = X.transpose()[test_index]\n",
    "    X_test = X_test.transpose()\n",
    "    # print(X_test.shape) # print size of X_test\n",
    "    y_test_3 = y[test_index]\n",
    "    \n",
    "\n",
    "    # balance the classes, so training set consists of 50% normal and 50% abnormal ECG's\n",
    "    ros = RandomOverSampler(sampling_strategy='minority')\n",
    "    X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "    X_train = X_resampled\n",
    "    y_train = y_resampled   \n",
    "\n",
    "    ## PIPELINE 3: RobustScaler --> PCA + univariate --> SVM_linear\n",
    "    # Define pipeline 3\n",
    "    pipeline_3 = Pipeline([\n",
    "        ('scaler', RobustScaler()),\n",
    "        ('var_threshold', VarianceThreshold(threshold=0.0)),\n",
    "        ('pca', PCA()),\n",
    "        ('univar_feat_sel', SelectKBest(f_classif)),\n",
    "        ('clf', SVC())\n",
    "    ])\n",
    "    # Define hyperparameters of pipeline 2\n",
    "    param_grid = {\n",
    "    'pca__n_components': [0.5,0.75,0.9, 0.95, 0.99],\n",
    "    'univar_feat_sel__k': [ 'all'],\n",
    "    'clf__C': np.logspace(-3, 1, 20),\n",
    "    'clf__kernel': ['linear']\n",
    "    }\n",
    "\n",
    "    # Perform grid search with inner cross-validation, part 1\n",
    "    rand_search = RandomizedSearchCV(pipeline_3, param_distributions=param_grid, cv=inner_cv, scoring='roc_auc', n_jobs=-1) # optimize parameters\n",
    "    rand_search.fit(X_train, y_train)\n",
    "\n",
    "    # Storing the best hyperparameters of each outer-CV loop in a DataFrame\n",
    "    best_hp_3 = best_hp_3.append(rand_search.best_params_,ignore_index=True)\n",
    "\n",
    "    # Print the best hyperparameters and score\n",
    "    # print('Best hyperparameters after first grid search:', rand_search.best_params_)\n",
    "    # print('Best score after first grid search:', rand_search.best_score_)\n",
    "\n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components = rand_search.best_params_['pca__n_components'])\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    # print(f'Size of X_Train after PC {X_train.shape}')\n",
    "\n",
    "    # Get the selected feature indices from the univariate feature selection\n",
    "    sel_kb = SelectKBest(f_classif, k='all')\n",
    "    sel_kb.fit(X_train, y_train)\n",
    "    p_values = sel_kb.pvalues_\n",
    "\n",
    "    reject_fdr, pvals_fdr, _, _ = multipletests(pvals=p_values, alpha=0.05, method='fdr_bh')\n",
    "    features_selected=np.array(np.where(reject_fdr)[0])\n",
    "    # print(f'Size of selected features {features_selected.shape}')\n",
    "\n",
    "    # Apply univariate feature selection on X_train\n",
    "    # print(f'This is the size of X_train before univariate: {X_train.shape}')\n",
    "    X_train = X_train[:, features_selected]\n",
    "    # print(f'This is the size of X_train after univariate: {X_train.shape}')\n",
    "\n",
    "    # Train the classifier on the selected features with the best hyperparameters to create best trained classifier\n",
    "    clf_3 = SVC(C=rand_search.best_params_['clf__C'], kernel=rand_search.best_params_['clf__kernel'])\n",
    "    clf_3.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_3 = clf_3.predict(X_train)\n",
    "\n",
    "    if hasattr(clf_3, 'predict_proba'):\n",
    "    # The first column gives the probability for class = 0, so we take\n",
    "    # the second which gives the probability class = 1:\n",
    "        y_score = clf_3.predict_proba(X_train)[:, 1]\n",
    "    else:\n",
    "        y_score = y_pred_3\n",
    "\n",
    "    auc_train_3 = metrics.roc_auc_score(y_train, y_score)\n",
    "    # Storing the AUC train values of each outer CV loop\n",
    "    best_auc_train_3.append(auc_train_3)\n",
    "\n",
    "    # cv results\n",
    "    #pd.DataFrame(grid_nb.cv_results_)\n",
    "\n",
    "    # Evaluate the classifier on the test data\n",
    "    X_test = pca.transform(X_test)  # Apply the PCA transformation to the test data\n",
    "    X_test = X_test[:, features_selected]  # Apply the feature selection to the PCA-transformed test data\n",
    "    y_pred_3 = clf_3.predict(X_test)\n",
    "    auc_test_3 = metrics.roc_auc_score(y_test_3, y_pred_3)\n",
    "    # Storing the AUC test values of each outer CV loop\n",
    "    best_auc_test_3.append(auc_test_3)\n",
    "\n",
    "print(f'The  optimal hyperparameters per split {best_hp_3}')\n",
    "print(f'The training AUcs of the nested cv {best_auc_train_3}')\n",
    "print(f'The test AUCs of the nested cv {best_auc_test_3}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIPELINE 4\n",
    "- PCA-UNI --> KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\irisv\\AppData\\Local\\Temp\\ipykernel_17308\\3538847933.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_hp_4 = best_hp_4.append(rand_search.best_params_,ignore_index=True)\n",
      "C:\\Users\\irisv\\AppData\\Local\\Temp\\ipykernel_17308\\3538847933.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_hp_4 = best_hp_4.append(rand_search.best_params_,ignore_index=True)\n",
      "C:\\Users\\irisv\\AppData\\Local\\Temp\\ipykernel_17308\\3538847933.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_hp_4 = best_hp_4.append(rand_search.best_params_,ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The  optimal hyperparameters per split:   univar_feat_sel__k  pca__n_components  clf__p  clf__n_neighbors  \\\n",
      "0                all               0.90       1                 4   \n",
      "1                all               0.90       1                 6   \n",
      "2                all               0.90       1                 4   \n",
      "3                all               0.95       2                 4   \n",
      "\n",
      "   clf__leaf_size  \n",
      "0              18  \n",
      "1              12  \n",
      "2              17  \n",
      "3               8  \n",
      "The training AUcs of the nested cv [1.0, 0.9562955254942768, 0.9698231009365246, 1.0]\n",
      "The test AUCs of the nested cv [0.36363636363636365, 0.5166666666666666, 0.6500000000000001, 0.75]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\irisv\\AppData\\Local\\Temp\\ipykernel_17308\\3538847933.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_hp_4 = best_hp_4.append(rand_search.best_params_,ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# PIPELINE 4\n",
    "# Define outer and inner cross validation\n",
    "outer_cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42) # to do: in discussie zetten waarom loopen nog beter zou zijn voor generaliseren\n",
    "inner_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "best_auc_train_4 = []\n",
    "best_auc_test_4= []\n",
    "best_hp_4 = pd.DataFrame()\n",
    "\n",
    "for train_index, test_index in outer_cv.split(X, y): \n",
    "    X_train = X.transpose()[train_index]\n",
    "    X_train = X_train.transpose()\n",
    "    # print(X_train.shape) # print size of X_train\n",
    "    y_train = y[train_index]\n",
    "    \n",
    "    X_test = X.transpose()[test_index]\n",
    "    X_test = X_test.transpose()\n",
    "    # print(X_test.shape) # print size of X_test\n",
    "    y_test_4 = y[test_index]\n",
    "\n",
    "    # balance the classes, so training set consists of 50% normal and 50% abnormal ECG's\n",
    "    ros = RandomOverSampler(sampling_strategy='minority')\n",
    "    X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "    X_train = X_resampled\n",
    "    # print(f'Size of X_train after resampling {X_train.shape}')\n",
    "    y_train = y_resampled   \n",
    "\n",
    "    ## PIPELINE 4: RobustScaler --> PCA + univariate --> KNN\n",
    "    # Define pipeline 4\n",
    "    pipeline_4 = Pipeline([\n",
    "        ('scaler', RobustScaler()),\n",
    "        ('var_threshold', VarianceThreshold(threshold=0.0)),\n",
    "        ('pca', PCA()),\n",
    "        ('univar_feat_sel', SelectKBest(f_classif)),\n",
    "        ('clf', KNeighborsClassifier())\n",
    "    ])\n",
    "    # Define hyperparameters of pipeline 3\n",
    "    param_grid = {\n",
    "    'pca__n_components': [0.5,0.75,0.9,0.95,0.99],\n",
    "    'univar_feat_sel__k': ['all'],\n",
    "    'clf__n_neighbors': list(range(4,26,2)), # op 4 begonnen, want uit learning curves bleek dat \n",
    "    'clf__p': [1,2],\n",
    "    'clf__leaf_size': np.arange(1,26,1)\n",
    "    }\n",
    "\n",
    "    # Perform grid search with inner cross-validation, part 1\n",
    "    rand_search = RandomizedSearchCV(pipeline_4, param_distributions=param_grid, n_iter = 50, cv=inner_cv, scoring='roc_auc', n_jobs=-1) # optimize parameters\n",
    "    rand_search.fit(X_train, y_train)\n",
    "\n",
    "    # Storing the best hyperparameters of each outer-CV loop in a DataFrame\n",
    "    best_hp_4 = best_hp_4.append(rand_search.best_params_,ignore_index=True)\n",
    "\n",
    "    # Print the best hyperparameters and score\n",
    "    # print('Best hyperparameters after first grid search:', rand_search.best_params_)\n",
    "    # print('Best score after first grid search:', rand_search.best_score_)\n",
    "\n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components = rand_search.best_params_['pca__n_components'])\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    # print(f'Size of X_Train after PC {X_train.shape}')\n",
    "\n",
    "    # Get the selected feature indices from the univariate feature selection\n",
    "    sel_kb = SelectKBest(f_classif, k='all')\n",
    "    sel_kb.fit(X_train, y_train)\n",
    "    p_values = sel_kb.pvalues_\n",
    "\n",
    "    reject_fdr, pvals_fdr, _, _ = multipletests(pvals=p_values, alpha=0.05, method='fdr_bh')\n",
    "    features_selected=np.array(np.where(reject_fdr)[0])\n",
    "    # print(f'Size of selected features {features_selected.shape}')\n",
    "    \n",
    "    # Apply univariate feature selection on X_train\n",
    "    # print(f'This is the size of X_train before univariate: {X_train.shape}')\n",
    "    X_train = X_train[:, features_selected]\n",
    "    # print(f'This is the size of X_train after univariate: {X_train.shape}')\n",
    "\n",
    "    # Train the classifier on the selected features with the best hyperparameters to create best trained classifier\n",
    "    clf_4 = KNeighborsClassifier(n_neighbors=rand_search.best_params_['clf__n_neighbors'], p=rand_search.best_params_['clf__p'], leaf_size=rand_search.best_params_['clf__leaf_size'])\n",
    "    clf_4.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_4 = clf_4.predict(X_train)\n",
    "\n",
    "    if hasattr(clf_4, 'predict_proba'):\n",
    "    # The first column gives the probability for class = 0, so we take\n",
    "    # the second which gives the probability class = 1:\n",
    "        y_score = clf_4.predict_proba(X_train)[:, 1]\n",
    "    else:\n",
    "        y_score = y_pred_4\n",
    "\n",
    "    auc_train_4 = metrics.roc_auc_score(y_train, y_score)\n",
    "    # Storing the AUC train values of each outer CV loop\n",
    "    best_auc_train_4.append(auc_train_4)\n",
    "\n",
    "    # cv results\n",
    "    #pd.DataFrame(grid_nb.cv_results_)\n",
    "\n",
    "    # Evaluate the classifier on the test data\n",
    "    X_test = pca.transform(X_test)  # Apply the PCA transformation to the test data\n",
    "    X_test = X_test[:, features_selected]  # Apply the feature selection to the PCA-transformed test data\n",
    "    y_pred_4 = clf_4.predict(X_test)\n",
    "    auc_test_4 = metrics.roc_auc_score(y_test_4, y_pred_4)\n",
    "    \n",
    "    # Storing the AUC test values of each outer CV loop\n",
    "    best_auc_test_4.append(auc_test_4)\n",
    "\n",
    "print(f'The  optimal hyperparameters per split: {best_hp_4}')\n",
    "print(f'The training AUcs of the nested cv {best_auc_train_4}')\n",
    "print(f'The test AUCs of the nested cv {best_auc_test_4}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIPELINE 5 \n",
    "- LASSO --> KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.047e-03, tolerance: 1.500e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.108e-03, tolerance: 1.500e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\irisv\\AppData\\Local\\Temp\\ipykernel_17308\\1979123666.py:76: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_hp_5 = best_hp_5.append(rand_search.best_params_,ignore_index=True)\n",
      "c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.401e-03, tolerance: 1.550e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\irisv\\AppData\\Local\\Temp\\ipykernel_17308\\1979123666.py:76: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_hp_5 = best_hp_5.append(rand_search.best_params_,ignore_index=True)\n",
      "C:\\Users\\irisv\\AppData\\Local\\Temp\\ipykernel_17308\\1979123666.py:76: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_hp_5 = best_hp_5.append(rand_search.best_params_,ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The  optimal hyperparameters per split:    clf__p  clf__n_neighbors  clf__leaf_size\n",
      "0       2                 4              12\n",
      "1       2                 4              16\n",
      "2       2                 8              13\n",
      "3       1                 4              12\n",
      "The training AUcs of the nested cv [0.9966666666666667, 0.9963579604578563, 0.9614984391259106, 0.9958376690946931]\n",
      "The test AUCs of the nested cv [0.75, 0.6666666666666666, 0.75, 0.5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\irisv\\AppData\\Local\\Temp\\ipykernel_17308\\1979123666.py:76: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_hp_5 = best_hp_5.append(rand_search.best_params_,ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# PIPELINE 5\n",
    "\n",
    "# Define outer and inner cross validation\n",
    "outer_cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42) # to do: in discussie zetten waarom loopen nog beter zou zijn voor generaliseren\n",
    "inner_cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "\n",
    "best_auc_train_5 = []\n",
    "best_auc_test_5= []\n",
    "best_hp_5 = pd.DataFrame()\n",
    "\n",
    "for train_index, test_index in outer_cv.split(X, y): \n",
    "    X_train = X.transpose()[train_index]\n",
    "    X_train = X_train.transpose()\n",
    "    # print(X_train.shape) # print size of X_train\n",
    "    y_train = y[train_index]\n",
    "\n",
    "    X_test = X.transpose()[test_index]\n",
    "    X_test = X_test.transpose()\n",
    "    # print(X_test.shape) # print size of X_test\n",
    "    y_test_5 = y[test_index]\n",
    "\n",
    "    # balance the classes, so training set consists of 50% normal and 50% abnormal ECG's\n",
    "    ros = RandomOverSampler(sampling_strategy='minority')\n",
    "    X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "    X_train = X_resampled\n",
    "    # print(f'Size of X_train after resampling {X_train.shape}')\n",
    "    y_train = y_resampled   \n",
    "\n",
    "    ## PIPELINE 5: RobustScaler --> LASSO --> KNN\n",
    "    # Define pipeline 5\n",
    "    pipeline_5a = Pipeline([\n",
    "        ('scaler', RobustScaler()),\n",
    "        ('var_threshold', VarianceThreshold(threshold=0.0)),\n",
    "        ('lasso', Lasso()),\n",
    "    ])\n",
    "    # Define hyperparameters of pipeline 5\n",
    "    param_grid = {\n",
    "    'lasso__alpha': np.logspace(-10, 1, 100),\n",
    "    }\n",
    "\n",
    "    # Perform randomized search with inner cross-validation to find best alpha\n",
    "    rand_search = RandomizedSearchCV(pipeline_5a, param_distributions=param_grid, n_iter = 50, cv=inner_cv, scoring='roc_auc',n_jobs=-1) # optimize parameters\n",
    "    rand_search.fit(X_train, y_train)\n",
    "\n",
    "    # Print the best hyperparameters and score\n",
    "    # print(f'Best hyperparameters after first randomized search: {rand_search.best_params_}')\n",
    "    # print(f'Best score after first randomized search: {rand_search.best_score_}')\n",
    "    \n",
    "    # print(f'This is the size of X_train before LASSO: {X_train.shape}')\n",
    "    # Create a new Lasso model using the best alpha value\n",
    "    lasso = Lasso(alpha=rand_search.best_params_['lasso__alpha'])\n",
    "    lasso.fit(X_train, y_train)\n",
    "\n",
    "    # Get the coefficients of the Lasso model, find them and define the new X_train with less features\n",
    "    coef = lasso.coef_\n",
    "    selected_features = np.where(coef != 0)[0]\n",
    "    X_train = X_train.iloc[:, selected_features]\n",
    "    # print(f'This is the size of X_train after LASSO: {X_train.shape}')\n",
    "\n",
    "    # pipeline 5.b\n",
    "    pipeline_5b = Pipeline([\n",
    "        ('clf', KNeighborsClassifier())\n",
    "    ])\n",
    "    # Define hyperparameters of pipeline 5b\n",
    "    param_grid = {\n",
    "    'clf__n_neighbors': list(range(4,26,2)),\n",
    "    'clf__p': [1,2],\n",
    "    'clf__leaf_size': np.arange(1,26,1)\n",
    "    }\n",
    "\n",
    "    # Perform grid search with inner cross-validation, part 2\n",
    "    rand_search = RandomizedSearchCV(pipeline_5b, param_distributions=param_grid, n_iter = 50, cv=inner_cv, scoring='roc_auc', n_jobs=-1) # optimize parameters\n",
    "    rand_search.fit(X_train, y_train)\n",
    "\n",
    "    # Storing the best hyperparameters of each outer-CV loop in a DataFrame\n",
    "    best_hp_5 = best_hp_5.append(rand_search.best_params_,ignore_index=True)\n",
    "\n",
    "    # Print the best hyperparameters and score\n",
    "    # print(f'Best hyperparameters after second randomized search: {rand_search.best_params_}')\n",
    "    # print(f'Best score after second randomized search: {rand_search.best_score_}')\n",
    "\n",
    "    # Train the classifier on the selected features with the best hyperparameters to create best trained classifier\n",
    "    clf_5 = KNeighborsClassifier(n_neighbors=rand_search.best_params_['clf__n_neighbors'], p=rand_search.best_params_['clf__p'], leaf_size=rand_search.best_params_['clf__leaf_size'])\n",
    "    clf_5.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_5 = clf_5.predict(X_train)\n",
    "\n",
    "    if hasattr(clf_5, 'predict_proba'):\n",
    "    # The first column gives the probability for class = 0, so we take\n",
    "    # the second which gives the probability class = 1:\n",
    "        y_score = clf_5.predict_proba(X_train)[:, 1]\n",
    "    else:\n",
    "        y_score = y_pred_5\n",
    "\n",
    "    auc_train_5 = metrics.roc_auc_score(y_train, y_score)\n",
    "    # Storing the AUC test values of each outer CV loop\n",
    "    best_auc_train_5.append(auc_train_5)\n",
    "   \n",
    "    # cv results\n",
    "    #pd.DataFrame(grid_nb.cv_results_)\n",
    "\n",
    "    # Evaluate the classifier on the test data\n",
    "    X_test = X_test.iloc[:, selected_features] # apply lasso feature reduction to X_test\n",
    "    # print(f'The size of X_test after applying lasso is {X_test.shape}')\n",
    "    y_pred_5 = clf_5.predict(X_test)\n",
    "    auc_test_5 = metrics.roc_auc_score(y_test_5, y_pred_5)\n",
    "\n",
    "# Storing the AUC test values of each outer CV loop\n",
    "    best_auc_test_5.append(auc_test_5)\n",
    "\n",
    "print(f'The  optimal hyperparameters per split: {best_hp_5}')\n",
    "print(f'The training AUcs of the nested cv {best_auc_train_5}')\n",
    "print(f'The test AUCs of the nested cv {best_auc_test_5}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIPELINE 6\n",
    "- PCA-UNI --> Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "32 fits failed out of a total of 100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "32 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 434, in fit\n",
      "    raise ValueError(\"Out of bag estimation only available if bootstrap=True\")\n",
      "ValueError: Out of bag estimation only available if bootstrap=True\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.98222222 0.81555556 0.9               nan\n",
      " 0.78333333 0.93555556        nan        nan        nan 0.82888889\n",
      " 1.         0.93333333        nan        nan 0.93777778 0.94666667\n",
      " 0.99777778 1.         0.89333333 0.96              nan        nan\n",
      " 0.99111111 0.97555556 1.         0.99777778 0.98222222        nan\n",
      "        nan        nan 0.67333333 1.         0.96888889        nan\n",
      " 0.81222222 1.         0.90666667        nan 1.         0.80666667\n",
      " 0.97111111 0.98666667 0.86555556 1.         0.94222222        nan\n",
      " 0.75       0.78      ]\n",
      "  warnings.warn(\n",
      "C:\\Users\\irisv\\AppData\\Local\\Temp\\ipykernel_17308\\2888349706.py:59: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_hp_6 = best_hp_6.append(rand_search.best_params_,ignore_index=True)\n",
      "c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "18 fits failed out of a total of 100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "18 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 434, in fit\n",
      "    raise ValueError(\"Out of bag estimation only available if bootstrap=True\")\n",
      "ValueError: Out of bag estimation only available if bootstrap=True\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.82916667 0.92083333 0.98958333 0.80416667        nan 0.82916667\n",
      " 0.97291667 0.85              nan 0.90833333 0.99583333 0.88541667\n",
      " 0.5        0.77395833 0.92708333 0.81875    0.79166667 0.92291667\n",
      " 1.         0.98333333        nan 0.94375    0.97916667 0.92083333\n",
      " 0.9625     1.         0.93333333 0.84791667 0.99166667 0.92916667\n",
      "        nan 0.975      0.95       0.86875    0.99583333 0.97916667\n",
      "        nan        nan        nan 1.         0.90833333        nan\n",
      " 1.         0.90416667 0.84583333 0.96458333 0.86458333 0.88333333\n",
      " 0.94583333        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\irisv\\AppData\\Local\\Temp\\ipykernel_17308\\2888349706.py:59: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_hp_6 = best_hp_6.append(rand_search.best_params_,ignore_index=True)\n",
      "c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "22 fits failed out of a total of 100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "22 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 434, in fit\n",
      "    raise ValueError(\"Out of bag estimation only available if bootstrap=True\")\n",
      "ValueError: Out of bag estimation only available if bootstrap=True\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.65416667        nan 0.6375            nan 0.92083333 0.9375\n",
      " 0.65625    0.975      0.89375    0.95       0.97083333 0.74895833\n",
      " 0.95416667 0.89166667 0.87916667 0.94166667 0.67708333 0.5\n",
      " 0.94375    0.97083333 0.678125          nan 0.8               nan\n",
      " 0.70208333 0.88333333 0.66354167 0.73333333 0.9625     0.71145833\n",
      " 0.82291667 0.8625     0.5        0.7625     0.73333333        nan\n",
      "        nan 0.8875     0.98333333        nan 0.73229167 0.5\n",
      "        nan 0.82604167        nan 0.91875           nan 0.71666667\n",
      "        nan 0.99583333]\n",
      "  warnings.warn(\n",
      "C:\\Users\\irisv\\AppData\\Local\\Temp\\ipykernel_17308\\2888349706.py:59: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_hp_6 = best_hp_6.append(rand_search.best_params_,ignore_index=True)\n",
      "c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "20 fits failed out of a total of 100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 434, in fit\n",
      "    raise ValueError(\"Out of bag estimation only available if bootstrap=True\")\n",
      "ValueError: Out of bag estimation only available if bootstrap=True\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.99166667 0.9625     0.93541667 0.9        0.96458333\n",
      " 0.77083333 0.78333333 0.96458333 0.88541667        nan 0.92291667\n",
      " 0.99166667        nan 0.94166667 0.94583333 0.9625     0.83125\n",
      " 0.99166667 0.9        0.88125    0.76666667        nan 0.94166667\n",
      " 0.97708333 0.99166667        nan 0.9375     0.5        0.9125\n",
      " 0.99375    0.93333333 0.87291667 0.9375     0.89791667 0.99791667\n",
      "        nan 0.99166667 0.99375    0.92708333        nan 0.93125\n",
      " 0.86458333        nan 0.96875           nan 0.95208333 0.82708333\n",
      "        nan 0.925     ]\n",
      "  warnings.warn(\n",
      "C:\\Users\\irisv\\AppData\\Local\\Temp\\ipykernel_17308\\2888349706.py:59: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_hp_6 = best_hp_6.append(rand_search.best_params_,ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The  optimal hyperparameters per split:   univar_feat_sel__k  pca__n_components  clf__warm_start  clf__oob_score  \\\n",
      "0                all               0.90            False            True   \n",
      "1                all               0.95            False           False   \n",
      "2                all               0.90            False           False   \n",
      "3                all               0.75            False           False   \n",
      "\n",
      "   clf__n_estimators  clf__min_weight_fraction_leaf  clf__min_samples_split  \\\n",
      "0                395                       0.229167                       9   \n",
      "1                225                       0.083333                       9   \n",
      "2                325                       0.000000                       5   \n",
      "3                995                       0.000000                       7   \n",
      "\n",
      "   clf__min_samples_leaf clf__max_features clf__criterion clf__class_weight  \\\n",
      "0                      2              log2           gini          balanced   \n",
      "1                      2              log2        entropy          balanced   \n",
      "2                      2              log2        entropy          balanced   \n",
      "3                      6              sqrt       log_loss          balanced   \n",
      "\n",
      "   clf__bootstrap  \n",
      "0            True  \n",
      "1           False  \n",
      "2           False  \n",
      "3           False  \n",
      "The training AUcs of the nested cv [0.8677777777777778, 0.9521331945889699, 1.0, 0.9183142559833507]\n",
      "The test AUCs of the nested cv [0.45454545454545453, 0.6666666666666666, 0.5, 0.5999999999999999]\n"
     ]
    }
   ],
   "source": [
    "# PIPELINE 6\n",
    "# Define outer and inner cross validation\n",
    "outer_cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42) # to do: in discussie zetten waarom loopen nog beter zou zijn voor generaliseren\n",
    "inner_cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "\n",
    "best_auc_train_6 = []\n",
    "best_auc_test_6= []\n",
    "best_hp_6 = pd.DataFrame()\n",
    "\n",
    "for train_index, test_index in outer_cv.split(X, y): \n",
    "    X_train = X.transpose()[train_index]\n",
    "    X_train = X_train.transpose()\n",
    "    # print(X_train.shape) # print size of X_train\n",
    "    y_train = y[train_index]\n",
    "    \n",
    "    X_test = X.transpose()[test_index]\n",
    "    X_test = X_test.transpose()\n",
    "    # print(X_test.shape) # print size of X_test\n",
    "    y_test_6 = y[test_index]\n",
    "    \n",
    "    \n",
    "\n",
    "    # balance the classes, so training set consists of 50% normal and 50% abnormal ECG's\n",
    "    ros = RandomOverSampler(sampling_strategy='minority')\n",
    "    X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "    X_train = X_resampled\n",
    "    # print(f'Size of X_train after resampling {X_train.shape}')\n",
    "    y_train = y_resampled   \n",
    "\n",
    "    ## PIPELINE 6: RobustScaler --> PCA + univariate --> RF\n",
    "    # Define pipeline 6\n",
    "    pipeline_6 = Pipeline([\n",
    "        ('scaler', RobustScaler()),\n",
    "        ('var_threshold', VarianceThreshold(threshold=0.0)),\n",
    "        ('pca', PCA()),\n",
    "        ('univar_feat_sel', SelectKBest(f_classif)),\n",
    "        ('clf', RandomForestClassifier())])\n",
    "    \n",
    "    # Define hyperparameters of pipeline 6\n",
    "    param_distributions = {'pca__n_components': [0.5,0.75,0.9,0.95,0.99],\n",
    "                               'univar_feat_sel__k': ['all'],\n",
    "                               'clf__n_estimators' : range(5,1000,5),\n",
    "                               'clf__criterion' :['gini','entropy','log_loss'],\n",
    "                               'clf__min_samples_split':range(2,10),\n",
    "                               'clf__min_samples_leaf':range(1,10),\n",
    "                               'clf__min_weight_fraction_leaf' : np.linspace(0, 0.5, 25),\n",
    "                               'clf__max_features':['sqrt','log2',None],\n",
    "                               'clf__bootstrap':[True,False],\n",
    "                               'clf__oob_score':[True,False],\n",
    "                               'clf__warm_start':[True,False],\n",
    "                               'clf__class_weight':['balanced','balanced']}\n",
    "\n",
    "    # Perform grid search with inner cross-validation, part 1\n",
    "    rand_search = RandomizedSearchCV(pipeline_6, param_distributions=param_distributions, n_iter = 50, cv=inner_cv, scoring='roc_auc', n_jobs=-1) # optimize parameters\n",
    "    rand_search.fit(X_train, y_train)\n",
    "\n",
    "    \n",
    "    # Storing the best hyperparameters of each outer-CV loop in a DataFrame\n",
    "    best_hp_6 = best_hp_6.append(rand_search.best_params_,ignore_index=True)\n",
    "\n",
    "    # Print the best hyperparameters and score\n",
    "    # print('Best hyperparameters after first grid search:', rand_search.best_params_)\n",
    "    # print('Best score after first grid search:', rand_search.best_score_)\n",
    "\n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components = rand_search.best_params_['pca__n_components'])\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    # print(f'Size of X_Train after PC {X_train.shape}')\n",
    "\n",
    "    # Get the selected feature indices from the univariate feature selection\n",
    "    sel_kb = SelectKBest(f_classif, k='all')\n",
    "    sel_kb.fit(X_train, y_train)\n",
    "    p_values = sel_kb.pvalues_\n",
    "\n",
    "    reject_fdr, pvals_fdr, _, _ = multipletests(pvals=p_values, alpha=0.05, method='fdr_bh')\n",
    "    features_selected=np.array(np.where(reject_fdr)[0])\n",
    "    # print(f'Size of selected features {features_selected.shape}')\n",
    "    \n",
    "    # Apply univariate feature selection on X_train\n",
    "    # print(f'This is the size of X_train before univariate: {X_train.shape}')\n",
    "    X_train = X_train[:, features_selected]\n",
    "    # print(f'This is the size of X_train after univariate: {X_train.shape}')\n",
    "\n",
    "    # Train the classifier on the selected features with the best hyperparameters to create best trained classifier   \n",
    "    clf_6 = RandomForestClassifier(n_estimators=rand_search.best_params_['clf__n_estimators'], max_features=rand_search.best_params_['clf__max_features'], min_samples_split=rand_search.best_params_['clf__min_samples_split'], bootstrap=rand_search.best_params_['clf__bootstrap'], criterion=rand_search.best_params_['clf__criterion'], min_samples_leaf=rand_search.best_params_['clf__min_samples_leaf'], min_weight_fraction_leaf=rand_search.best_params_['clf__min_weight_fraction_leaf'], oob_score=rand_search.best_params_['clf__oob_score'],warm_start=rand_search.best_params_['clf__warm_start'],class_weight=rand_search.best_params_['clf__class_weight'])\n",
    "    clf_6.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_6 = clf_6.predict(X_train)\n",
    "\n",
    "    if hasattr(clf_6, 'predict_proba'):\n",
    "    # The first column gives the probability for class = 0, so we take\n",
    "    # the second which gives the probability class = 1:\n",
    "        y_score = clf_6.predict_proba(X_train)[:, 1]\n",
    "    else:\n",
    "        y_score = y_pred_6\n",
    "\n",
    "    auc_train_6 = metrics.roc_auc_score(y_train, y_score)\n",
    "     # Storing the AUC test values of each outer CV loop\n",
    "    best_auc_train_6.append(auc_train_6)\n",
    "\n",
    "    # print(f'The auc of the training data is {auc}')\n",
    "    # cv results\n",
    "    #pd.DataFrame(grid_nb.cv_results_)\n",
    "\n",
    "    # Evaluate the classifier on the test data\n",
    "    X_test = pca.transform(X_test)  # Apply the PCA transformation to the test data\n",
    "    X_test = X_test[:, features_selected]  # Apply the feature selection to the PCA-transformed test data\n",
    "    y_pred_6 = clf_6.predict(X_test)\n",
    "    auc_test_6 = metrics.roc_auc_score(y_test_6, y_pred_6)\n",
    "    \n",
    "    # Storing the AUC test values of each outer CV loop\n",
    "    best_auc_test_6.append(auc_test_6)\n",
    "\n",
    "print(f'The  optimal hyperparameters per split: {best_hp_6}')\n",
    "print(f'The training AUcs of the nested cv {best_auc_train_6}')\n",
    "print(f'The test AUCs of the nested cv {best_auc_test_6}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIPELINE 7\n",
    "- LASSO --> Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "18 fits failed out of a total of 100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "18 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 434, in fit\n",
      "    raise ValueError(\"Out of bag estimation only available if bootstrap=True\")\n",
      "ValueError: Out of bag estimation only available if bootstrap=True\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.82222222 0.8               nan 0.82888889 0.77111111 0.82666667\n",
      " 0.82666667 0.75555556 0.81555556 0.85777778 0.78666667 0.80222222\n",
      " 0.76111111 0.84              nan 0.82666667 0.84888889 0.77333333\n",
      " 0.85777778 0.78777778 0.75555556 0.84222222 0.78       0.68333333\n",
      " 0.81777778        nan        nan 0.85777778 0.84       0.68333333\n",
      " 0.76222222 0.75333333 0.81777778 0.75555556        nan 0.76888889\n",
      " 0.76       0.84888889 0.80888889 0.76222222        nan        nan\n",
      "        nan 0.86222222        nan 0.82666667 0.82666667 0.81333333\n",
      " 0.8        0.74666667]\n",
      "  warnings.warn(\n",
      "C:\\Users\\irisv\\AppData\\Local\\Temp\\ipykernel_17308\\4201202602.py:80: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_hp_7 = best_hp_7.append(rand_search.best_params_,ignore_index=True)\n",
      "c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.037e-02, tolerance: 1.550e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "26 fits failed out of a total of 100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "26 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 434, in fit\n",
      "    raise ValueError(\"Out of bag estimation only available if bootstrap=True\")\n",
      "ValueError: Out of bag estimation only available if bootstrap=True\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.99375           nan 1.                nan 0.99375    0.99375\n",
      " 0.5        0.98958333        nan 0.98125    1.         0.98125\n",
      " 1.         0.99375           nan 0.99375           nan 0.84166667\n",
      " 1.         0.99375    1.         0.99375           nan 0.98020833\n",
      "        nan 1.                nan        nan 0.99375    1.\n",
      " 0.9875            nan        nan 1.         0.99375    1.\n",
      " 0.99375    0.5        1.         1.                nan 0.98125\n",
      " 0.99375    1.         1.         0.99375    0.99375    0.9875\n",
      "        nan 0.99375   ]\n",
      "  warnings.warn(\n",
      "c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "C:\\Users\\irisv\\AppData\\Local\\Temp\\ipykernel_17308\\4201202602.py:80: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_hp_7 = best_hp_7.append(rand_search.best_params_,ignore_index=True)\n",
      "c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "14 fits failed out of a total of 100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "14 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 434, in fit\n",
      "    raise ValueError(\"Out of bag estimation only available if bootstrap=True\")\n",
      "ValueError: Out of bag estimation only available if bootstrap=True\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.75208333 0.78958333 0.78333333 0.7875     0.68333333        nan\n",
      " 0.78958333 0.7625     0.82083333 0.75833333 0.75416667 0.77916667\n",
      "        nan 0.64583333        nan 0.80208333 0.73020833        nan\n",
      " 0.7625     0.76458333 0.7        0.77916667 0.77291667 0.74791667\n",
      " 0.77291667 0.5        0.77916667 0.79791667 0.82708333 0.7875\n",
      " 0.76666667 0.7625            nan 0.68125    0.74583333        nan\n",
      " 0.5        0.72708333 0.76458333 0.76666667 0.78333333 0.70625\n",
      " 0.78541667 0.78541667 0.77916667 0.73125           nan 0.78541667\n",
      " 0.75       0.76875   ]\n",
      "  warnings.warn(\n",
      "c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "C:\\Users\\irisv\\AppData\\Local\\Temp\\ipykernel_17308\\4201202602.py:80: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_hp_7 = best_hp_7.append(rand_search.best_params_,ignore_index=True)\n",
      "c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.409e-02, tolerance: 1.550e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "28 fits failed out of a total of 100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "28 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 434, in fit\n",
      "    raise ValueError(\"Out of bag estimation only available if bootstrap=True\")\n",
      "ValueError: Out of bag estimation only available if bootstrap=True\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\irisv\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.9875            nan 0.95625    0.96458333 0.94583333\n",
      "        nan        nan 0.97916667 0.98541667        nan        nan\n",
      " 0.94166667 0.91041667 0.97291667        nan 0.97291667 0.96875\n",
      "        nan 0.98125    0.96875    0.9625     0.88958333 0.95625\n",
      "        nan 0.975             nan 0.96875    0.9625     0.94375\n",
      "        nan 0.975      0.975      0.89375    0.80416667 0.97708333\n",
      " 0.97083333 0.9        0.94791667 0.80416667 0.9625     0.96875\n",
      " 0.80416667        nan 0.95208333        nan 0.96875    0.95416667\n",
      "        nan 0.96875   ]\n",
      "  warnings.warn(\n",
      "C:\\Users\\irisv\\AppData\\Local\\Temp\\ipykernel_17308\\4201202602.py:80: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_hp_7 = best_hp_7.append(rand_search.best_params_,ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The  optimal hyperparameters per split:    clf__warm_start  clf__oob_score  clf__n_estimators  \\\n",
      "0            False           False                235   \n",
      "1             True           False                935   \n",
      "2             True           False                710   \n",
      "3            False            True                385   \n",
      "\n",
      "   clf__min_weight_fraction_leaf  clf__min_samples_split  \\\n",
      "0                       0.062500                       4   \n",
      "1                       0.187500                       7   \n",
      "2                       0.145833                       2   \n",
      "3                       0.270833                       5   \n",
      "\n",
      "   clf__min_samples_leaf clf__max_features clf__criterion clf__class_weight  \\\n",
      "0                      3              log2       log_loss          balanced   \n",
      "1                      4              log2        entropy          balanced   \n",
      "2                      6              log2           gini          balanced   \n",
      "3                      9              log2       log_loss          balanced   \n",
      "\n",
      "   clf__bootstrap  \n",
      "0           False  \n",
      "1            True  \n",
      "2           False  \n",
      "3            True  \n",
      "The training AUcs of the nested cv [1.0, 1.0, 1.0, 0.9968782518210197]\n",
      "The test AUCs of the nested cv [0.5, 0.5, 0.5, 0.6500000000000001]\n"
     ]
    }
   ],
   "source": [
    "# Define outer and inner cross validation\n",
    "outer_cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42) # to do: in discussie zetten waarom loopen nog beter zou zijn voor generaliseren\n",
    "inner_cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "\n",
    "best_auc_train_7 = []\n",
    "best_auc_test_7= []\n",
    "best_hp_7 = pd.DataFrame()\n",
    "\n",
    "for train_index, test_index in outer_cv.split(X, y): \n",
    "    X_train = X.transpose()[train_index]\n",
    "    X_train = X_train.transpose()\n",
    "    # print(X_train.shape) # print size of X_train\n",
    "    y_train = y[train_index]\n",
    "\n",
    "    X_test = X.transpose()[test_index]\n",
    "    X_test = X_test.transpose()\n",
    "    # print(X_test.shape) # print size of X_test\n",
    "    y_test_7 = y[test_index]\n",
    "\n",
    "    # balance the classes, so training set consists of 50% normal and 50% abnormal ECG's\n",
    "    ros = RandomOverSampler(sampling_strategy='minority')\n",
    "    X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "    X_train = X_resampled\n",
    "    # print(f'Size of X_train after resampling {X_train.shape}')\n",
    "    y_train = y_resampled   \n",
    "\n",
    "    ## PIPELINE 7: RobustScaler --> LASSO --> RF\n",
    "    # Define pipeline 7\n",
    "    pipeline_7a = Pipeline([\n",
    "        ('scaler', RobustScaler()),\n",
    "        ('lasso', Lasso()),\n",
    "    ])\n",
    "    # Define hyperparameters of pipeline 5\n",
    "    param_grid = {\n",
    "    'lasso__alpha': np.logspace(-10, 1, 100),\n",
    "    }\n",
    "\n",
    "    # Perform randomized search with inner cross-validation to find best alpha\n",
    "    rand_search = RandomizedSearchCV(pipeline_5a, param_distributions=param_grid, n_iter = 50, cv=inner_cv, scoring='roc_auc',n_jobs=-1) # optimize parameters\n",
    "    rand_search.fit(X_train, y_train)\n",
    "\n",
    "    # Print the best hyperparameters and score\n",
    "    # print(f'Best hyperparameters after first randomized search: {rand_search.best_params_}')\n",
    "    # print(f'Best score after first randomized search: {rand_search.best_score_}')\n",
    "    \n",
    "    # print(f'This is the size of X_train before LASSO: {X_train.shape}')\n",
    "    # Create a new Lasso model using the best alpha value\n",
    "    lasso = Lasso(alpha=rand_search.best_params_['lasso__alpha'])\n",
    "    lasso.fit(X_train, y_train)\n",
    "\n",
    "    # Get the coefficients of the Lasso model, find them and define the new X_train with less features\n",
    "    coef = lasso.coef_\n",
    "    selected_features = np.where(coef != 0)[0]\n",
    "    X_train = X_train.iloc[:, selected_features]\n",
    "    # print(f'This is the size of X_train after LASSO: {X_train.shape}')\n",
    "\n",
    "## PIPELINE 7: RobustScaler --> LASSO --> RF\n",
    "    # Define pipeline 7\n",
    "    pipeline_7 = Pipeline([\n",
    "        ('clf', RandomForestClassifier())])\n",
    "    \n",
    "    # Define hyperparameters of pipeline 7\n",
    "    param_distributions = {'clf__n_estimators' : range(5,1000,5),\n",
    "                               'clf__criterion' :['gini','entropy','log_loss'],\n",
    "                               'clf__min_samples_split':range(2,10),\n",
    "                               'clf__min_samples_leaf':range(1,10),\n",
    "                               'clf__min_weight_fraction_leaf' : np.linspace(0, 0.5, 25),\n",
    "                               'clf__max_features':['sqrt','log2',None],\n",
    "                               'clf__bootstrap':[True,False],\n",
    "                               'clf__oob_score':[True,False],\n",
    "                               'clf__warm_start':[True,False],\n",
    "                               'clf__class_weight':['balanced','balanced']}\n",
    "\n",
    "    # Perform grid search with inner cross-validation, part 1\n",
    "    rand_search = RandomizedSearchCV(pipeline_7, param_distributions=param_distributions, n_iter = 50, cv=inner_cv, scoring='roc_auc', n_jobs=-1) # optimize parameters\n",
    "    rand_search.fit(X_train, y_train)\n",
    "\n",
    "    \n",
    "    # Storing the best hyperparameters of each outer-CV loop in a DataFrame\n",
    "    best_hp_7 = best_hp_7.append(rand_search.best_params_,ignore_index=True)\n",
    "\n",
    "    # Print the best hyperparameters and score\n",
    "    # print('Best hyperparameters after first grid search:', rand_search.best_params_)\n",
    "    # print('Best score after first grid search:', rand_search.best_score_)\n",
    "\n",
    "    # Train the classifier on the selected features with the best hyperparameters to create best trained classifier   \n",
    "    clf_7 = RandomForestClassifier(n_estimators=rand_search.best_params_['clf__n_estimators'], max_features=rand_search.best_params_['clf__max_features'], min_samples_split=rand_search.best_params_['clf__min_samples_split'], bootstrap=rand_search.best_params_['clf__bootstrap'], criterion=rand_search.best_params_['clf__criterion'], min_samples_leaf=rand_search.best_params_['clf__min_samples_leaf'], min_weight_fraction_leaf=rand_search.best_params_['clf__min_weight_fraction_leaf'], oob_score=rand_search.best_params_['clf__oob_score'],warm_start=rand_search.best_params_['clf__warm_start'],class_weight=rand_search.best_params_['clf__class_weight'])\n",
    "    clf_7.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_7 = clf_7.predict(X_train)\n",
    "\n",
    "    if hasattr(clf_7, 'predict_proba'):\n",
    "    # The first column gives the probability for class = 0, so we take\n",
    "    # the second which gives the probability class = 1:\n",
    "        y_score = clf_7.predict_proba(X_train)[:, 1]\n",
    "    else:\n",
    "        y_score = y_pred_7\n",
    "\n",
    "    auc_train_7 = metrics.roc_auc_score(y_train, y_score)\n",
    "     # Storing the AUC test values of each outer CV loop\n",
    "    best_auc_train_7.append(auc_train_7)\n",
    "\n",
    "    # print(f'The auc of the training data is {auc}')\n",
    "    # cv results\n",
    "    #pd.DataFrame(grid_nb.cv_results_)\n",
    "\n",
    "    # Evaluate the classifier on the test data\n",
    "    X_test = X_test.iloc[:, selected_features]  # Apply the feature selection to the PCA-transformed test data\n",
    "    y_pred_7 = clf_7.predict(X_test)\n",
    "    auc_test_7 = metrics.roc_auc_score(y_test_7, y_pred_7)\n",
    "    \n",
    "    # Storing the AUC test values of each outer CV loop\n",
    "    best_auc_test_7.append(auc_test_7)\n",
    "\n",
    "print(f'The  optimal hyperparameters per split: {best_hp_7}')\n",
    "print(f'The training AUcs of the nested cv {best_auc_train_7}')\n",
    "print(f'The test AUCs of the nested cv {best_auc_test_7}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ouput visualization\n",
    "- Tabel ROC/AUC\n",
    "- Plot ROC/AUC\n",
    "- Confusion matrix\n",
    "- Tabel chosen hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table ROC-AUC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\irisv\\AppData\\Local\\Temp\\ipykernel_17308\\2218610843.py:36: FutureWarning: this method is deprecated in favour of `Styler.to_html()`\n",
      "  HTML(format_table(df).render())\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_5ed26 th {\n",
       "  font-size: 14pt;\n",
       "  text-align: center;\n",
       "  border: 1px solid #ddd;\n",
       "  padding: 8px;\n",
       "}\n",
       "#T_5ed26 td {\n",
       "  font-size: 12pt;\n",
       "  text-align: center;\n",
       "  border: 1px solid #ddd;\n",
       "  padding: 8px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_5ed26\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_5ed26_level0_col0\" class=\"col_heading level0 col0\" >Pipeline 1</th>\n",
       "      <th id=\"T_5ed26_level0_col1\" class=\"col_heading level0 col1\" >Pipeline 3</th>\n",
       "      <th id=\"T_5ed26_level0_col2\" class=\"col_heading level0 col2\" >Pipeline 4</th>\n",
       "      <th id=\"T_5ed26_level0_col3\" class=\"col_heading level0 col3\" >Pipeline 5</th>\n",
       "      <th id=\"T_5ed26_level0_col4\" class=\"col_heading level0 col4\" >Pipeline 6</th>\n",
       "      <th id=\"T_5ed26_level0_col5\" class=\"col_heading level0 col5\" >Pipeline 7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_5ed26_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_5ed26_row0_col0\" class=\"data row0 col0\" >0.500000</td>\n",
       "      <td id=\"T_5ed26_row0_col1\" class=\"data row0 col1\" >0.500000</td>\n",
       "      <td id=\"T_5ed26_row0_col2\" class=\"data row0 col2\" >0.363636</td>\n",
       "      <td id=\"T_5ed26_row0_col3\" class=\"data row0 col3\" >0.750000</td>\n",
       "      <td id=\"T_5ed26_row0_col4\" class=\"data row0 col4\" >0.454545</td>\n",
       "      <td id=\"T_5ed26_row0_col5\" class=\"data row0 col5\" >0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ed26_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_5ed26_row1_col0\" class=\"data row1 col0\" >0.666667</td>\n",
       "      <td id=\"T_5ed26_row1_col1\" class=\"data row1 col1\" >0.666667</td>\n",
       "      <td id=\"T_5ed26_row1_col2\" class=\"data row1 col2\" >0.516667</td>\n",
       "      <td id=\"T_5ed26_row1_col3\" class=\"data row1 col3\" >0.666667</td>\n",
       "      <td id=\"T_5ed26_row1_col4\" class=\"data row1 col4\" >0.666667</td>\n",
       "      <td id=\"T_5ed26_row1_col5\" class=\"data row1 col5\" >0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ed26_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_5ed26_row2_col0\" class=\"data row2 col0\" >0.750000</td>\n",
       "      <td id=\"T_5ed26_row2_col1\" class=\"data row2 col1\" >0.750000</td>\n",
       "      <td id=\"T_5ed26_row2_col2\" class=\"data row2 col2\" >0.650000</td>\n",
       "      <td id=\"T_5ed26_row2_col3\" class=\"data row2 col3\" >0.750000</td>\n",
       "      <td id=\"T_5ed26_row2_col4\" class=\"data row2 col4\" >0.500000</td>\n",
       "      <td id=\"T_5ed26_row2_col5\" class=\"data row2 col5\" >0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ed26_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_5ed26_row3_col0\" class=\"data row3 col0\" >0.700000</td>\n",
       "      <td id=\"T_5ed26_row3_col1\" class=\"data row3 col1\" >0.300000</td>\n",
       "      <td id=\"T_5ed26_row3_col2\" class=\"data row3 col2\" >0.750000</td>\n",
       "      <td id=\"T_5ed26_row3_col3\" class=\"data row3 col3\" >0.500000</td>\n",
       "      <td id=\"T_5ed26_row3_col4\" class=\"data row3 col4\" >0.600000</td>\n",
       "      <td id=\"T_5ed26_row3_col5\" class=\"data row3 col5\" >0.650000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tabel ROC-AUC\n",
    "from IPython.display import HTML\n",
    "\n",
    "# create the DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Pipeline 1': best_auc_test_1,\n",
    "    #'Pipeline 2': best_auc_test_2,\n",
    "    'Pipeline 3': best_auc_test_3,\n",
    "    'Pipeline 4': best_auc_test_4,\n",
    "    'Pipeline 5': best_auc_test_5,\n",
    "    'Pipeline 6': best_auc_test_6,\n",
    "    'Pipeline 7': best_auc_test_7\n",
    "})\n",
    "\n",
    "# define a function to format the table\n",
    "def format_table(df):\n",
    "    return df.style.set_table_styles([{\n",
    "        'selector': 'th',\n",
    "        'props': [\n",
    "            ('font-size', '14pt'),\n",
    "            ('text-align', 'center'),\n",
    "            ('border', '1px solid #ddd'),\n",
    "            ('padding', '8px')\n",
    "        ]\n",
    "    }, {\n",
    "        'selector': 'td',\n",
    "        'props': [\n",
    "            ('font-size', '12pt'),\n",
    "            ('text-align', 'center'),\n",
    "            ('border', '1px solid #ddd'),\n",
    "            ('padding', '8px')\n",
    "        ]\n",
    "    }])\n",
    "\n",
    "# display the DataFrame as a table\n",
    "print('Table ROC-AUC')\n",
    "HTML(format_table(df).render())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meuk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipelines\n",
    "#1\n",
    "def pipeline1(X_train, y_train, X_test, y_test):\n",
    "    # Preprocessing\n",
    "    scaler = RobustScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Variance Threshold\n",
    "    vt = VarianceThreshold(threshold=0.0)\n",
    "    X_train = vt.fit_transform(X_train)\n",
    "    X_test = vt.transform(X_test)\n",
    "\n",
    "    # PCA\n",
    "    n_samples = X_train.shape[0]\n",
    "    n_features = X_train.shape[1]\n",
    "    n_features = min(n_samples, n_features)\n",
    "    pca = PCA(n_components=n_features)\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "\n",
    "    # Univariate selection\n",
    "    kb = SelectKBest(f_classif, k='all') # juiste code erin als af\n",
    "    X_train = kb.fit_transform(X_train, y_train)\n",
    "    p_values_train = kb.pvalues_\n",
    "    reject_fdr, pvals_fdr, _, _ = multipletests(pvals=p_values_train, alpha=0.05, method='fdr_bh')\n",
    "    features_selected_train = np.array(np.where(reject_fdr)[0])\n",
    "    print(features_selected_train.shape)\n",
    "    X_train = X_train[:,features_selected_train]\n",
    "\n",
    "    X_test = kb.transform(X_test)\n",
    "    p_values_test = kb.pvalues_\n",
    "    reject_fdr, pvals_fdr, _, _ = multipletests(pvals=p_values_test, alpha=0.05, method='fdr_bh')\n",
    "    features_selected_test = np.array(np.where(reject_fdr)[0])\n",
    "    print(features_selected_test.shape)\n",
    "    X_test = X_test[:,features_selected_test]\n",
    "\n",
    "    # KNN classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    roc_auc_score = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    return roc_auc_score\n",
    "\n",
    "def pipeline2(X_train, y_train, X_test, y_test, alpha=0.5, n_neighbors=5):\n",
    "    # Preprocessing\n",
    "    scaler = RobustScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Variance Threshold\n",
    "    vt = VarianceThreshold(threshold=0.0)\n",
    "    X_train = vt.fit_transform(X_train)\n",
    "    X_test = vt.transform(X_test)\n",
    "\n",
    "    # LASSO regularization\n",
    "    lasso = Lasso(alpha=alpha)\n",
    "    X_train = lasso.fit_transform(X_train, y_train)\n",
    "    X_test = lasso.transform(X_test)\n",
    "\n",
    "    # KNN classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    roc_auc_score = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    return roc_auc_score\n",
    "\n",
    "# Define the parameters for grid search\n",
    "param_grid1 = {\n",
    "    'n_pca': [], #doesn't have parameters that need grid_search\n",
    "    'k_features': [], #doesn't have parameters that need grid_search\n",
    "    'n_neighbors': list(range(1, 26, 2))\n",
    "}\n",
    "\n",
    "param_grid2 = {\n",
    "    'alpha': [np.logspace(-5, 1, 100)],\n",
    "    'n_neighbors': list(range(1, 26, 2))\n",
    "}\n",
    "\n",
    "# Define the outer and inner cross-validation\n",
    "outer_cv = StratifiedKFold(n_splits=4) #als we 4 splits doen, wordt de data in 25%/75% gesplit\n",
    "inner_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform the grid search using nested cross-validation\n",
    "best_scores1 = []\n",
    "best_scores2 = []\n",
    "\n",
    "best_n_neighbors = []\n",
    "results_1 = []\n",
    "results_2 = []\n",
    "\n",
    "for train_index, test_index in outer_cv.split(X, y): \n",
    "    X_train = X.transpose()[train_index]\n",
    "    X_train = X_train.transpose()\n",
    "    print(X_train.shape)\n",
    "    y_train = y[train_index]\n",
    "    \n",
    "    X_test = X.transpose()[test_index]\n",
    "    X_test = X_test.transpose()\n",
    "    y_test = y[test_index]\n",
    "    print(X_test.shape)\n",
    "\n",
    "    # Create a grid search to find the optimal k using a gridsearch and 10-fold cross validation\n",
    "#     grid_search1 = GridSearchCV(estimator=pipeline1, param_grid=param_grid1, cv=inner_cv, scoring='roc_auc') #error logisch, want pipeline1 is een functie\n",
    "#     grid_search1.fit(X_train, y_train)\n",
    "#     best_scores1.append(grid_search1.best_score_)\n",
    "\n",
    "#     grid_search2 = GridSearchCV(estimator=pipeline2, param_grid=param_grid2, cv=inner_cv, scoring='roc_auc')\n",
    "#     grid_search2.fit(X_train, y_train)\n",
    "#     best_scores2.append(grid_search2.best_score_)\n",
    "\n",
    "# # Print the best scores and the average score for each pipeline\n",
    "# print(best_scores1)\n",
    "# print(best_scores2)\n",
    "\n",
    "# # Get resulting classifier, pipeline 1\n",
    "# best_1 = grid_search1.best_estimator_\n",
    "# print(f'Best classifier: k={best_1.n_neighbors}')\n",
    "# best_n_neighbors.append(best_1.n_neighbors)\n",
    "\n",
    "# # Test the classifier on the test data, pipeline 1\n",
    "# probabilities = best_1.predict_proba(X_test)\n",
    "# scores_1 = probabilities[:, 1]\n",
    "\n",
    "# # Get the auc, pipeline 1\n",
    "# auc_1 = metrics.roc_auc_score(y_test, scores_1)\n",
    "# results_1.append({\n",
    "#     'auc': auc_1,\n",
    "#     'k': best_1.n_neighbors,\n",
    "#     'set': 'test'\n",
    "# })\n",
    "\n",
    "# # Create results dataframe and plot it, pipeline 1\n",
    "# results = pd.DataFrame(results_1)\n",
    "# seaborn.boxplot(y='auc', x='set', data=results_1)\n",
    "\n",
    "# optimal_n = int(np.median(best_n_neighbors))\n",
    "# print(f\"The optimal N={optimal_n}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
