{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TM10007 Assignment template -- ECG data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in c:\\users\\ellem\\miniconda3\\lib\\site-packages (0.0.post1)\n",
      "Requirement already satisfied: numpy in c:\\users\\ellem\\miniconda3\\lib\\site-packages (1.23.5)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ellem\\miniconda3\\lib\\site-packages (3.6.2)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\ellem\\miniconda3\\lib\\site-packages (0.10.1)\n",
      "Requirement already satisfied: statsmodels in c:\\users\\ellem\\miniconda3\\lib\\site-packages (0.13.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ellem\\miniconda3\\lib\\site-packages (from matplotlib) (22.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\ellem\\miniconda3\\lib\\site-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\ellem\\miniconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\ellem\\miniconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ellem\\miniconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ellem\\miniconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ellem\\miniconda3\\lib\\site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ellem\\miniconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\ellem\\miniconda3\\lib\\site-packages (from imbalanced-learn) (1.9.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\ellem\\miniconda3\\lib\\site-packages (from imbalanced-learn) (1.1.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\ellem\\miniconda3\\lib\\site-packages (from imbalanced-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ellem\\miniconda3\\lib\\site-packages (from imbalanced-learn) (2.2.0)\n",
      "Requirement already satisfied: patsy>=0.5.2 in c:\\users\\ellem\\miniconda3\\lib\\site-packages (from statsmodels) (0.5.3)\n",
      "Requirement already satisfied: pandas>=0.25 in c:\\users\\ellem\\miniconda3\\lib\\site-packages (from statsmodels) (1.5.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ellem\\miniconda3\\lib\\site-packages (from pandas>=0.25->statsmodels) (2022.7)\n",
      "Requirement already satisfied: six in c:\\users\\ellem\\miniconda3\\lib\\site-packages (from patsy>=0.5.2->statsmodels) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install sklearn numpy matplotlib imbalanced-learn statsmodels\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels\n",
    "import seaborn\n",
    "import warnings\n",
    "\n",
    "from scipy.stats import shapiro, lognorm\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, learning_curve, GridSearchCV, StratifiedKFold, cross_val_score, KFold, train_test_split, RandomizedSearchCV #, multipletests\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn import datasets as ds, model_selection, metrics, neighbors\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif, SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Required and still not used:\n",
    "#import torch\n",
    "#import seaborn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd() # This fn will return the Current Working Directory\n",
    "\n",
    "zip_path = os.path.join(cwd, 'ecg', 'ecg_data.zip')\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(os.path.join(cwd, 'ecg'))\n",
    "\n",
    "data_path = os.path.join(cwd, 'ecg', 'ecg_data.csv')\n",
    "data = pd.read_csv(data_path, index_col=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146 people have an abnormal ECG\n",
      "681 people have a normal ECG\n",
      "The percentage of abnoraml ECGs in this dataset is 17.654171704957676 %\n"
     ]
    }
   ],
   "source": [
    "# split labels from data\n",
    "X = data.loc[:, data.columns != 'label']  #alles behalve label\n",
    "y = data['label']  # labels\n",
    "\n",
    "# normal / abnormal ECGs\n",
    "total_abnormal_ECG = np.count_nonzero(y) \n",
    "total_normal_ECG = y.size -np.count_nonzero(y) \n",
    "percentage_abnormal = total_abnormal_ECG / (total_abnormal_ECG + total_normal_ECG)*100\n",
    "\n",
    "print(f'{total_abnormal_ECG} people have an abnormal ECG')\n",
    "print(f'{total_normal_ECG} people have a normal ECG')\n",
    "print(f'The percentage of abnoraml ECGs in this dataset is {percentage_abnormal} %')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for normal distribution - before outliers and missing data correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normally distributed\n",
    "# stat = []\n",
    "# p = []\n",
    "# for col in X.columns:\n",
    "#     if X[col].dtype == 'float64' or X[col].dtype == 'int64':\n",
    "#         s, pv = shapiro(X[col])\n",
    "#         stat.append(s)\n",
    "#         p.append(pv)\n",
    "#     else:\n",
    "#         stat.append(None)\n",
    "#         p.append(None)\n",
    "\n",
    "# # hier nog iets printen of het wel/niet normally distributed is?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing data\n",
    "- Removing features if there is lot of data missing (replace all for a value)\n",
    "- Removing samples (in this case patients) if there is a lot of data missing\n",
    "- Imputation for generating data to fill us missing values -> median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Missing data\n",
    "# X = X.replace(0, np.nan)  # make all zeros to NaN\n",
    "# nan_count = X.isna().sum().sum()  # count missing data -> 10500 in our dataset\n",
    "\n",
    "# # Delete missing data when > --% of feature of sample is missing\n",
    "# X = X.dropna(axis='columns', how='all') # deletes a feature if all values of a column (so feature) are empty\n",
    "# X = X.dropna(axis='rows', how='all') # deletes a patient if all values of a row (so sample) are empty\n",
    "\n",
    "# # Missing data to median per feature\n",
    "# for column in X.columns:\n",
    "#     X[column].fillna(X[column].median(), inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "- Detect outliers using Z-score since data is not nornally distributed\n",
    "- Replace outliers by the median of that feature\n",
    "- Print -> check wether the outliers are changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # supress performance warning\n",
    "# warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# # create a new dataframe to store the results\n",
    "# results = pd.DataFrame({'Column': X.columns, 'W': stat, 'p-value': p}) \n",
    "# mean_p_value = results['p-value'].mean()  # p-value is really small. If p-value is bigger than 0.05, then data is normally distributed. SO its not\n",
    "# median_p_value = results['p-value'].median()  # p-value is really small. If p-value is bigger than 0.05, then data is normally distributed. SO its not\n",
    "\n",
    "# # Outliers: Tukey's fence \n",
    "# k=3\n",
    "# fences=pd.DataFrame()\n",
    "# outliers = pd.DataFrame(False, index=X.index, columns=X.columns) # create an empty DataFrame for outliers\n",
    "\n",
    "# for col in X.columns:\n",
    "#     q1, q3 = np.percentile(X[col], [25, 75])\n",
    "#     iqr = q3 - q1\n",
    "#     lower_fence = q1 - k*iqr\n",
    "#     upper_fence = q3 + k*iqr\n",
    "#     fences[col]=[lower_fence, upper_fence]\n",
    "#     for row in X.index:\n",
    "#         if X.loc[row, col] < lower_fence or X.loc[row, col] > upper_fence:\n",
    "#             outliers.loc[row, col] = True # mark the place as an outlier\n",
    "\n",
    "# row_count = (outliers == True).sum(axis=1)\n",
    "# col_count = (outliers == True).sum(axis=0)\n",
    "# total_count = row_count.sum() + col_count.sum()\n",
    "# print(f'The total number of outliers in dataset x is {total_count}')\n",
    "\n",
    "# # create a copy of x to modify\n",
    "# new_x = X.copy()\n",
    "\n",
    "# # replace outliers with median of x by column\n",
    "# for col in outliers.columns:\n",
    "#     median = X.loc[outliers[col] == False, col].median() # median of column where outlier is False\n",
    "#     new_x.loc[outliers[col], col] = median # replace outliers with median"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for normal distribution - after outliers and missing data correction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing zero Variance features\n",
    "not sure yet where to put that"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together in pipelines"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 3\n",
    "- RobustScaler --> PCA + univariate --> Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(620, 9000)\n",
      "(207, 9000)\n"
     ]
    }
   ],
   "source": [
    "# Define outer and inner cross validation\n",
    "outer_cv = StratifiedKFold(n_splits=4)\n",
    "inner_cv = StratifiedKFold(n_splits=2) #, shuffle=True, random_state=42)\n",
    "\n",
    "for train_index, test_index in outer_cv.split(X, y): \n",
    "    X_train = X.transpose()[train_index]\n",
    "    X_train = X_train.transpose()\n",
    "    print(X_train.shape)\n",
    "    # print size of X_train\n",
    "    y_train = y[train_index]\n",
    "    \n",
    "    X_test = X.transpose()[test_index]\n",
    "    X_test = X_test.transpose()\n",
    "    y_test = y[test_index]\n",
    "    # print size of X_test\n",
    "    print(X_test.shape)\n",
    "\n",
    "    # balance the classes, so training set consists of 50% normal and 50% abnormal ECG's\n",
    "    ros = RandomOverSampler(sampling_strategy='minority')\n",
    "    X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "    X_train = X_resampled\n",
    "    y_train = y_resampled   \n",
    "\n",
    "    # pipeline 1:\n",
    "    # pipeline 2: \n",
    "    ## PIPELINE 3: RobustScaler --> PCA + univariate --> Gaussian Naive Bayes\n",
    "    # Define pipeline 3\n",
    "    pipeline_3 = Pipeline([\n",
    "        ('scaler', RobustScaler()),\n",
    "        ('var_threshold', VarianceThreshold(threshold=0.0)),\n",
    "        ('pca', PCA()),\n",
    "        ('univar_feat_sel', SelectKBest(f_classif)),\n",
    "        ('clf', GaussianNB())\n",
    "    ])\n",
    "    # Define hyperparameters of pipeline 3\n",
    "    param_grid = {\n",
    "    'pca__n_components': [0.5,0.75,0.9],\n",
    "    'univar_feat_sel__k': [5, 10, 20, 30],\n",
    "    'clf__var_smoothing': np.logspace(0,-9, num=100),\n",
    "    }\n",
    "\n",
    "    # Perform grid search with inner cross-validation, part 1\n",
    "    rand_search = RandomizedSearchCV(pipeline_3, param_distributions=param_grid, n_iter=50, cv=inner_cv, scoring='roc_auc') #klopt roc_auc?\n",
    "    rand_search.fit(X_train, y_train)\n",
    "\n",
    "    # Print the best hyperparameters and score\n",
    "    print('Best hyperparameters after first grid search:', rand_search.best_params_)\n",
    "    print('Best score after first grid search:', rand_search.best_score_)\n",
    "\n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components = rand_search.best_params_['pca__n_components'])\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    print(f'Size of X_Train after PC {X_train.shape}')\n",
    "\n",
    "    # Get the selected feature indices from the univariate feature selection\n",
    "    sel_kb = SelectKBest(f_classif, k='all')\n",
    "    sel_kb.fit(X_train, y_train)\n",
    "    p_values = sel_kb.pvalues_\n",
    "\n",
    "    reject_fdr, pvals_fdr, _, _ = multipletests(pvals=p_values, alpha=0.05, method='fdr_bh')\n",
    "    features_selected=np.array(np.where(reject_fdr)[0])\n",
    "    print(f'Size of selected features {features_selected.shape}')\n",
    "\n",
    "    # Apply univariate feature selection on X_train\n",
    "    print(f'This is the size of X_train before univariate: {X_train.shape}')\n",
    "    X_train = X_train[:, features_selected]\n",
    "    print(f'This is the size of X_train after univariate: {X_train.shape}')\n",
    "\n",
    "    # Train the classifier on the selected features with the best hyperparameters to create best trained classifier\n",
    "    clf_3 = GaussianNB(var_smoothing=rand_search.best_params_['clf__var_smoothing'], cv=inner_cv)\n",
    "    clf_3.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf_3.predict(X_train)\n",
    "\n",
    "    if hasattr(clf_3, 'predict_proba'):\n",
    "    # The first column gives the probability for class = 0, so we take\n",
    "    # the second which gives the probability class = 1:\n",
    "        y_score = clf_3.predict_proba(X_train)[:, 1]\n",
    "    else:\n",
    "        y_score = y_pred\n",
    "\n",
    "    auc=metrics.roc_auc_score(y_train, y_score)\n",
    "\n",
    "    print(f'The auc of the training data is {auc}')\n",
    "    # cv results\n",
    "    #pd.DataFrame(grid_nb.cv_results_)\n",
    "\n",
    "    # Evaluate the classifier on the test data\n",
    "    X_test = pca.transform(X_test)  # Apply the PCA transformation to the test data\n",
    "    X_test = X_test_pca[:, features_selected]  # Apply the feature selection to the PCA-transformed test data\n",
    "    y_pred = clf_3.predict(X_test)\n",
    "    auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "    print(f'The auc of the test data is {auc}')\n",
    "\n",
    "    # pipeline 4: \n",
    "    # pipeline 5:\n",
    "    # pipeline 6:\n",
    "    # pipeline 7:\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIPELINE 2\n",
    "- RobustScaler --> PCA + univariate --> SVM_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define outer and inner cross validation\n",
    "outer_cv = StratifiedKFold(n_splits=4)\n",
    "inner_cv = StratifiedKFold(n_splits=2) #, shuffle=True, random_state=42)\n",
    "\n",
    "for train_index, test_index in outer_cv.split(X, y): \n",
    "    X_train = X.transpose()[train_index]\n",
    "    X_train = X_train.transpose()\n",
    "    print(X_train.shape)\n",
    "    # print size of X_train\n",
    "    y_train = y[train_index]\n",
    "    \n",
    "    X_test = X.transpose()[test_index]\n",
    "    X_test = X_test.transpose()\n",
    "    y_test = y[test_index]\n",
    "    # print size of X_test\n",
    "    print(X_test.shape)\n",
    "\n",
    "    # balance the classes, so training set consists of 50% normal and 50% abnormal ECG's\n",
    "    ros = RandomOverSampler(sampling_strategy='minority')\n",
    "    X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "    X_train = X_resampled\n",
    "    y_train = y_resampled   \n",
    "\n",
    "    ## PIPELINE 2: RobustScaler --> PCA + univariate --> SVM_linear\n",
    "    # Define pipeline 2\n",
    "    pipeline_2 = Pipeline([\n",
    "        ('scaler', RobustScaler()),\n",
    "        ('var_threshold', VarianceThreshold(threshold=0.0)),\n",
    "        ('pca', PCA()),\n",
    "        ('univar_feat_sel', SelectKBest(f_classif)),\n",
    "        ('clf', SVM())\n",
    "    ])\n",
    "    # Define hyperparameters of pipeline 3\n",
    "    param_grid = {\n",
    "    'pca__n_components': [0.5,0.75,0.9],\n",
    "    'univar_feat_sel__k': [5, 10, 20, 30],\n",
    "    'classifier__C': np.logspace(-3, 1, 20)\n",
    "    'classifier__kernel': ['linear']\n",
    "    }\n",
    "\n",
    "    # Perform grid search with inner cross-validation, part 1\n",
    "    rand_search = RandomizedSearchCV(pipeline_2, param_grid=param_grid, cv=inner_cv, scoring='roc_auc') # optimize parameters\n",
    "    rand_search.fit(X_train, y_train)\n",
    "\n",
    "    # Print the best hyperparameters and score\n",
    "    print('Best hyperparameters after first grid search:', rand_search.best_params_)\n",
    "    print('Best score after first grid search:', rand_search.best_score_)\n",
    "\n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components = rand_search.best_params_['pca__n_components'])\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    print(f'Size of X_Train after PC {X_train.shape}')\n",
    "\n",
    "    # Get the selected feature indices from the univariate feature selection\n",
    "    sel_kb = SelectKBest(f_classif, k='all')\n",
    "    sel_kb.fit(X_train, y_train)\n",
    "    p_values = sel_kb.pvalues_\n",
    "\n",
    "    reject_fdr, pvals_fdr, _, _ = multipletests(pvals=p_values, alpha=0.05, method='fdr_bh')\n",
    "    features_selected=np.array(np.where(reject_fdr)[0])\n",
    "    print(f'Size of selected features {features_selected.shape}')\n",
    "\n",
    "    # Apply univariate feature selection on X_train\n",
    "    print(f'This is the size of X_train before univariate: {X_train.shape}')\n",
    "    X_train = X_train[:, features_selected]\n",
    "    print(f'This is the size of X_train after univariate: {X_train.shape}')\n",
    "\n",
    "    # Train the classifier on the selected features with the best hyperparameters to create best trained classifier\n",
    "    clf_2 = SVC(C=rand_search.best_params_['classifier__C'], kernel=rand_search.best_params_['classifier__kernel'])\n",
    "    clf_2.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf_2.predict(X_train)\n",
    "\n",
    "    if hasattr(clf_2, 'predict_proba'):\n",
    "    # The first column gives the probability for class = 0, so we take\n",
    "    # the second which gives the probability class = 1:\n",
    "        y_score = clf_2.predict_proba(X_train)[:, 1]\n",
    "    else:\n",
    "        y_score = y_pred\n",
    "\n",
    "    auc = metrics.roc_auc_score(y_train, y_score)\n",
    "\n",
    "    print(f'The auc of the training data is {auc}')\n",
    "    # cv results\n",
    "    #pd.DataFrame(grid_nb.cv_results_)\n",
    "\n",
    "    # Evaluate the classifier on the test data\n",
    "    X_test = pca.transform(X_test)  # Apply the PCA transformation to the test data\n",
    "    X_test = X_test_pca[:, features_selected]  # Apply the feature selection to the PCA-transformed test data\n",
    "    y_pred = clf_2.predict(X_test)\n",
    "    auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "    print(f'The auc of the test data is {auc}')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIPELINE 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define outer and inner cross validation\n",
    "outer_cv = StratifiedKFold(n_splits=4)\n",
    "inner_cv = StratifiedKFold(n_splits=10) #, shuffle=True, random_state=42)\n",
    "\n",
    "for train_index, test_index in outer_cv.split(X, y): \n",
    "    X_train = X.transpose()[train_index]\n",
    "    X_train = X_train.transpose()\n",
    "    print(X_train.shape)\n",
    "    # print size of X_train\n",
    "    y_train = y[train_index]\n",
    "    \n",
    "    X_test = X.transpose()[test_index]\n",
    "    X_test = X_test.transpose()\n",
    "    y_test = y[test_index]\n",
    "    # print size of X_test\n",
    "    print(X_test.shape)\n",
    "\n",
    "    # balance the classes, so training set consists of 50% normal and 50% abnormal ECG's\n",
    "    ros = RandomOverSampler(sampling_strategy='minority')\n",
    "    X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "    X_train = X_resampled\n",
    "    print(f'Size of X_train after resampling {X_train.shape}')\n",
    "    y_train = y_resampled   \n",
    "\n",
    "    ## PIPELINE 4: RobustScaler --> PCA + univariate --> Quadratic Discriminant\n",
    "    # Define pipeline 4\n",
    "    pipeline_4 = Pipeline([\n",
    "        ('scaler', RobustScaler()),\n",
    "        ('var_threshold', VarianceThreshold(threshold=0.0)),\n",
    "        ('pca', PCA()),\n",
    "        ('univar_feat_sel', SelectKBest(f_classif)),\n",
    "        ('clf', QuadraticDiscriminantAnalysis())\n",
    "    ])\n",
    "    # Define hyperparameters of pipeline 3\n",
    "    param_grid = {\n",
    "    'pca__n_components': [0.5,0.75,0.9,0.95,0.99],\n",
    "    'univar_feat_sel__k': ['all']\n",
    "    }\n",
    "\n",
    "    # Perform grid search with inner cross-validation, part 1\n",
    "    rand_search = RandomizedSearchCV(pipeline_4, param_distributions=param_grid,n_iter = 50, cv=inner_cv, scoring='roc_auc') # optimize parameters\n",
    "    rand_search.fit(X_train, y_train)\n",
    "\n",
    "    # Print the best hyperparameters and score\n",
    "    print('Best hyperparameters after first grid search:', rand_search.best_params_)\n",
    "    print('Best score after first grid search:', rand_search.best_score_)\n",
    "\n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components = rand_search.best_params_['pca__n_components'])\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    print(f'Size of X_Train after PC {X_train.shape}')\n",
    "\n",
    "    # Get the selected feature indices from the univariate feature selection\n",
    "    sel_kb = SelectKBest(f_classif, k='all')\n",
    "    sel_kb.fit(X_train, y_train)\n",
    "    p_values = sel_kb.pvalues_\n",
    "\n",
    "    reject_fdr, pvals_fdr, _, _ = multipletests(pvals=p_values, alpha=0.05, method='fdr_bh')\n",
    "    features_selected=np.array(np.where(reject_fdr)[0])\n",
    "    print(f'Size of selected features {features_selected.shape}')\n",
    "    \n",
    "    # Apply univariate feature selection on X_train\n",
    "    print(f'This is the size of X_train before univariate: {X_train.shape}')\n",
    "    X_train = X_train[:, features_selected]\n",
    "    print(f'This is the size of X_train after univariate: {X_train.shape}')\n",
    "\n",
    "    # Train the classifier on the selected features with the best hyperparameters to create best trained classifier\n",
    "    clf_4 = QuadraticDiscriminantAnalysis()\n",
    "    clf_4.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf_4.predict(X_train)\n",
    "\n",
    "    if hasattr(clf_4, 'predict_proba'):\n",
    "    # The first column gives the probability for class = 0, so we take\n",
    "    # the second which gives the probability class = 1:\n",
    "        y_score = clf_4.predict_proba(X_train)[:, 1]\n",
    "    else:\n",
    "        y_score = y_pred\n",
    "\n",
    "    auc = metrics.roc_auc_score(y_train, y_score)\n",
    "\n",
    "    print(f'The auc of the training data is {auc}')\n",
    "    # cv results\n",
    "    #pd.DataFrame(grid_nb.cv_results_)\n",
    "\n",
    "    # Evaluate the classifier on the test data\n",
    "    X_test = pca.transform(X_test)  # Apply the PCA transformation to the test data\n",
    "    X_test = X_test[:, features_selected]  # Apply the feature selection to the PCA-transformed test data\n",
    "    y_pred = clf_4.predict(X_test)\n",
    "    auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "    print(f'The auc of the test data is {auc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(620, 9000)\n",
      "(207, 9000)\n",
      "(620, 9000)\n",
      "(207, 9000)\n",
      "(620, 9000)\n",
      "(207, 9000)\n",
      "(621, 9000)\n",
      "(206, 9000)\n"
     ]
    }
   ],
   "source": [
    "# Define the pipelines\n",
    "#1\n",
    "def pipeline1(X_train, y_train, X_test, y_test):\n",
    "    # Preprocessing\n",
    "    scaler = RobustScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Variance Threshold\n",
    "    vt = VarianceThreshold(threshold=0.0)\n",
    "    X_train = vt.fit_transform(X_train)\n",
    "    X_test = vt.transform(X_test)\n",
    "\n",
    "    # PCA\n",
    "    n_samples = X_train.shape[0]\n",
    "    n_features = X_train.shape[1]\n",
    "    n_features = min(n_samples, n_features)\n",
    "    pca = PCA(n_components=n_features)\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "\n",
    "    # Univariate selection\n",
    "    kb = SelectKBest(f_classif, k='all') # juiste code erin als af\n",
    "    X_train = kb.fit_transform(X_train, y_train)\n",
    "    p_values_train = kb.pvalues_\n",
    "    reject_fdr, pvals_fdr, _, _ = multipletests(pvals=p_values_train, alpha=0.05, method='fdr_bh')\n",
    "    features_selected_train = np.array(np.where(reject_fdr)[0])\n",
    "    print(features_selected_train.shape)\n",
    "    X_train = X_train[:,features_selected_train]\n",
    "\n",
    "    X_test = kb.transform(X_test)\n",
    "    p_values_test = kb.pvalues_\n",
    "    reject_fdr, pvals_fdr, _, _ = multipletests(pvals=p_values_test, alpha=0.05, method='fdr_bh')\n",
    "    features_selected_test = np.array(np.where(reject_fdr)[0])\n",
    "    print(features_selected_test.shape)\n",
    "    X_test = X_test[:,features_selected_test]\n",
    "\n",
    "    # KNN classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    roc_auc_score = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    return roc_auc_score\n",
    "\n",
    "def pipeline2(X_train, y_train, X_test, y_test, alpha=0.5, n_neighbors=5):\n",
    "    # Preprocessing\n",
    "    scaler = RobustScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Variance Threshold\n",
    "    vt = VarianceThreshold(threshold=0.0)\n",
    "    X_train = vt.fit_transform(X_train)\n",
    "    X_test = vt.transform(X_test)\n",
    "\n",
    "    # LASSO regularization\n",
    "    lasso = Lasso(alpha=alpha)\n",
    "    X_train = lasso.fit_transform(X_train, y_train)\n",
    "    X_test = lasso.transform(X_test)\n",
    "\n",
    "    # KNN classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    roc_auc_score = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    return roc_auc_score\n",
    "\n",
    "# Define the parameters for grid search\n",
    "param_grid1 = {\n",
    "    'n_pca': [], #doesn't have parameters that need grid_search\n",
    "    'k_features': [], #doesn't have parameters that need grid_search\n",
    "    'n_neighbors': list(range(1, 26, 2))\n",
    "}\n",
    "\n",
    "param_grid2 = {\n",
    "    'alpha': [np.logspace(-5, 1, 100)],\n",
    "    'n_neighbors': list(range(1, 26, 2))\n",
    "}\n",
    "\n",
    "# Define the outer and inner cross-validation\n",
    "outer_cv = StratifiedKFold(n_splits=4) #als we 4 splits doen, wordt de data in 25%/75% gesplit\n",
    "inner_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform the grid search using nested cross-validation\n",
    "best_scores1 = []\n",
    "best_scores2 = []\n",
    "\n",
    "best_n_neighbors = []\n",
    "results_1 = []\n",
    "results_2 = []\n",
    "\n",
    "for train_index, test_index in outer_cv.split(X, y): \n",
    "    X_train = X.transpose()[train_index]\n",
    "    X_train = X_train.transpose()\n",
    "    print(X_train.shape)\n",
    "    y_train = y[train_index]\n",
    "    \n",
    "    X_test = X.transpose()[test_index]\n",
    "    X_test = X_test.transpose()\n",
    "    y_test = y[test_index]\n",
    "    print(X_test.shape)\n",
    "\n",
    "    # Create a grid search to find the optimal k using a gridsearch and 10-fold cross validation\n",
    "#     grid_search1 = GridSearchCV(estimator=pipeline1, param_grid=param_grid1, cv=inner_cv, scoring='roc_auc') #error logisch, want pipeline1 is een functie\n",
    "#     grid_search1.fit(X_train, y_train)\n",
    "#     best_scores1.append(grid_search1.best_score_)\n",
    "\n",
    "#     grid_search2 = GridSearchCV(estimator=pipeline2, param_grid=param_grid2, cv=inner_cv, scoring='roc_auc')\n",
    "#     grid_search2.fit(X_train, y_train)\n",
    "#     best_scores2.append(grid_search2.best_score_)\n",
    "\n",
    "# # Print the best scores and the average score for each pipeline\n",
    "# print(best_scores1)\n",
    "# print(best_scores2)\n",
    "\n",
    "# # Get resulting classifier, pipeline 1\n",
    "# best_1 = grid_search1.best_estimator_\n",
    "# print(f'Best classifier: k={best_1.n_neighbors}')\n",
    "# best_n_neighbors.append(best_1.n_neighbors)\n",
    "\n",
    "# # Test the classifier on the test data, pipeline 1\n",
    "# probabilities = best_1.predict_proba(X_test)\n",
    "# scores_1 = probabilities[:, 1]\n",
    "\n",
    "# # Get the auc, pipeline 1\n",
    "# auc_1 = metrics.roc_auc_score(y_test, scores_1)\n",
    "# results_1.append({\n",
    "#     'auc': auc_1,\n",
    "#     'k': best_1.n_neighbors,\n",
    "#     'set': 'test'\n",
    "# })\n",
    "\n",
    "# # Create results dataframe and plot it, pipeline 1\n",
    "# results = pd.DataFrame(results_1)\n",
    "# seaborn.boxplot(y='auc', x='set', data=results_1)\n",
    "\n",
    "# optimal_n = int(np.median(best_n_neighbors))\n",
    "# print(f\"The optimal N={optimal_n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
